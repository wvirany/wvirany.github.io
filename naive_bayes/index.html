<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content=""/>

<title>
    
    Naive Bayes | Walter Virany
    
</title>

<link rel="canonical" href="http://localhost:1313/naive_bayes/"/>












<link rel="stylesheet" href="/assets/combined.min.70663b98395cb46c50fa4dfa0b5a36b1136b531b6fed206fa9944c2cc27b3221.css" media="all">





  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">Walter Virany</h1>
    
    
    

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /blog
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/me" >
                /me
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/why" >
                /why?
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        







<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">Naive Bayes</h1>
    

    

    <p class="single-readtime">
      

      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <p>First, let&rsquo;s establish the Naive Bayes classification problem.</p>
<p>Supppose we are given a dataset of $n$ iid observations $\cD = \{(\x^{(i)}, y_i)\}_{i=1}^n$, where each $\x^{(i)}$ is some $d$-dimensional input vector and $y_i \in \{1, 2, \ldots, K\}$ is the corresponding class label. Given a new input $\x$, the goal of classification is to correctly assign $\x$ to one of $K$ classes.</p>
<p>Let&rsquo;s distinguish between two approaches to classification: <em>discriminative</em> vs. <em>generative</em> modeling.</p>
<p>In the <strong>discriminative modeling</strong> approach, we wish to model the distribution over class labels $p(y \mid \x)$. Thus, given an input $\x$, we should be able to assign probabilities to each of the $K$ classes, and choose the $y$ which maximizes this probabilitiy.</p>
<p>In the <strong>generative modeling</strong> approach, we wish to model the sample generating process by estimating each class-conditional probability density. That is, using Bayes&rsquo; rule, we can write</p>
<p>$$
\begin{equation}\label{1}
p(y \mid \x) \propto p(\x \mid y)p(y).
\end{equation}
$$</p>
<p>We see that this achieves the same result as the discriminative modeling approach, but requires a bit more work - instead of skipping straight to the step of modeling the class probabilities given an input, we aim to model $p(\x \mid y)$ for each class $K$. Moreover, we also wish to estimate the prior class probabilities $p(y)$. One might ask: why bother? Well, in doing so, we gain the ability to generate new data points by first sampling $y \sim p(y)$, then sampling $x \sim p(\x \mid y)$.</p>
<p>Naive Bayes takes a generative modeling approach to classification.</p>
<p>There are many ways to estimate the densities $p(\x \mid y)$ and $p(y)$. One such method is maximum likelihood estimation (MLE), where we assume a particular parametric form of each distribution&mdash;for example, $\x$ might be drawn from a Gaussian distribution, with parameters $\bmu_k$ and $\bSigma_k$, and $y$ might be drawn from a multinomial distribution with $K$ classes, with probabilities $p(y = k) = \pi_k$. Then, we could find the parameters which maximize the likelihood of the observed dataset and use these to estimate the probability densities. However, for the sake of understanding Naive Bayes, I won&rsquo;t focus on density-estimation techniques here.</p>
<p>Now, let&rsquo;s assume that the features of $\x$ are binary. Then, we can write the joint distribution over the features of $\x$, conditioned on $y$ as</p>
<p>$$
\begin{equation}\label{2}
p(\x \mid y) = p(x_1, x_2, \ldots, x_d \mid y),
\end{equation}
$$</p>
<p>where each $x_i \in \{0, 1\}$. Then, if we want to estimate the probability density of $\x$ being in a particular class $y$, we would need to assign a probability to each possible configuration of $\x$ &mdash; there are $2^d$ possible such configurations! This requires estimating $2^d - 1$ parameters. <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>  In order to gain an accurate estimation for such a distribution, we would need <strong>a lot of data</strong> &mdash;  this is an example of the <em>curse of dimensionality</em>. Moreover, this is just for a single class $y$.</p>
<p>This calls for a need to reduce the dimensionality of our parameter space.</p>
<p><strong>What is conditional independence?</strong></p>
<p>The &ldquo;naive&rdquo; assumption in Naive Bayes is that the features of our inputs are conditionally independent. Under this assumption, we can factor the joint distribution in $\eqref{2}$ as</p>
<p>$$
p(x_1, \ldots, x_d \mid y) = p(x_1 \mid y) \cdots p(x_2 \mid y).
$$</p>
<p>One way to say this is &ldquo;$x_1, x_2, \ldots, x_d$ are independent, conditioned on $y$.&rdquo; Now, we can model each individual distribution $p(x_i \mid y)$ independently. In the case of binary features, this only requires $d$ parameters for each class instead of $2^d-1$.</p>
<p><strong>When does Naive Bayes work?</strong></p>
<p>Let&rsquo;s consider two classification problems:</p>
<p><strong>Example: Spam vs. ham</strong></p>
<p><strong>Example: Medical diagnosis</strong></p>
<p>Consider the task of diagnosing heart disease based on the following symptoms:</p>
<ul>
<li>$C$ - chest pain</li>
<li>$B$ - high blood pressure</li>
<li>$S$ - shortness of breath</li>
</ul>
<p>Furthermore, let $H$ indicate whether or not the patient has heart disease. Given a patient with any combination of these symptoms, we wish to diagnose their condition.</p>
<p>Now, suppose we know</p>
<ul>
<li>$P(H) = 0.05$</li>
<li>$P(\neg H) = 0.95$</li>
</ul>
<p>That is, 5% of the population suffers from heart disease. Moreover, suppose we know that for people with heart disease:</p>
<ul>
<li>$P(C \mid H) = 0.8$</li>
<li>$P(B \mid H) = 0.6$</li>
<li>$P(S \mid H) = 0.7$</li>
</ul>
<p>and that for people without heart disease:</p>
<ul>
<li>$P(C \mid \neg H) = 0.05$</li>
<li>$P(B \mid \neg H) = 0.2$</li>
<li>$P(S \mid \neg H) = 0.1$</li>
</ul>
<p>If a patient has heart disease, the probability that they suffer from a given symptom is high. On the other hand, the probability that a healthy patient suffers from a given symptom is low.</p>
<p>Now, suppose we have a patient come in who is suffering from all three symptoms. From Bayes&rsquo; rule, we have</p>
<p>$$
\begin{align*}
P(H \mid C, B, S) &amp;\propto P(C, B, S \mid H)P(H) \\[6pt]
\text{ \scriptsize (naive Bayes assumption) } \qquad &amp;= P(C\mid H) \, P(B\mid H) \, P(S\mid H) \, P(H) \\[6pt]
&amp;= (0.8) * (0.6) * (0.7) * (0.05) \\[6pt]
&amp;= 0.0168.
\end{align*}
$$</p>
<p>Similarly,</p>
<p>$$
\begin{align*}
P(\neg H \mid C, B, S) &amp;\propto P(C \mid \neg H) \, P(B \mid \neg H) \, P(S \mid \neg H) \, P(\neg H) \\[6pt]
&amp;= (0.05) * (0.2) * (0.1) * (0.95) \\
&amp;= 0.00095.
\end{align*}
$$</p>
<p>The result:</p>
<p>$$
P(H \mid C, B, S) = \frac{P(C, B, S \mid H)P(H)}{P(H)}
$$</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Estimating a discrete probability distribution with $n$ possible inputs only requires $n-1$ parameters due to the summation constraint:
$$
\sum_{i=1}^n p_i = 1,
$$
where $p_i$ is the probability of the $i^{\text{th}}$ event. The $n^{\text{th}}$ probability is defined implicitly as $1 - \sum_{i=1}^{n-1}p_i$.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    
  </div>

  

  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      
<style>
  .katex a {
    text-decoration: none;
    color: inherit;
  }
  .katex a:hover {
    text-decoration: none;
  }
</style>

<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ],
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}",
        "\\R": "\\mathbb{R}",
        "\\bR": "\\mathbf{R}",
        "\\C": "\\mathbb{C}",
        "\\Z": "\\mathbb{Z}",
        "\\N": "\\mathbb{N}",
        "\\Q": "\\mathbb{Q}",
        "\\E": "\\mathbb{E}",
        "\\cD": "\\mathcal{D}",
        "\\var": "\\operatorname{Var}",
        "\\cov":"\\operatorname{cov}",
        "\\x": "\\mathbf{x}",
        "\\X": "\\mathbf{X}",
        "\\w": "\\mathbf{w}",
        "\\W": "\\mathbf{W}",
        "\\y": "\\mathbf{y}",
        "\\z": "\\mathbf{z}",
        "\\Z": "\\mathbf{Z}",
        "\\u": "\\mathbf{u}",
        "\\U": "\\mathbf{U}",
        "\\V": "\\mathbf{V}",
        "\\I": "\\mathbf{I}",
        "\\zv": "\\mathbf{0}",
        "\\A": "\\mathbf{A}",
        "\\a": "\\mathbf{a}",
        "\\B": "\\mathbf{B}",
        "\\b": "\\mathbf{b}",
        "\\c": "\\mathbf{c}",
        "\\D": "\\mathbf{D}",
        "\\f": "\\mathbf{f}",
        "\\M": "\\mathbf{M}",
        "\\m": "\\mathbf{m}",
        "\\bC": "\\mathbf{C}",
        "\\J": "\\mathbf{J}",
        "\\K": "\\mathbf{K}",
        "\\L": "\\mathbf{L}",
        "\\bS": "\\mathbf{S}",
        "\\bmu": "\\boldsymbol{\\mu}",
        "\\bphi": "\\boldsymbol{\\phi}",
        "\\bepsilon": "\\boldsymbol{\\epsilon}",
        "\\bSigma": "\\boldsymbol{\\Sigma}",
        "\\bLambda": "\\boldsymbol{\\Lambda}",
        "\\bPhi": "\\boldsymbol{\\Phi}",
        "\\zero": "\\mathbf{0}",
        "\\one": "\\mathbf{1}",
        "\\T": "^{\\top}",
        "\\p": "^\\prime",
        "\\inv": "^{-1}",
        "\\ij": "_{ij}",
        "\\Norm": "\\mathcal{N}",
        "\\GP": "\\mathcal{GP}",
        "\\bmid": "\\,\\Big|\\,",
        "\\gam": "\\text{Gamma}",
        "\\nll": "\\text{NLL}",
        "\\argmin": "\\underset{#1}{\\operatorname{argmin}}",
        "\\argmax": "\\underset{#1}{\\operatorname{argmax}}\\;",
        "\\diag": "\\operatorname{diag}",
        "\\tr": "\\operatorname{tr}",
        "\\pbmu": "\\frac{\\partial}{\\partial \\boldsymbol{\\mu}}",
        "\\pSigma": "\\frac{\\partial}{\\partial \\Sigma}",
        "\\pbx": "\\frac{\\partial}{\\partial \\mathbf{x}}",
        "\\px": "\\frac{\\partial}{\\partial x}",
        "\\pbA": "\\frac{\\partial}{\\partial \\mathbf{A}}",
        "\\ml": "_\\text{ML}",
      }
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>


<script>
  
  function updateFigureNumbers() {

      const figRefs = document.querySelectorAll('.fig-ref');
      figRefs.forEach(ref => {
          const figId = ref.getAttribute('href').slice(1);
          const figElement = document.getElementById(figId);
          if (figElement) {
              const figIndex = Array.from(figures).indexOf(figElement) + 1;
              ref.textContent = `Figure ${figIndex}`;
          }
      });
  }

  
  window.addEventListener('load', updateFigureNumbers);
</script>



</html>