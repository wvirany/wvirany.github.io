<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Walter Virany</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Walter Virany</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>Some math behind the Gaussian distribution</title>
      <link>http://localhost:1313/posts/gaussian/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gaussian/</guid>
      <description>Despite its ubiquity, I have frequently found myself in a state somewhere between discomfort and panic each time I am faced with the task of manipulating the Gaussian distribution, particularly in multiple dimensions. So, I&amp;rsquo;ve taken the opportunity to work through some detailed derivations involving the Gaussian.&#xA;In this blog, I focus on the multivariate Gaussian distribution, beginning by reasoning about its shape and properties in high dimensions. Then, I derive some useful formulas such as conditioning, marginalization, and certain transformations.</description>
    </item>
    <item>
      <title>Understanding the Neural Network Gaussian Process</title>
      <link>http://localhost:1313/posts/nngp/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nngp/</guid>
      <description>Despite their overwhelming success in modern machine learning, deep neural networks remain poorly understood from a theoretical perspective. Classical statistical wisdom dictates that overparameterized models (i.e., models with more degrees of freedom than data samples) should overfit noisy data and thus generalize poorly. Yet, even in cases in which deep neural networks fit noisy training data almost perfectly, they still exhibit good generalizability. This contradiction has highlighted a serious gap between the theory and practice of deep learning, motivating the need for a more complete theoretical framework.</description>
    </item>
    <item>
      <title>Predicting Microbiome-Phenotype Associations in the Human Gut</title>
      <link>http://localhost:1313/posts/microbiome/</link>
      <pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/microbiome/</guid>
      <description>The human microbiome is a data rich ecosystem, which is generally known to have strong associations with host phenotypes (i.e., the set of observable characteristics of an organism). Yet, the extents of these relationships are not yet deeply understood. Until recently, the computing capabilities necessary to thoroughly analyze human microbiome data were severely lacking. However, due to novel advancements in sequencing technologies and data analysis techniques, there now exists a plethora of data, along with robust statistical tools, which can be leveraged to gain valuable insights into the impact of the human microbiome on overall health.</description>
    </item>
    <item>
      <title>About me?</title>
      <link>http://localhost:1313/me/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/me/</guid>
      <description>I recently graduated from the University of Colorado with a B.S. in applied mathematics as well as minors in computer science and physics.&#xD;My interests lie broadly in machine learning, statistics, and optimization, with applications in chemistry, biology, and medicine.&#xD;Updates&#xD;10 Jan 2025&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Presented at JMM 2025&#xA;15 Oct 2024&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Starting a research internship at Mila&#xA;23 Jun 2024&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Participating in RIPS at IPAM for Summer 2024&#xA;9 May 2024</description>
    </item>
    <item>
      <title>Why the blog?</title>
      <link>http://localhost:1313/why/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/why/</guid>
      <description>I initially created an academic page to host a portfolio where I could show off my projects to potential employers. However, it quickly turned into a small passion for writing about the things I&amp;rsquo;m working on. Now, I enjoy maintaining this blog for several reasons:&#xA;First, I often find myself looking for resources which dumb down complicated research topics or offer helpful career advice. I have benefitted substantially from other academic blogs on the internet, so I maintain this blog in an attempt to contribute to that community1.</description>
    </item>
  </channel>
</rss>
