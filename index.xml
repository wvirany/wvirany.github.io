<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Home on Walter Virany</title>
    <link>http://localhost:1313/</link>
    <description>Recent content in Home on Walter Virany</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Fri, 13 Jun 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>A spelled-out introduction to Gaussian processes</title>
      <link>http://localhost:1313/posts/gp_intro/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gp_intro/</guid>
      <description>Gaussian processes (GPs) have confounded me since I was first introduced to them. Many introductions talk about the beauty of implicitly defining infinitely many basis functions, or performing Bayesian inference directly in the space of functions. These explanations can seem daunting at first, but in this blog I aim to build up to GPs from a basic linear model,&#xA;spelled-out introduction.&#xA;Bayesian linear regression The linear model To build the foundation for GPs, I&amp;rsquo;ll start by considering a Bayesian treatment of linear regression.</description>
    </item>
    <item>
      <title>Some math behind the Gaussian distribution</title>
      <link>http://localhost:1313/posts/gaussian/</link>
      <pubDate>Fri, 13 Jun 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/gaussian/</guid>
      <description>Despite its ubiquity, I have frequently found myself in a state somewhere between discomfort and panic each time I am faced with the task of manipulating the Gaussian distribution, particularly in multiple dimensions. So, I&amp;rsquo;ve taken the opportunity to work through some detailed derivations involving the Gaussian.&#xA;In this blog, I focus on the multivariate Gaussian distribution, beginning by reasoning about its shape and properties in high dimensions. Then, I derive some useful formulas such as conditioning, marginalization, and certain transformations.</description>
    </item>
    <item>
      <title>MLE vs. Bayesian inference for the Gaussian distribution</title>
      <link>http://localhost:1313/drafts/bayes_inference/</link>
      <pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/drafts/bayes_inference/</guid>
      <description>In my previous post, I developed several key results involving the Gaussian distribution. Now, I&amp;rsquo;ll show two methods for estimating the parameters of a Gaussian distribution from data; maximum likelihood estimation (MLE) and Bayesian inference.&#xA;Maximum likelihood estimation In practice, we rarely know the true underlying distribution from which data is generated. The goal of maximum likelihood estimation (MLE) is to estimate the true parameters of a distribution from an observed set of data.</description>
    </item>
    <item>
      <title>Bayesian Optimization for Molecular Design</title>
      <link>http://localhost:1313/drafts/bayes_opt/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/drafts/bayes_opt/</guid>
      <description>Background Goal is to maximize some function $f$, i.e., find $$ \begin{align*} x^* \in \arg \max_{x \in \mathcal{X}} \end{align*} $$&#xA;$f$ need not be analytic (i.e., can&amp;rsquo;t write it down), nor computable - we just need some access to information at specified points </description>
    </item>
    <item>
      <title>A VQ-VAE for Molecular Representation Learning</title>
      <link>http://localhost:1313/drafts/vq-vae/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/drafts/vq-vae/</guid>
      <description>In this blog, I build a VQ-VAE for for protein structure tokenization which learns meaningful representations of molecules.&#xA;Variational Autoencoders First, I&amp;rsquo;ll start with an overview of Variational Autoencoders (VAE), as the VQ-VAE is a natural extension.&#xA;A VAE consists of two main components: an encoder and a decoder network. The encoder network projects samples into a low-dimensional latent space1, which usually takes the form of a standard Gaussian, whereas the decoder network reconstructs data samples from these low-dimensional representations.</description>
    </item>
    <item>
      <title>Understanding the Neural Network Gaussian Process</title>
      <link>http://localhost:1313/posts/nngp/</link>
      <pubDate>Tue, 20 Aug 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/nngp/</guid>
      <description>Despite their overwhelming success in modern machine learning, deep neural networks remain poorly understood from a theoretical perspective. Classical statistical wisdom dictates that overparameterized models (i.e., models with more degrees of freedom than data samples) should overfit noisy data and thus generalize poorly. Yet, even in cases in which deep neural networks fit noisy training data almost perfectly, they still exhibit good generalizability. This contradiction has highlighted a serious gap between the theory and practice of deep learning, motivating the need for a more complete theoretical framework.</description>
    </item>
    <item>
      <title>Predicting Microbiome-Phenotype Associations in the Human Gut</title>
      <link>http://localhost:1313/posts/microbiome/</link>
      <pubDate>Thu, 14 Dec 2023 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/posts/microbiome/</guid>
      <description>The human microbiome is a data rich ecosystem, which is generally known to have strong associations with host phenotypes (i.e., the set of observable characteristics of an organism). Yet, the extents of these relationships are not yet deeply understood. Until recently, the computing capabilities necessary to thoroughly analyze human microbiome data were severely lacking. However, due to novel advancements in sequencing technologies and data analysis techniques, there now exists a plethora of data, along with robust statistical tools, which can be leveraged to gain valuable insights into the impact of the human microbiome on overall health.</description>
    </item>
    <item>
      <title></title>
      <link>http://localhost:1313/resources/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/resources/</guid>
      <description>Some useful resources roughly organized by topic&#xA;Machine Learning Blogs Bayesian Optimization A Tutorial on Bayesian Optimization, Peter Frazier&#xA;Bayesian Optimization Tutorial, Martin Krasser&#xA;Gaussian Processes Gaussian Process Regression Tutorial, Martin Krasser&#xA;Sparse Gaussian Processes, Martin Krasser&#xA;Cheminformatics Tools RDKit FingerprintGenerator Tutorial&#xA;datamol A useful RDKit wrapper for handling molecules&#xA;Tutorials BoTorch Tutorials Textbooks Gaussian Processes for Machine Learning, Rasmussen &amp;amp; Williams&#xA;Probabilistic Machine Learning: An Introduction&#xA;Bayesian Optimization Book, Garnett</description>
    </item>
    <item>
      <title>About me</title>
      <link>http://localhost:1313/me/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/me/</guid>
      <description>I recently graduated from the University of Colorado with a B.S. in applied mathematics as well as minors in computer science and physics.&#xD;My interests lie broadly in machine learning, statistics, and optimization, with applications in chemistry, biology, and medicine.&#xD;Updates&#xD;10 Jan 2025&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Presented at JMM 2025&#xA;15 Oct 2024&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Starting a research internship at Mila&#xA;23 Jun 2024&#xA;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;&amp;nbsp;Participating in RIPS at IPAM for Summer 2024&#xA;9 May 2024</description>
    </item>
    <item>
      <title>Generating Synthesizable Molecules (1/2)</title>
      <link>http://localhost:1313/drafts/synthesizable-molecules-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/drafts/synthesizable-molecules-1/</guid>
      <description>Part 1: Projecting Molecules into Synthesizable Chemical Spaces</description>
    </item>
    <item>
      <title>Why the blog?</title>
      <link>http://localhost:1313/why/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/why/</guid>
      <description>I initially created an academic page to host a portfolio where I could show off my projects to potential employers. However, it quickly turned into a small passion for writing about the things I&amp;rsquo;m working on. Now, I enjoy maintaining this blog for several reasons:&#xA;First, I often find myself looking for resources which dumb down complicated research topics or offer helpful career advice. I have benefitted substantially from other academic blogs on the internet, so I maintain this blog in an attempt to contribute to that community1.</description>
    </item>
  </channel>
</rss>
