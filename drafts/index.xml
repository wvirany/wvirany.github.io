<?xml version="1.0" encoding="utf-8" standalone="yes"?>
<rss version="2.0" xmlns:atom="http://www.w3.org/2005/Atom">
  <channel>
    <title>Drafts on Walter Virany</title>
    <link>http://localhost:1313/drafts/</link>
    <description>Recent content in Drafts on Walter Virany</description>
    <generator>Hugo</generator>
    <language>en-us</language>
    <lastBuildDate>Sun, 25 May 2025 00:00:00 +0000</lastBuildDate>
    <atom:link href="http://localhost:1313/drafts/index.xml" rel="self" type="application/rss+xml" />
    <item>
      <title>MLE vs. Bayesian inference for the Gaussian distribution</title>
      <link>http://localhost:1313/drafts/bayes_inference/</link>
      <pubDate>Sun, 25 May 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/drafts/bayes_inference/</guid>
      <description>In my previous post, I developed several key results involving the Gaussian distribution. Now, I&amp;rsquo;ll show two methods for estimating the parameters of a Gaussian distribution from data; maximum likelihood estimation (MLE) and Bayesian inference.&#xA;Maximum likelihood estimation In practice, we rarely know the true underlying distribution from which data is generated. The goal of maximum likelihood estimation (MLE) is to estimate the true parameters of a distribution from an observed set of data.</description>
    </item>
    <item>
      <title>Bayesian Optimization for Molecular Design</title>
      <link>http://localhost:1313/drafts/bayes_opt/</link>
      <pubDate>Sat, 01 Feb 2025 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/drafts/bayes_opt/</guid>
      <description>Background Goal is to maximize some function $f$, i.e., find $$ \begin{align*} x^* \in \arg \max_{x \in \mathcal{X}} \end{align*} $$&#xA;$f$ need not be analytic (i.e., can&amp;rsquo;t write it down), nor computable - we just need some access to information at specified points </description>
    </item>
    <item>
      <title>A VQ-VAE for Molecular Representation Learning</title>
      <link>http://localhost:1313/drafts/vq-vae/</link>
      <pubDate>Mon, 09 Dec 2024 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/drafts/vq-vae/</guid>
      <description>In this blog, I build a VQ-VAE for for protein structure tokenization which learns meaningful representations of molecules.&#xA;Variational Autoencoders First, I&amp;rsquo;ll start with an overview of Variational Autoencoders (VAE), as the VQ-VAE is a natural extension.&#xA;A VAE consists of two main components: an encoder and a decoder network. The encoder network projects samples into a low-dimensional latent space1, which usually takes the form of a standard Gaussian, whereas the decoder network reconstructs data samples from these low-dimensional representations.</description>
    </item>
    <item>
      <title>Generating Synthesizable Molecules (1/2)</title>
      <link>http://localhost:1313/drafts/synthesizable-molecules-1/</link>
      <pubDate>Mon, 01 Jan 0001 00:00:00 +0000</pubDate>
      <guid>http://localhost:1313/drafts/synthesizable-molecules-1/</guid>
      <description>Part 1: Projecting Molecules into Synthesizable Chemical Spaces</description>
    </item>
  </channel>
</rss>
