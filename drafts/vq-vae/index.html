<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content=""/>

<title>
    
    A VQ-VAE for Molecular Representation Learning | Walter Virany
    
</title>

<link rel="canonical" href="http://localhost:1313/drafts/vq-vae/"/>












<link rel="stylesheet" href="/assets/combined.min.c5b19f349890ba8c8308e8d948f1754211fca49303531d62bb79927877c7df0e.css" media="all">





  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">Walter Virany</h1>
    
    
    

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /blog
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/me" >
                /me
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/why" >
                /why?
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        







<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">A VQ-VAE for Molecular Representation Learning</h1>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2024-12-09T00:00:00&#43;00:00">December 9, 2024</time>
      

      
      &nbsp; Â· &nbsp;
      6 min read
      
    </p>

  </div>

  

  

  
  <aside class="toc">
    <p><strong>Table of contents</strong></p>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#variational-autoencoders">Variational Autoencoders</a>
      <ul>
        <li><a href="#variational-inference">Variational Inference</a></li>
        <li><a href="#a-simple-example">A Simple Example</a></li>
      </ul>
    </li>
    <li><a href="#vq-vaes">VQ-VAEs</a>
      <ul>
        <li><a href="#implementation">Implementation</a></li>
      </ul>
    </li>
    <li><a href="#learning-representations-of-molecules">Learning Representations of Molecules</a></li>
    <li><a href="#references-and-further-reading">References and Further Reading</a></li>
  </ul>
</nav>
  </aside>
  

  

  <div class="single-content">
    <p>In this blog, I build a VQ-VAE for for protein structure tokenization which learns meaningful representations of molecules.</p>
<h2 id="variational-autoencoders">Variational Autoencoders</h2>
<p>First, I&rsquo;ll start with an overview of Variational Autoencoders (VAE), as the VQ-VAE is a natural extension.</p>
<p>A VAE consists of two main components: an encoder and a decoder network. The encoder network projects samples into a low-dimensional latent space<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, which usually takes the form of a standard Gaussian, whereas the decoder network reconstructs data samples from these low-dimensional representations. Thus, the training objective aims to maximize the likelihood of generated samples coming from the true data distribution with the decoder while accurately modeling the latent space with the encoder.</p>
<p>Once trained, new data points can be generated by sampling from the latent distribution, and passing samples through the decoder. Alternatively, the encoder network can be used to produce meaningful low-dimensional representations of data, which is useful for data compression, representation learning, and other downstream tasks.</p>
<h3 id="variational-inference">Variational Inference</h3>
<p>Suppose our data $\mathbf{x}$ is generated by some random process, which depends on a continuous random variable $\mathbf{z}$. First, $\mathbf{z}$ is sampled from a prior distribution $p(\mathbf{z})$. Then, the data samples are generated from a conditional distribution $p(\mathbf{x}|\mathbf{z})$. We assume both of these distributions to be Gaussian. Unfortunately, we don&rsquo;t know these underlying distributions, nor can we observe the latent variables $\mathbf{z}$. However, we <em>can</em> approximate them.</p>
<p>Given observations $\mathbf{x}$, we can compute the posterior of the latent distribution $p(\mathbf{z|x})$. This can be used to gain information about the true latent variable distribution. Using Bayes&rsquo; formula, we can write</p>
<p>$$
p(\mathbf{z}|\mathbf{x}) = \frac{p(x|z)p(z)}{p(x)}.
$$</p>
<p>However, this depends on the the marginal distribution of the data $p(\mathbf{x})$, the computation of which involves integrating all possible values of $\mathbf{z}$:</p>
<p>$$
p(\mathbf{x}) = \int p(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}.
$$</p>
<p>Instead of trying to compute the posterior distribution, we can instead approximate it. This is done via our encoder network, which we denote $q_{\theta}({\mathbf{z}})$. Thus, we try to minimize the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> between the approximate latent density and the posterior distribution:</p>
<p>$$
q_{\theta}^{*}(\mathbf{z}) \in \argmin_{\theta} D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})).
$$</p>
<p>Of course, we don&rsquo;t know the posterior. However, we can rewrite this as</p>
<p>$$
\begin{align*}
D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})) &amp;= \mathbb{E} \left[ \log \left( \frac{q_{\theta}(\mathbf{z})}{p(\mathbf{z}|\mathbf{x})} \right) \right] \\
&amp;= \mathbb{E} \left[ \log q_\theta(\mathbf{z}) \right] - \mathbb{E} \left[ \log p(\mathbf{z} | \mathbf{x}) \right] \\
&amp;= \mathbb{E} \left[ \log q_{\theta}(\mathbf{z}) \right] - \mathbb{E} \left[ \log p(\mathbf{z}, \mathbf{x}) \right] + \mathbb{E} \left[ \log p(\mathbf{x}) \right],
\end{align*}
$$</p>
<p>where the expectation is with respect to $\mathbf{z} \sim q_{\theta}(\mathbf{z})$. Thus, we can drop the expectation around the last term, since it is independent of $\mathbf{z}$. By rearranging this, we see that</p>
<p>$$
\log p(\mathbf{x}) - D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})) = \mathbb{E} \left[ \log p(\mathbf{z}, \mathbf{x}) \right] - \mathbb{E} \left[ \log q_{\theta}(\mathbf{z}) \right].
$$</p>
<p>We see that by maximizing the RHS, we simultaneously maximize the likelihood of the data while minimizing the KL divergence between the the true and approximate posterior distributions. We define this to be the evidence lower bound (ELBO), which we use as our loss function in training<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>To compute the ELBO</p>
<h3 id="a-simple-example">A Simple Example</h3>
<p>Here I&rsquo;ll implement a simple example of a VAE for image generation. We&rsquo;ll use this later as the foundation for our VQ-VAE. First, I&rsquo;ll import some useful packages and configure my device:</p>
<!-- IMPORT PACKAGES -->
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">import</span> torch
</span></span><span style="display:flex;"><span><span style="color:#00f">import</span> torch.nn <span style="color:#00f">as</span> nn
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> torch.utils.data.dataloader <span style="color:#00f">import</span> DataLoader
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> torchvision <span style="color:#00f">import</span> datasets, transforms
</span></span><span style="display:flex;"><span><span style="color:#00f">import</span> cv2
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> tqdm <span style="color:#00f">import</span> tqdm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>device = torch.device(<span style="color:#a31515">&#39;cuda&#39;</span> <span style="color:#00f">if</span> torch.cuda.is_available() <span style="color:#00f">else</span> <span style="color:#a31515">&#39;cpu&#39;</span>)
</span></span></code></pre></div><p>I&rsquo;ll also download the MNIST dataset and create a <code>DataLoader</code> object:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>batch_size = 64
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Transform images to tensors</span>
</span></span><span style="display:flex;"><span>transform = transforms.Compose([
</span></span><span style="display:flex;"><span>    transforms.ToTensor()
</span></span><span style="display:flex;"><span>])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Download and load MNIST</span>
</span></span><span style="display:flex;"><span>train_dataset = datasets.MNIST(
</span></span><span style="display:flex;"><span>    root=<span style="color:#a31515">&#39;./data&#39;</span>,  <span style="color:#008000"># Where to store the dataset</span>
</span></span><span style="display:flex;"><span>    train=<span style="color:#00f">True</span>,     <span style="color:#008000"># Use training set</span>
</span></span><span style="display:flex;"><span>    download=<span style="color:#00f">True</span>,  <span style="color:#008000"># Download if not present</span>
</span></span><span style="display:flex;"><span>    transform=transform
</span></span><span style="display:flex;"><span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Create DataLoader</span>
</span></span><span style="display:flex;"><span>train_loader = DataLoader(
</span></span><span style="display:flex;"><span>    train_dataset,
</span></span><span style="display:flex;"><span>    batch_size=batch_size,
</span></span><span style="display:flex;"><span>    shuffle=<span style="color:#00f">True</span>
</span></span><span style="display:flex;"><span>)
</span></span></code></pre></div><p>Now, I&rsquo;ll start by implementing the encoder network. This takes an image as input, then predicts the mean and log-variance of the latent distribution:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">class</span> <span style="color:#2b91af">Encoder</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> __init__(self, input_dim=28*28, latent_dim=2):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self.common_MLP = nn.Sequential(
</span></span><span style="display:flex;"><span>            nn.Linear(input_dim, 196),
</span></span><span style="display:flex;"><span>            nn.Tanh(),
</span></span><span style="display:flex;"><span>            nn.Linear(196, 128),
</span></span><span style="display:flex;"><span>            nn.Tanh(),
</span></span><span style="display:flex;"><span>            nn.Linear(128, 64)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self.mean_MLP = nn.Sequential(
</span></span><span style="display:flex;"><span>            nn.Linear(64, 16),
</span></span><span style="display:flex;"><span>            nn.Tanh(),
</span></span><span style="display:flex;"><span>            nn.Linear(16, latent_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self.var_MLP = nn.Sequential(
</span></span><span style="display:flex;"><span>            nn.Linear(64, 16),
</span></span><span style="display:flex;"><span>            nn.Tanh(),
</span></span><span style="display:flex;"><span>            nn.Linear(16, latent_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> forward(self, x):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        B = x.shape[0]
</span></span><span style="display:flex;"><span>        x = x.view(B, -1) <span style="color:#008000"># Flattens to (batch_size, 784)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        x = self.common_MLP(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        mean = self.mean_MLP(x)
</span></span><span style="display:flex;"><span>        log_var = self.var_MLP(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00f">return</span> mean, log_var
</span></span></code></pre></div><p>Next, I implement the decoder network. This takes a latent vector as input and produces the reconstructed image:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">class</span> <span style="color:#2b91af">Decoder</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> __init__(self, latent_dim=2, output_dim=28*28):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self.latent_dim = latent_dim
</span></span><span style="display:flex;"><span>        self.output_dim = output_dim
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self.layers = nn.Sequential(
</span></span><span style="display:flex;"><span>            nn.Linear(latent_dim, 16),
</span></span><span style="display:flex;"><span>            nn.Tanh(),
</span></span><span style="display:flex;"><span>            nn.Linear(16, 64),
</span></span><span style="display:flex;"><span>            nn.Tanh(),
</span></span><span style="display:flex;"><span>            nn.Linear(64, 128),
</span></span><span style="display:flex;"><span>            nn.Tanh(),
</span></span><span style="display:flex;"><span>            nn.Linear(128, 256),
</span></span><span style="display:flex;"><span>            nn.Tanh(),
</span></span><span style="display:flex;"><span>            nn.Linear(256, output_dim)
</span></span><span style="display:flex;"><span>        )
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> forward(self, x):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00f">for</span> layer <span style="color:#00f">in</span> self.layers:
</span></span><span style="display:flex;"><span>            x = layer(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># For 2D square image, get dimension</span>
</span></span><span style="display:flex;"><span>        dim = int(self.output_dim**.5)
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#00f">return</span> x.view(x.shape[0], 1, dim, dim)
</span></span></code></pre></div><p>We can put this together into a full VAE:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">class</span> <span style="color:#2b91af">VAE</span>(nn.Module):
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> __init__(self, input_dim=28*28, output_dim=28*28, latent_dim=2):
</span></span><span style="display:flex;"><span>        super().__init__()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        self.encoder = Encoder(input_dim=input_dim, latent_dim=latent_dim)
</span></span><span style="display:flex;"><span>        self.decoder = Decoder(latent_dim=latent_dim, output_dim=output_dim)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> sample(self, mean, log_var):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Convert logvar to std</span>
</span></span><span style="display:flex;"><span>        std = torch.exp(0.5 * log_var)
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Sample standard Normal</span>
</span></span><span style="display:flex;"><span>        epsilon = torch.randn_like(std)
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># z is Normal w/ mean and var as predicted</span>
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Note: Var(std * eps) = std^2 * Var(eps) = std^2</span>
</span></span><span style="display:flex;"><span>        z = mean + std * epsilon
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00f">return</span> z
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> forward(self, x):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        mean, log_var = self.encoder(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        z = self.sample(mean, log_var)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        out = self.decoder(z)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00f">return</span> mean, log_var, out
</span></span></code></pre></div><p>This encodes an image into latent space by sampling from a Normal distribution with mean and variance as predicted by the encoder network. Then, it produces a reconstructed image by passing the latent vector through the decoder network.</p>
<p>Finally, we define the training function:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">def</span> train_vae(model, data_loader, num_epochs=10, learning_rate=1e-3):
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    optimizer = torch.optim.AdamW(model.parameters(), lr=learning_rate)
</span></span><span style="display:flex;"><span>    criterion = torch.nn.MSELoss()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">for</span> t <span style="color:#00f">in</span> range(num_epochs):
</span></span><span style="display:flex;"><span>        
</span></span><span style="display:flex;"><span>        <span style="color:#00f">for</span> im, label <span style="color:#00f">in</span> tqdm(data_loader):
</span></span><span style="display:flex;"><span>            im = im.float().to(device)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            optimizer.zero_grad()
</span></span><span style="display:flex;"><span>            
</span></span><span style="display:flex;"><span>            <span style="color:#008000"># Forward pass</span>
</span></span><span style="display:flex;"><span>            mean, log_var, out = model(im)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#008000"># Compute Losses</span>
</span></span><span style="display:flex;"><span>            kl_loss = -0.5 * torch.sum(1 + log_var - mean.pow(2) - log_var.exp())
</span></span><span style="display:flex;"><span>            reconstruction_loss = criterion(out, im)
</span></span><span style="display:flex;"><span>            loss = reconstruction_loss + 0.00001 * kl_loss
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>            <span style="color:#008000"># Backward pass</span>
</span></span><span style="display:flex;"><span>            loss.backward()
</span></span><span style="display:flex;"><span>            optimizer.step()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Total loss averaged over batch </span>
</span></span><span style="display:flex;"><span>        print(<span style="color:#a31515">f</span><span style="color:#a31515">&#34;Epoch: </span><span style="color:#a31515">{</span>t+1<span style="color:#a31515">}</span><span style="color:#a31515"> | Loss: </span><span style="color:#a31515">{</span>torch.mean(loss)<span style="color:#a31515">:</span><span style="color:#a31515">.4f</span><span style="color:#a31515">}</span><span style="color:#a31515">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    print(<span style="color:#a31515">&#39;Done Training ...&#39;</span>)
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>model = VAE()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>train_vae(model, train_loader, num_epochs=10)
</span></span></code></pre></div><h2 id="vq-vaes">VQ-VAEs</h2>
<h3 id="implementation">Implementation</h3>
<h2 id="learning-representations-of-molecules">Learning Representations of Molecules</h2>
<h2 id="references-and-further-reading">References and Further Reading</h2>
<ol>
<li>
<p>A. v. d. Oord, O. Vinyals, and K. Kavukcuoglu. <a href="http://arxiv.org/abs/1711.00937">Neural Discrete Representation Learning.</a>. NeurIPS 2017.</p>
</li>
<li>
<p>ESM3 (or ESMFold)</p>
</li>
</ol>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The term &ldquo;latent&rdquo; means hidden; we generally cannot observe these variables.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>The marginal distribution $p(\mathbf{x})$ is also known as the &ldquo;evidence&rdquo;. Thus, since the KL divergence is always positive, we see that $\log p(\mathbf{x}) \geq \text{ELBO}$, hence the name.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    
  </div>

  

  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      
<style>
  .katex a {
    text-decoration: none;
    color: inherit;
  }
  .katex a:hover {
    text-decoration: none;
  }
</style>

<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ],
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{\\text{#1}}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}",
        "\\R": "\\mathbb{R}",
        "\\bR": "\\mathbf{R}",
        "\\C": "\\mathbb{C}",
        "\\Z": "\\mathbb{Z}",
        "\\N": "\\mathbb{N}",
        "\\Q": "\\mathbb{Q}",
        "\\E": "\\mathbb{E}",
        "\\cD": "\\mathcal{D}",
        "\\var": "\\operatorname{Var}",
        "\\cov":"\\operatorname{cov}",
        "\\x": "\\mathbf{x}",
        "\\X": "\\mathbf{X}",
        "\\w": "\\mathbf{w}",
        "\\W": "\\mathbf{W}",
        "\\y": "\\mathbf{y}",
        "\\z": "\\mathbf{z}",
        "\\Z": "\\mathbf{Z}",
        "\\u": "\\mathbf{u}",
        "\\U": "\\mathbf{U}",
        "\\V": "\\mathbf{V}",
        "\\I": "\\mathbf{I}",
        "\\A": "\\mathbf{A}",
        "\\a": "\\mathbf{a}",
        "\\B": "\\mathbf{B}",
        "\\b": "\\mathbf{b}",
        "\\c": "\\mathbf{c}",
        "\\D": "\\mathbf{D}",
        "\\M": "\\mathbf{M}",
        "\\m": "\\mathbf{m}",
        "\\bC": "\\mathbf{C}",
        "\\J": "\\mathbf{J}",
        "\\K": "\\mathbf{K}",
        "\\L": "\\mathbf{L}",
        "\\bS": "\\mathbf{S}",
        "\\bmu": "\\boldsymbol{\\mu}",
        "\\bphi": "\\boldsymbol{\\phi}",
        "\\bepsilon": "\\boldsymbol{\\epsilon}",
        "\\bSigma": "\\boldsymbol{\\Sigma}",
        "\\bLambda": "\\boldsymbol{\\Lambda}",
        "\\bPhi": "\\boldsymbol{\\Phi}",
        "\\zero": "\\mathbf{0}",
        "\\one": "\\mathbf{1}",
        "\\T": "^{\\top}",
        "\\p": "^\\prime",
        "\\inv": "^{-1}",
        "\\ij": "_{ij}",
        "\\Norm": "\\mathcal{N}",
        "\\gam": "\\text{Gamma}",
        "\\nll": "\\text{NLL}",
        "\\argmin": "\\underset{#1}{\\operatorname{argmin}}",
        "\\argmax": "\\underset{#1}{\\operatorname{argmax}}",
        "\\diag": "\\operatorname{diag}",
        "\\tr": "\\operatorname{tr}",
        "\\pbmu": "\\frac{\\partial}{\\partial \\boldsymbol{\\mu}}",
        "\\pSigma": "\\frac{\\partial}{\\partial \\Sigma}",
        "\\pbx": "\\frac{\\partial}{\\partial \\mathbf{x}}",
        "\\px": "\\frac{\\partial}{\\partial x}",
        "\\pbA": "\\frac{\\partial}{\\partial \\mathbf{A}}",
        "\\ml": "_\\text{ML}",
      }
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>


<script>
  
  function updateFigureNumbers() {

      const figRefs = document.querySelectorAll('.fig-ref');
      figRefs.forEach(ref => {
          const figId = ref.getAttribute('href').slice(1);
          const figElement = document.getElementById(figId);
          if (figElement) {
              const figIndex = Array.from(figures).indexOf(figElement) + 1;
              ref.textContent = `Figure ${figIndex}`;
          }
      });
  }

  
  window.addEventListener('load', updateFigureNumbers);
</script>



</html>