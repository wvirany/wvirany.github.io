<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content=""/>

<title>
    
    Some math behind the Gaussian distribution | Walter Virany
    
</title>

<link rel="canonical" href="http://localhost:1313/posts/gaussian1/"/>












<link rel="stylesheet" href="/assets/combined.min.ef88cc3bb753e21b57298cf2932226b881ffb795068f96a4a9137efb27155f78.css" media="all">





  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">Walter Virany</h1>
    
    
    

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /blog
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/me" >
                /me
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/why" >
                /why?
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        







<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">Some math behind the Gaussian distribution</h1>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2025-05-01T00:00:00&#43;00:00">May 1, 2025</time>
      

      
      &nbsp; Â· &nbsp;
      27 min read
      
    </p>

  </div>

  

  

  
  <aside class="toc">
    <p><strong>Table of contents</strong></p>
    <nav id="TableOfContents">
  <ul>
    <li><a href="#properties">Properties</a>
      <ul>
        <li><a href="#normalization">Normalization</a></li>
        <li><a href="#moments">Moments</a></li>
      </ul>
    </li>
    <li><a href="#conditioning">Conditioning</a></li>
    <li><a href="#marginalization">Marginalization</a></li>
    <li><a href="#transformations">Transformations</a>
      <ul>
        <li><a href="#affine-transformations">Affine transformations</a></li>
        <li><a href="#sums-of-gaussians">Sums of Gaussians</a></li>
      </ul>
    </li>
    <li><a href="#references-and-further-reading">References and further reading</a></li>
    <li><a href="#appendix">Appendix</a>
      <ul>
        <li><a href="#short-proofs">Short proofs</a></li>
      </ul>
    </li>
  </ul>
</nav>
  </aside>
  

  

  <div class="single-content">
    <!-- #### TO DO:
- Change \Sigma to \bSigma?
- Can we make eqref display the tag of the equation instead of the label? -> Ask Claude
- Where can I put CSS?
- How can I edit katex-display CSS in themes/assets/main.css? Maybe this can fix cutoff issues
- Comment on how I'm trying to keep this self-contained?
- FIX RESULT FOR LINEAR GAUSSIAN SYSTEM: conditional mean is wrong
- Fix formatting
- Coherence between appendix and main blog
- Format appendix like the examples
- Links to examples and include them in TOC?
- Equation numbers for results from section on Bayes' rule for linear Gaussian systems
- Change TOC: maybe use same style as example boxes
- Fix equation numbers
- Examples
- Fill in appendix, proofs
- Discussion of Mahalanobis distance, Z-score, reconcile with example 1
- Comment about our frequent strategy of removing constants from sums in the exponent because they just become multiplicative scalars, which we account for add the end by normalizing
- Interpreting the moments!
- Double-check all results
- Tops of partials cut off (e.g., jacobian, matrix derivative rules) (and some fractions! e.g., see posteriors of univariate Gaussian)
- Conditioning: Add the other forms of the conditional params?
- Add all the results at the top for quick reference, or maybe just a nice TOC
- In "moments": show the formula for the second moment of a univariate Gaussian, and reference it properly (whether it's a footnote or appendix)
- Sum and product rule of probability in appendix + references to them
- Proper way to reference derivative rules for MLE section
 -->
<style>
  details {
    border: 1px solid black;
    border-radius: 8px;
    padding: 0.5em 0.5em 0em;
    margin-bottom: 2em;
    margin-top: 2em;
  }
  
  summary {
    font-weight: bold;
    margin: -0.5em -0.5em 0;
    padding: 0.5em;
    cursor: pointer;
    border-bottom: 1px solid #aaa;
    border-radius: 8px 8px 0 0;
  }
  
  details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: 0;
  }
  
  details p {
    padding-top: 0.1em;
  }
</style>
<p>Despite its ubiquity, I have frequently found myself in a state somewhere between <em>discomfort</em> and <em>panic</em> each time I am faced with the task of manipulating the Gaussian distribution, particularly in multiple dimensions. So, I&rsquo;ve taken the opportunity to work through some detailed derivations involving the Gaussian.</p>
<p>In this blog, I focus on the multivariate Gaussian distribution, beginning by reasoning about its shape and properties in high dimensions. Then, I derive some useful formulas, such as conditioning, marginalization, and transformations. I also include several examples along the way.</p>
<h2 id="properties">Properties</h2>
<p>In this section, I&rsquo;ll start by considering some basic properties of the Gaussian distribution. First, I&rsquo;ll show that surfaces on which the likelihood is constant form ellipsoids. Then, we&rsquo;ll see that the multivariate Gaussian distribution is indeed normalized, making it a valid probability distribution. Finally, we&rsquo;ll consider the first- and second-order moments of the multivariate Gaussian distribution in order to provide an interpretation of its parameters.</p>
<p>We write the Gaussian distribution for a random vector $\x \in \R^d$ as</p>
<p>$$
\begin{equation*}
\Norm(\x \mid \bmu, \Sigma) = \frac{1}{(2\pi)^{d/2}\lvert \Sigma \rvert^{1/2}}\exp\left( -\frac{1}{2} (\x - \bmu)\T\Sigma\inv(\x - \bmu)\right),
\end{equation*}
$$</p>
<p>where $\bmu \in \R^d$ is the mean vector and $\Sigma \in \R^{d\times d}$ is the covariance matrix. It&rsquo;s often nicer to work with the quadratic form in the exponent, which we define</p>
<p>$$
\begin{equation}\label{eq:mahalanobis}
\Delta^2 = (\x - \bmu)\T\Sigma\inv(\x-\bmu).
\end{equation}
$$</p>
<p>$\Delta$ is called the <em>Mahalanobis distance</em>, and is analagous to the z-score of a univariate Gaussian random variable $X$:</p>
<p>$$
\begin{equation*}
Z = \frac{X - \mu}{\sigma}.
\end{equation*}
$$</p>
<p>The z-score measures the number of standard deviations a X is from the mean. It&rsquo;s also interesting to note that the Mahalanobis distance reduces to the Euclidean distance when $\Sigma$ is the identity matrix.</p>
<p>Since the covariance matrix $\Sigma$ is real and symmetric, we can perform <a href="#eigenvalue-decomposition">eigenvalue decomposition</a> to write it in the form</p>
<p>$$
\begin{align*}
\Sigma &amp;=  \U\Lambda\U\T \\
&amp;=\sum_{i=1}^d\lambda_i\u_i\u_i\T,
\end{align*}
$$</p>
<p>Here, $\U$ is the matrix whose rows are given by $\u_i\T$, the eigenvectors of $\Sigma$, and $\Lambda = \diag(\lambda_1, \lambda_2, \ldots, \lambda_d)$ contains the corresponding eigenvalues. Note that we can choose the eigenvectors to be orthonormal (see ), i.e., <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>$$
\begin{equation*}
\u_i\T\u_j = \delta_{ij}.
\end{equation*}
$$</p>
<p>Thus, $\U$ is an orthogonal matrix, so $\U\U\T = \I$, and thus $\U\T = \U\inv.$ Moreover, we can easily write the inverse of the covariance matrix as</p>
<p>$$
\begin{equation*}
\Sigma\inv = \sum_{i=1}^d\frac{1}{\lambda_i}\u_i\u_i\T.
\end{equation*}
$$</p>
<p>Substituting this into $\eqref{eq:mahalanobis}$, we get</p>
<p>$$
\begin{align*}
\Delta^2 &amp;= \sum_{i=1}^d \frac{1}{\lambda_i}(\x - \bmu)\T\u_i\u_i\T(\x - \bmu) \\[1pt]
&amp;= \sum_{i=1}^d\frac{y_i^2}{\lambda_i},
\end{align*}
$$</p>
<p>where I&rsquo;ve introduced</p>
<p>$$
\begin{equation*}
y_i = \u_i\T(\x - \bmu),
\end{equation*}
$$</p>
<p>The set $\{y_i\}$ can then be seen as a transformed coordinate system, shifted by $\bmu$ and rotated by $\u_i.$<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Alternatively, we can write this as a vector:</p>
<p>$$
\begin{equation}\label{eq:ytransform}
\y = \U(\x - \bmu),
\end{equation}
$$</p>
<p>Now, all of the dependence of the Gaussian on $\x$ is determined by $\Delta^2.$ Thus, the Gaussian is constant on surfaces for which $\Delta^2$ is constant. Then, let</p>
<p>$$
\begin{equation*}
\Delta^2 = \sum_{i=1}^d\frac{y_i^2}{\lambda_i} = r
\end{equation*}
$$</p>
<p>for some constant $r.$ This defines the equation of an ellipsoid in $d$ dimensions.</p>
<details>
  <summary>Example: Level sets of the Gaussian</summary>
  <p>
  In the case of the Gaussian distribution, we often like to talk about the probability that an observation will fall within some range of values. For example, we might like to know the probability that a random variable will fall within one standard deviation from the mean.
<p>As an example, I&rsquo;ll consider the analagous case for a bivariate Gaussian, in which we would like to find the ellipses corresponding to the probabilities that a point falls within one, two, or three standard deviations from the mean.</p>
<p>First consider a univariate Gaussian random variable $X \sim \Norm(\mu, \sigma^2).$ The probability that $\X$ is within one standard deviation from the mean is given by</p>
<p>$$
\begin{align*}
P(\lvert X - \mu \rvert \leq \sigma) &amp;= P(-\sigma \leq X - \mu \leq \sigma) \\
&amp;= P(-1 \leq \frac{X - \mu}{\sigma} \leq 1) \\
&amp;= P(-1 \leq Z \leq 1),
\end{align*}
$$</p>
<p>where $Z = \frac{X - \mu}{\sigma}$ is a standard Normal random variable. Then, this probability is given by</p>
<p>$$
\begin{align*}
P(-1 \leq Z \leq 1) &amp;= P(Z \leq 1) - P(Z \leq -1) \\
&amp;= \Phi(1) - \Phi(-1)
\end{align*}
$$</p>
<p>where $\Phi(\cdot)$ is the cumulative distribution function of $Z$, for which the functional values are usually determined from a <a href="https://engineering.purdue.edu/ChanGroup/ECE302/files/normal_cdf.pdf">table</a>, or when in doubt, we can make the computers think for us:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">from</span> scipy.stats <span style="color:#00f">import</span> norm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#a31515">f</span><span style="color:#a31515">&#34;</span><span style="color:#a31515">{</span>norm.cdf(1) - norm.cdf(-1)<span style="color:#a31515">:</span><span style="color:#a31515">.3f</span><span style="color:#a31515">}</span><span style="color:#a31515">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>0.683
</span></span></code></pre></div><p>We can do this in a similar fashion to find the probabilities that $X$ falls within $2\sigma$ or $3\sigma$ from the mean, for which the values are approximately $0.954$ and $0.997$, respectively.</p>
<p>In the univariate case, the quantity $\lvert X - \mu \rvert / \sigma$ measures the number of standard deviations $X$ is from the mean. The Mahalanobis is analgous to this in the multivariate case. However, it&rsquo;s important to note that, in multiple dimensions, the number of standard deviations a random vector $\x$ is from the mean depends on the <em>direction</em> $\x$ is with respect to the mean. For example, a Gaussian will in general have different variances along different dimensions, and thus, depending on the direction, $\x$</p>
<p>Thus, we seek to find some constant $k$ for which</p>
<p>$$
\begin{equation*}
P(\Delta^2 \leq k^2) = 0.683.
\end{equation*}
$$</p>
<p>To do so, we note that $\Delta^2$ follows a chi-squared distribution. To see this, recall the expression for $\Delta^2$:</p>
<p>$$
\begin{align*}
\Delta^2 &amp;= (\x - \bmu)\T\U\T\Lambda\inv\U(\x - \bmu) \\
&amp;= \y\T\Lambda\inv\y.
\end{align*}
$$</p>
<p>Then, $\y$ is a random vector with zero mean and diagonal covariance $\Lambda.$ Since it has diagonal covariance, the elements of $\y$ are uncorrelated. In general, uncorrelated does not imply independence; however, <a href="#1">in the case of the Gaussian, it does</a>. Then, consider yet another transformation:</p>
<p>$$
\begin{equation*}
\z = \Lambda^{-1/2}\y,
\end{equation*}
$$</p>
<p>where $\Lambda^{-1/2} = \diag(\lambda_1^{-1/2}, \ldots, \lambda_d^{-1/2}).$ Now, the elements of $\z$ have been standardized, so $\z$ is a vector of standard Normal random variables. Then, we have</p>
<p>$$
\begin{align*}
\Delta^2 &amp;= \z\T\z \\
&amp;= z_1^2 + z_2^2 + \cdots + z_d^2.
\end{align*}
$$</p>
<p>Since each $z_i$ is an independent standard Normal, the sum $\Delta^2$ takes a chi-squared distribution with $d$ degrees of freedom. Then, consider the cumulative distribution function of a chi-squared random variable with $d=2$:</p>
<p>$$
\begin{equation*}
F_{\chi^2_2} (x) = P(\chi^2 \leq x).
\end{equation*}
$$</p>
<p>In this case, we know $F_{\chi^2_2} (x) = 0.683$, and we wish to find x. To do so, we can make use of the inverse cumulative distribution function, otherwise known as the <em>quantile function</em>:</p>
<p>$$
\begin{equation*}
k = F_{\chi^2_2}\inv (p) = Q(p).
\end{equation*}
$$</p>
<p>We can evaluate this in several ways, but the cumulative distribution function of $\chi^2_2$ takes a nice form, so we&rsquo;ll do it by hand:</p>
<p>$$
\begin{equation*}
F_{\chi^2_2}(x) = 1 - e^{-x/2}
\end{equation*}
$$</p>
<p>Hence,</p>
<p>$$
\begin{align*}
Q(p) &amp;= \log \frac{1}{(1 - p)^2} \\
Q(0.683) &amp;\approx 2.30.
\end{align*}
$$</p>
<p>Thus, the value of $k$ for which $P(\Delta^2 \leq k) = 0.683$ is approximately $2.30$. The equation for the corresponding ellipse in $y$-space is then given by</p>
<p>$$
\begin{equation*}
\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 2.30,
\end{equation*}
$$</p>
<p>which is an axis-aligned ellipse with semi-major and semi-minor axes given by $\sqrt{2.30\lambda_1}$ and $\sqrt{2.30\lambda_2}$ (the larger of the two being semi-major, the smaller being semi-minor). We can similarly evaluate $Q(.954)$ and $Q(.997)$, which give approximately $2.67$ and $5.05$, respectively.</p>
<p>In order to transform this to $x$-space, we have the inverse transformation of $\eqref{eq:ytransform}$:</p>
<p>$$
\begin{equation*}
\x = \U\T\y + \bmu.
\end{equation*}
$$</p>
<p>Now, suppose $p(\x)$ is a Gaussian with the following parameters:</p>
<p>$$
p(\x) = \Norm\left(
\begin{bmatrix}
0.4 \\
0.7
\end{bmatrix},
\begin{bmatrix}
1.0 &amp; 0.3 \\
0.3 &amp; 0.8 \\
\end{bmatrix}\right).
$$</p>
<p>I&rsquo;ll use <code>numpy</code> to solve for the eigenvalues:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">import</span> numpy a np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Define the mean and covariance matrix</span>
</span></span><span style="display:flex;"><span>mean = np.array([0.4, 0.7])
</span></span><span style="display:flex;"><span>cov = np.array([[1.0, 0.3], [0.3, 0.8]])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Calculate eigenvalues and eigenvectors</span>
</span></span><span style="display:flex;"><span>eigenvalues, eigenvectors = np.linalg.eigh(cov)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(eigenvalues.round(3))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[0.584 1.216]
</span></span></code></pre></div><p>Now, we can plot the ellipses in $y$-space:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"></code></pre></div>  </p>
</details>
<h3 id="normalization">Normalization</h3>
<p>Now, our goal is to show that the multivariate Gaussian distribution is normalized. Let&rsquo;s consider the Gaussian in the new coordinate system $\{y_i\}.$ Rearranging $\eqref{eq:ytransform}$, we can write the transformation as</p>
<p>$$
\begin{equation*}
\x = g(\y) = \U\T\y + \bmu.
\end{equation*}
$$</p>
<p>Then, to transform from $\x$-space to $\y$-space, we use the <a href="#change-of-variables">change of variables formula</a>, given by</p>
<p>$$
\begin{align*}
p_y(\y) &amp;= p_x(\x)\lvert \J \rvert \\[2pt]
&amp;= p_x(g(\y))\lvert \J \rvert.
\end{align*}
$$</p>
<p>Here, $\J$ is the Jacobian whose elements are given by</p>
<p>$$
\begin{equation*}
J_{ij} = \frac{\partial x_i}{\partial y_j}.
\end{equation*}
$$</p>
<p>The derivative of $\x$ with respect to $\y$ is $\U\T$, hence the elements of $\J$ are</p>
<p>$$
J_{ij} = U_{ji}.
$$</p>
<p>Then, to find the determinant of the Jacobian, we have</p>
<p>$$
\lvert \J \rvert ^2 = \lvert \U\T \rvert ^2 = \lvert \U\T \rvert \lvert \U \rvert = \lvert \U\T\U \rvert = \lvert \I \rvert = \mathbf{1}.
$$</p>
<p>Thus, $\lvert \J \rvert = \mathbf{1}$, making our transformation</p>
<p>$$
p_y(\y) = p_x(g(\y)).
$$</p>
<p>Then, we can write the Gaussian in terms of $\y$ as</p>
<p>$$
\begin{align*}
p_y(\y) &amp;= \frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}}\exp\left( -\frac{1}{2} (\x - \bmu)\T\Sigma\inv(\x - \bmu) \right).
\end{align*}
$$</p>
<p>Examining the term in the exponent, we have</p>
<p>$$
\begin{align*}
(\x - \bmu)\T\Sigma\inv(\x - \bmu)  &amp;= (\U\T\y)\T \Sigma\inv(\U\T\y) \\[1pt]
&amp;= \y\T\U (\U\T\Lambda\U)\inv \U\T\y \\[1pt]
&amp;= \y\T\U \U\inv\Lambda (\U\T)\inv \U\T\y \\[1pt]
&amp;= \y\T\Lambda\y.
\end{align*}
$$</p>
<p>So,</p>
<p>$$
\begin{align*}
p_y(\y) &amp;= \frac{1}{(2\pi)^{d/2}\lvert \Sigma \rvert ^{1/2}} \exp \left( -\frac{1}{2} \y\T\Lambda\y \right) \nonumber \\[1pt]
&amp;= \frac{1}{(2\pi)^{d/2}\lvert \Sigma \rvert ^{1/2}} \exp \left( -\frac{1}{2} \sum_{i=1}^d \frac{y_i^2}{\lambda_i} \right).
\end{align*}
$$</p>
<p>Then, it&rsquo;s useful to show that</p>
<p>$$
\begin{align*}
\lvert \Sigma \rvert &amp;= \lvert \U\Lambda\U\T \rvert = \lvert \U \rvert \lvert \Lambda \rvert \lvert \U\T \rvert = \lvert \Lambda \rvert = \prod_{i=1}^d \lambda_i,
\end{align*}
$$</p>
<p>hence</p>
<p>$$
\frac{1}{\lvert \Sigma \rvert^{1/2}} = \prod_{i=1}^d \frac{1}{\sqrt{\lambda_i}}.
$$</p>
<p>Thus, noting that the exponent of a sum becomes a product of exponents, we have</p>
<p>$$
p_y(\y) = \prod_{i=1}^d \frac{1}{\sqrt{2\pi\lambda_i}} \exp \left( -\frac{y_i^2}{2\lambda_i} \right).
$$</p>
<p>Then,</p>
<p>$$
\begin{align*}
\int_\y p_y(\y) d\y &amp;= \prod_{i=1}^d \int_{y_i}  \frac{1}{\sqrt{2\pi\lambda_i}} \exp \left( -\frac{y_i^2}{2\lambda_i} \right) dy_i.
\end{align*}
$$</p>
<p>We see that each element of the product is just a univariate Gaussian over $y_i$ with mean $0$ and variance $\lambda_i$, each of which integrates to 1. This shows that $p_y(\y)$ and thus $p_x(\x)$ is indeed normalized.</p>
<h3 id="moments">Moments</h3>
<p>Finally, we will examine the first and second moments of the Gaussian. The first moment is given by</p>
<p>$$
\begin{align*}
\E[\x] &amp;= \frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2} (\x - \bmu)\T\Sigma\inv(\x - \bmu) \right) \x \, d\x \\[3pt]
&amp;= \frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2} \z\T\Sigma\inv\z \right) (\z + \bmu ) \, d\z,
\end{align*}
$$</p>
<p>where I&rsquo;ve introduced the change of variables $\z = \x - \bmu.$ We can split this up as</p>
<p>$$
\frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}} \left[ \int\exp\left( -\frac{1}{2} \z\T\Sigma\inv\z \right) \z \, d\z + \int\exp\left( -\frac{1}{2} \z\T\Sigma\inv\z \right) \bmu \, d\z\right].
$$</p>
<p>Inspecting the first term, we see that $\exp(-\frac{1}{2}\z\T\Sigma\inv\z)$ is an even function in $\z$, and $\z$ is odd. Then, the product is an odd function, so the integral over a symmetric domain (in this case all of $\R^d$) is zero. The second term is just $\bmu$ times a Gaussian, which will integrate to 1 when multiplied by the normalization constant. Thus, we have the (perhaps unsurprising) result:</p>
<p>Now, in the univariate case, the second moment is given by $\E[x^2].$ In the multivariate case, there are $d^2$ second moments, each given by $\E[x_i, x_j]$ for $i, j \in [d].$ We can group these together to form the matrix $\E[\x\x\T].$ We write this as</p>
<p>$$
\begin{align}
\E[\x\x\T] &amp;= \frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2}(\x - \bmu)\T\Sigma\inv (\x-\bmu) \right) \x\x\T d\x \nonumber \\[3pt]
&amp;= \frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2}\z\T\Sigma\inv \z \right) (\z + \bmu)(\z + \bmu)\T d\z \nonumber \\[3pt]
&amp;= \frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2}\z\T\Sigma\inv \z \right) (\z\z\T + 2\z\T\bmu + \bmu\bmu\T)  d\z.
\end{align}
$$</p>
<p>By the same arguments as before, the term involving $\z\T\bmu$ will vanish due to symmetry, and the term involving $\bmu\bmu\T$ will integrate to $\bmu\bmu\T$ due to normalization. Then, we are left with the term involving $\z\z\T.$ Using the eigenvalue decomposition of $\Sigma$, we can write</p>
<p align=center>
$\y = \U\z, \quad$ or $\quad \z = \U\T\y.$
</p>
<p>Recall that $\U$ is the matrix whose rows are given by the eigenvectors of $\Sigma.$ So, $\U\T$ is the matrix whose <em>columns</em> are given by the eigenvectors. Thus,</p>
<p>$$
\begin{align*}
\z &amp;= \begin{bmatrix}
\u_1 &amp; \u_2 &amp; \cdots &amp; \u_d
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_d
\end{bmatrix} \\[3pt]
&amp;= \begin{bmatrix}
u_{11} &amp; u_{21} &amp; \cdots &amp; u_{d1} \\
u_{12} &amp; u_{22} &amp; \cdots &amp; u_{d2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
u_{1d} &amp; u_{2d} &amp; \cdots &amp; u_{dd} \\
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_d
\end{bmatrix} \\[3pt]
&amp;= \begin{bmatrix}
u_{11}y_1 + u_{21}y_2 + \cdots + u_{d1}y_d \\
u_{12}y_1 + u_{22}y_2 + \cdots + u_{d2}y_d \\
\vdots \\
u_{1d}y_1 + u_{2d}y_2 + \cdots + u_{dd}y_d \\
\end{bmatrix} = \sum_{i=1}^d y_i\u_i,
\end{align*}
$$</p>
<p>where $u_{ij}$ is the $j$th element of $\u_i.$ Then, using this expression for $\z$, and recalling the form for $p_y(\y)$ in $(3)$, we can write the first term of $(4)$ as</p>
<p>$$
\begin{align}
&amp;\frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}} \int \exp \left( - \sum_{k=1}^d \frac{y_k^2}{2\lambda_k} \right) \sum_{i=1}^d\sum_{j=1}^d y_i y_j \u_i\u_j\T d\y \nonumber \\[2pt]
&amp;\qquad= \frac{1}{(2\pi)^{d/2}\lvert\Sigma\rvert^{1/2}} \sum_{i=1}^d\sum_{j=1}^d \u_i\u_j\T  \int \exp \left( - \sum_{k=1}^d \frac{y_k^2}{2\lambda_k} \right) y_i y_j d\y.
\end{align}
$$</p>
<p>Now, the integral takes the form</p>
<p>$$
\begin{align*}
\int \exp \left( - \sum_{k=1}^d y_k^2\right) y_i y_j d\y.
\end{align*}
$$</p>
<p>When $i\neq j$, we can expand this as the product</p>
<p>$$
\begin{align*}
&amp; \prod_{k=1}^d \int \exp(-y_k^2) y_i y_j d\y \\[2pt]
&amp;= \int \! \exp(-y_1^2) dy_1 \cdots \! \int \exp(-y_i^2) y_i dy_i \cdots \int \! \exp(-y_j^2) y_j dy_j \cdots \int \! \exp(-y_d^2) dy_d.
\end{align*}
$$</p>
<p>In this case, due to our symmetry arguments, the terms involving $y_i$ and $y_j$ vanish, and hence the integral vanishes when $i\neq j.$ If $i=j$, then the second term in $(5)$ can be written as</p>
<p>$$
\sum_{i=1}^d \u_i\u_i\T \prod_{k=1}^d \int \frac{1}{\sqrt{2\pi\lambda_k}} \exp \left( - \frac{y_k^2}{2\lambda_k} \right) y_i^2 dy_k.
$$</p>
<p>where we brought the normalization constant inside the product. The terms in the product for which $i \neq k$ are just univariate Gaussian, and hence normalize to $1.$ Thus, the only term left in the product is</p>
<p>$$
\int \frac{1}{\sqrt{2\pi\lambda_k}} \exp \left( - \frac{y_i^2}{2\lambda_i} \right) y_i^2 dy_i,
$$</p>
<p>which is just the expression for the second moment of a univariate Gaussian with mean $0$ and variance $\lambda_i.$ In general, the second moment of a univariate Gaussian $\Norm(x \mid \mu, \sigma^2)$ is $\mu^2 + \sigma^2.$ Thus, we are left with</p>
<p>$$
\begin{align}
\E[\x\x\T] &amp;= \bmu\bmu\T + \sum_{i=1}^d \u_i\u_i\T \lambda_i \nonumber \\[1pt]
&amp;= \bmu\bmu\T + \Sigma.
\end{align}
$$</p>
<p>So, we have that the first and second moments of the Gaussian are given by $\E[\x] = \bmu$ and $\E[\x\x\T] = \bmu\bmu\T + \Sigma$, respectively.</p>
<h2 id="conditioning">Conditioning</h2>
<p>Now, suppose we have some random vector $\z \in \R^d$, specified by a Gaussian distribution:</p>
<p>$$
\z \sim \Norm(\z \mid \bmu, \Sigma).
$$</p>
<p>Then, suppose we partition $\z$ into two disjoint vectors $\x \in \R^m$ and $\y \in \R^{d-m}$:</p>
<p>$$
\z = \begin{pmatrix}
\x \\
\y
\end{pmatrix},
$$</p>
<p>and our goal is to find an expression for the conditional distribution $p(\x \mid \y).$ The parameters specifying the joint distribution can likewise be partitioned as follows:</p>
<p>$$
\bmu\ = \begin{pmatrix}
\bmu_x \\
\bmu_y
\end{pmatrix}, \quad
\Sigma = \begin{pmatrix}
\Sigma_{xx} &amp; \Sigma_{xy} \\
\Sigma_{yx} &amp; \Sigma_{yy}
\end{pmatrix}.
$$</p>
<p>Note that since $\Sigma$ is symmetric, we have $\Sigma_{xx} = \Sigma_{xx}\T, \Sigma_{yy} = \Sigma_{yy}\T$, and $\Sigma_{xy} = \Sigma_{yx}\T.$ It&rsquo;s also useful to define the precision matrix $\Lambda = \Sigma\inv$, and its partitioned form: <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>$$
\Lambda = \begin{pmatrix}
\Lambda_{xx} &amp; \Lambda_{xy} \\
\Lambda_{yx} &amp; \Lambda_{yy}
\end{pmatrix}.
$$</p>
<p>Since the inverse of a symmetric matrix is itself symmetric, we have that $\Lambda = \Lambda\T$, hence the same properties hold as the covariance matrix regarding the symmetry between constituent parts of the partitioned matrix. However, it&rsquo;s important to note that the partitioned matrices of the precision matrix are not simply the inverses of the corresponding elements of the covariance matrix. Instead, we&rsquo;ll shortly see how to take the inverse of a partitioned matrix.</p>
<p>Now, one way to find an expression for the conditional $p(\x \mid \y)$ would be to simply use the <a href="#sum-and-product-rules-of-probability">product rule of probability</a>:</p>
<p>$$
\begin{align*}
p(\x, \y) &amp;= p(\x \mid \y) \, p(\y) \\[3pt]
\Rightarrow \quad p(\x \mid \y) &amp;= \frac{p(\x, \y)}{p(\y)}.
\end{align*}
$$</p>
<p>However, normalizing the resulting expression can be cumbersome. Instead, let&rsquo;s consider the quadratic form in the exponent of the joint distribution:</p>
<p>$$
\begin{align}
&amp; -\frac{1}{2}(\z - \bmu)\T\Sigma\inv (\z - \bmu) \nonumber \\[10pt]
&amp;\qquad= -\frac{1}{2} \begin{pmatrix}
\x - \bmu_x \\
\y - \bmu_y
\end{pmatrix}\T
\begin{pmatrix}
\Lambda_{xx} &amp; \Lambda_{xy} \\
\Lambda_{yx} &amp; \Lambda_{yy}
\end{pmatrix}
\begin{pmatrix}
\x - \bmu_x \\
\y - \bmu_y
\end{pmatrix} \nonumber \\[15pt]
&amp;\qquad= -\frac{1}{2} (\x - \bmu_x)\T\Lambda_{xx} (\x - \bmu_x) - \frac{1}{2}(\x - \bmu_x)\T \Lambda_{xy} (\y - \bmu_y) \nonumber \\
&amp;\qquad\qquad - \frac{1}{2}(\y-\bmu_y)\T\Lambda_{yx} (\x - \bmu_x) - \frac{1}{2} (\y - \bmu_y)\T \Lambda_{yy} (\y - \bmu_y) \nonumber \\[10pt]
&amp;\qquad= -\frac{1}{2} (\x - \bmu_x)\T\Lambda_{xx} (\x - \bmu_x) - (\x-\bmu_x)\T\Lambda_{xy} (\y - \bmu_y) \nonumber \\
&amp;\qquad\qquad - \frac{1}{2} (\y - \bmu_y)\T \Lambda_{yy} (\y - \bmu_y).
\end{align}
$$</p>
<p>In the last line, I use the fact that <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
<p>$$
(\x - \bmu_x)\T\Lambda_{xy}  (y - \bmu_y) = (\y - \bmu_y)\T\Lambda_{yx}  (\x - \bmu_x).
$$</p>
<p>I&rsquo;ll repeatedly use this fact in the following calculations to combine cross terms.</p>
<p>Evaluating the conditional $p(\x \mid \y)$ involves fixing $\y$ and treating this as a function of $\x.$ Then, since the expression in $(6)$ is a quadratic function of $\x$, the resulting distribution $p(\x \mid \y)$ will also take the form of a Gaussian. So, our goal is to find the mean $\bmu_{x\mid y}$ and covariance $\Sigma_{x\mid y}$ which specify this distribution. To do so, note that in general, we can write the exponent of a Gaussian as</p>
<p>$$
\begin{equation}\label{eq:generalgaussian}
-\frac{1}{2}(\z - \bmu)\T \Sigma\inv (\z - \bmu) = -\frac{1}{2}\z\T\Sigma\inv\z + \z\T\Sigma\inv\bmu\ + c,
\end{equation}
$$</p>
<p>where $c$ denotes all the terms independent of $\z.$ Thus, if we can rewrite $(6)$ in this form, we can identify the coefficients of the quadratic and linear terms in $\x$ as the mean and covariance of $p(\x \mid \y).$ This may not seem clear at first, but I think going through the process will illuminate things.</p>
<p>Expanding $(6)$ gives</p>
<p>$$
-\frac{1}{2} \x\T\Lambda_{xx} \x + \x\T\Lambda_{xx} \bmu_x - \x\T\Lambda_{xy} \y + \x\T\Lambda_{xy} \bmu_y + c,
$$</p>
<p>where $c$ again denotes all terms which do not depend on $\x.$ Equating this to the general form as in the right-hand side of $\eqref{eq:generalgaussian}$, we have</p>
<p>$$
-\frac{1}{2} \x\T\Lambda_{xx} \x + \x\T\Lambda_{xx} \bmu_x - \x\T\Lambda_{xy} \y + \x\T\Lambda_{xy} \bmu_y = -\frac{1}{2}\x\T\Sigma_{x\mid y}\inv \x + \x\T\Sigma_{x\mid y}\inv \bmu_{x \mid y}.
$$</p>
<p>Immediately, we can equate the quadratic terms to see that</p>
<p>$$
\begin{equation}
\Sigma_{x\mid y}\inv = \Lambda_{xx}.
\end{equation}
$$</p>
<p>Then, collecting the linear terms, we have</p>
<p>$$
\x\T\Lambda_{xx} \bmu_x - \x\T\Lambda_{xy} \y + \x\T \Lambda_{xy} \bmu_y = \x\T\left( \Lambda_{xx} \bmu_x - \Lambda_{xy}(\y - \bmu_y) \right).
$$</p>
<p>Thus, we have</p>
<p>$$
\Sigma_{x\mid y}\inv \bmu_{x\mid y} = \Lambda_{xx} \bmu_x - \Lambda_{xy} (\y - \bmu_y),
$$</p>
<p>or, using $(8)$:</p>
<p>$$
\begin{equation}
\bmu_{x\mid y} = \bmu_x - \Lambda_{xx}\inv\Lambda_{xy} (\y - \bmu_y).
\end{equation}
$$</p>
<p>Here, we&rsquo;ve expressed the quantities $\bmu_{x\mid y}$ and $\Sigma_{x\mid y}$ in terms of $\Lambda.$ Instead, we can express them in terms of $\Sigma.$ To do so, we&rsquo;ll use the matrix inversion identity:</p>
<p>$$
\begin{pmatrix}
\A &amp; \B \\
\bC &amp; \D
\end{pmatrix}\inv = \begin{pmatrix}
\M &amp; -\M\B\D\inv \\
-\D\inv\bC\M &amp; \D\inv + \D\inv\bC\M\B\D
\end{pmatrix},
$$</p>
<p>where $\M$ is the <a href="#the-schur-complement">Schur complement</a>, defined</p>
<p>$$
\M = (\A - \B\D\inv\bC)\inv.
$$</p>
<p>Then, since</p>
<p>$$
\begin{pmatrix}
\Lambda_{xx} &amp; \Lambda_{xy} \\
\Lambda_{yx} &amp; \Lambda_{yy}
\end{pmatrix}\inv =
\begin{pmatrix}
\Sigma_{xx} &amp; \Sigma_{xy} \\
\Sigma_{yx} &amp; \Sigma_{yy}
\end{pmatrix},
$$</p>
<p>we have</p>
<p>$$
\begin{align*}
\Lambda_{xx} &amp;= (\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}\inv\Sigma_{yx})\inv, \\[4pt]
\Lambda_{xy} &amp;= - (\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}\inv\Sigma_{yx})\inv \Sigma_{xy} \Sigma_{yy}\inv.
\end{align*}
$$</p>
<p>Plugging these expressions into $(8)$ and $(9)$ gives</p>
<p>$$
\Sigma_{x\mid y} = \Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}\inv\Sigma_{yx}
$$</p>
<p>and</p>
<p>$$
\begin{align*}
\bmu_{x\mid y} &amp;= \bmu_x + (\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}\inv\Sigma_{yx}) (\Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}\inv\Sigma_{yx})\inv \Sigma_{xy} \Sigma_{yy}\inv (\y - \bmu_y) \\[2pt]
&amp;= \bmu_x - \Sigma_{xy}\Sigma_{yy}\inv (\y - \bmu_y).
\end{align*}
$$</p>
<p>Thus, $p(\x \mid \y)$ is the Gaussian distribution given by the following parameters:</p>
<p>$$
\begin{align}
\quad \bmu_{x\mid y} &amp;= \bmu_x + \Sigma_{xy}\Sigma_{yy}\inv(\y - \bmu_y) \quad \\[2pt]
\quad \Sigma_{x\mid y} &amp;= \Sigma_{xx} - \Sigma_{xy}\Sigma_{yy}\inv\Sigma_{yx}. \quad
\end{align}
$$</p>
<h2 id="marginalization">Marginalization</h2>
<p>Now, given the joint distribution $p(\x, \y)$ as above, suppose we wish to find the marginal distribution</p>
<p>$$
\begin{equation}
p(\x) = \int p(\x, \y) d\y.
\end{equation}
$$</p>
<p>Our goal, then, is to integrate out $\y$ to obtain a function of $\x.$ Then, we can normalize the resulting function of $\x$ to obtain a valid probability distribution. To do so, let&rsquo;s again consider the quadratic form in the exponent given by $(6).$ First, we collect all terms which depend on $\y$:</p>
<p>$$
\begin{align}
&amp;- (\x - \bmu_x)\T\Lambda_{xy} (\y - \bmu_y) - \frac{1}{2} (\y - \bmu_y)\T\Lambda_{yy} (\y - \bmu_y) \nonumber \\[2pt]
&amp;\qquad= -\frac{1}{2} \y\T \Lambda_{yy} \y + \y\T\Lambda_{yy} \bmu_y - \y\T \Lambda_{yx} (\x - \bmu_x) \nonumber \\[2pt]
&amp;\qquad= -\frac{1}{2}\y\T \Lambda_{yy} \y + \y\T \m,
\end{align}
$$</p>
<p>where I&rsquo;ve introduced</p>
<p>$$
\m = \Lambda_{yy} \bmu_y - \Lambda_{yx} (\x - \bmu_x).
$$</p>
<p>By <a href="#completing-the-square">completing the square</a>, we can write $(13)$ as</p>
<p>$$
-\frac{1}{2} (\y - \Lambda_{yy}\inv\m)\T \Lambda_{yy} (\y - \Lambda_{yy}\inv\m) + \frac{1}{2}\m\T\Lambda_{yy}\inv \m.
$$</p>
<p>Note that $\m$ does not depend on $\y$; however, it does depend on $\x.$ Now, we&rsquo;re able to factor the integral in $(11)$ as</p>
<p>$$
\exp\big( g(\x) \big)\int \exp \left\{ -\frac{1}{2} (\y - \Lambda_{yy}\inv\m)\T \Lambda_{yy} (\y - \Lambda_{yy}\inv\m) \right\} d\y,
$$</p>
<p>where $g(\x)$ contains all the remaining terms which do not depend on $\y.$ This integral is now easy to compute, since it is just an unnormalized Gaussian and will evaluate to the reciprocal of the corresponding normalization factor. Thus, the marginal distribution $p(\x)$ will have the exponential form given by $g(\x)$, and we can perform the same analysis by inspection to retrieve the values for the corresponding parameters $\bmu_x$ and $\Sigma_x.$</p>
<p>To acquire an expression for $g(\x)$, we consider all the remaining terms:</p>
<p>$$
\begin{align*}
g(\x) &amp;= -\frac{1}{2} (\x - \bmu_x)\T\Lambda_{xx} (\x - \bmu_x) + \x\T\Lambda_{xy} \bmu_y + \frac{1}{2}\m\T \Lambda_{yy}\inv \m \\[3pt]
&amp;= -\frac{1}{2} \x\T\Lambda_{xx} \x + \x\T \left( \Lambda_{xx} \bmu_x + \Lambda_{xy} \bmu_y \right) \\
&amp;\quad + \frac{1}{2} \bigg[ \big( \Lambda_{yy} \bmu_y - \Lambda_{yx}(\x-\bmu_x) \big)\T \Lambda_{yy}\inv \big( \Lambda_{yy} \bmu_y - \Lambda_{yx}(\x-\bmu_x) \big) \bigg].
\end{align*}
$$</p>
<p>Then, expanding this and dropping all constant terms with respect to $\x$, we have</p>
<p>$$
\begin{align*}
g(\x) &amp;= -\frac{1}{2}\x\T \left( \Lambda_{xx} + \Lambda_{xy}\Lambda_{yy}\inv\Lambda_{yx} \right) \x + \x\T\left( \Lambda_{xx} + \Lambda_{xy}\Lambda_{yy}\inv\Lambda_{yx} \right) \bmu_x \\[2pt]
&amp;= -\frac{1}{2}\x\T \Sigma_{xx}\inv \x + \x\T\Sigma_{xx}\inv\bmu_x,
\end{align*}
$$</p>
<p>where we have</p>
<p>$$
\Sigma_{xx} = \left( \Lambda_{xx} + \Lambda_{xy}\Lambda_{yy}\inv\Lambda_{yx} \right)\inv
$$</p>
<p>from the matrix inversion formula. Comparing this to our general form in $(7)$, we have the following expressions for the mean and covariance of the marginal $p(\x)$:</p>
<p>$$
\begin{align}
\E[\x] &amp;= \bmu_x \\
\cov[\x] &amp;= \Sigma_{xx}.
\end{align}
$$</p>
<p>That is, the mean and covariance of the marginal distribution are found by simply taking the &ldquo;slices&rdquo; of the partitioned matrices from the joint distribution which correspond to the marginal variable.</p>
<details>
  <summary>Example: Conditioning vs. marginalization</summary>
  <p>
  Let's consider a bivariate Gaussian distribution and see what happens when we condition vs. marginalize.
  </p>
</details>
<h2 id="transformations">Transformations</h2>
<p>Here I derive the resulting expressions for the new pdfs of the random variables which are the result of several different transformations of Gaussian random variables. Interestingly, we&rsquo;ll see that the Gaussian is closed under a variety of transformations.</p>
<h3 id="affine-transformations">Affine transformations</h3>
<p>Suppose $\x \in \R^d$, with $p(\x) = \Norm(\x \mid \bmu, \Sigma)$. Then, let $\y = \A\x + \b$, for some $\A \in \R^{n \times d}$ and $\b \in \R^n$. Now, we&rsquo;d like to find the density for $\y$.</p>
<p>The derivation is fairly straightforward. We start by using the change of variables formula:</p>
<p>$$
p(\y) = p(\x) \bigg| \frac{\partial\x}{\partial\y} \bigg|.
$$</p>
<p>Since this is an affine transformation, the Jacobian $\lvert \partial\x / \partial\y \rvert$ will be a constant, and hence is just a scaling factor. Thus, $p(\y)$ must take the same forma as $p(\x)$, i.e., a Gaussian.</p>
<p>Then, we can find the expressions for the mean and covariance of $p(\y)$. The mean is given by</p>
<p>$$
\bmu_y = \E[\A\x + \b] = \A \E[\x] + \b = \A \bmu + \b,
$$</p>
<p>and the covariance is given by</p>
<p>$$
\Sigma_y = \E[\y\y\T] - \bmu_y\bmu_y\T.
$$</p>
<p>Expanding the expression for the matrix of second moments gives</p>
<p>$$
\begin{align*}
\E[\y\y\T] &amp;= \E\left[ (\A\x+\b) (\A\x+\b)\T \right] \\
&amp;= \E[\A\x (\A\x)\T + 2 \b\T\A\x + \b\b\T] \\
&amp;= \A\E[\x\x\T]\A\T + 2\b\A\E[\x] + \b\b\T \\
&amp;= \A (\Sigma + \bmu\bmu\T) \A\T
\end{align*}
$$</p>
<h3 id="sums-of-gaussians">Sums of Gaussians</h3>
<p>The derivation for the sum of two Gaussian random variables is quite a bit more involved. For this case, I&rsquo;ll do the derivation for the univariate case, then generalize to multiple dimensions.</p>
<p>To start, note that the density function for the sum of any two independent random variables is given by their convolution. That is, suppose $X$ and $Y$ are independent random variables, with $X \sim f_x(x)$, and $Y \sim f_Y(y)$. Let $Z = X + Y$. Then,</p>
<p>$$
f_Z(z) = \int_{-\infty}^{\infty} f_X(x)f_Y(z-x) dx.
$$</p>
<p>To see this, consider the cumulative distribution function of $Z$:</p>
<p>$$
F_Z(z) = P(Z \leq z) = P(X + Y \leq z).
$$</p>
<p>Now, what is $P(X + Y \leq z)$? This is just the probability that two random sampled points $(X, Y)$, have a sum that is less than $z$. In other words, it&rsquo;s the</p>
<!-- ## Bayes' rule for linear Gaussian systems

Another useful result is Bayes' rule for linear Gaussian systems.[^fn5] In the previous sections, we had a random vector which specified a joint Gaussian distribution over $d$ random variables, and we wanted to find expressions for the marginal and conditional distributions of subsets of these random variables. Instead, suppose we are given a marginal distribution $p(\x)$ and a conditional distribution $p(\y \mid \x)$ and we wish to know the marginal distribution $p(\y)$ and the conditional $p(\x \mid \y).$ This can be seen as an application of Bayes' theorem. 

Specifically, consider the two random vectors $\x$ and $\y$ given by

$$
\begin{align*}
p(\x) &= \Norm(\x \mid \bmu, \Lambda\inv), \\\\[2pt]
p(\y \mid \x) &= \Norm(\y \mid \A\x+\b, \L\inv),
\end{align*}
$$

and suppose we want to know $p(\x \mid \y).$ First let's express $\x$ and $\y$ in terms of their joint distribution. As before, let

$$
\z = \begin{pmatrix}
\x \\\\
\y
\end{pmatrix}.
$$

Then, by the product rule of probability, we have

$$
p(\z) = p(\y \mid \x) \\, p(\x).
$$

It's nice to express this in terms of the log of the joint distribution:

$$
\begin{align}
&\log p(\z) = \log p(\y \mid \x) + \log p(\x) \nonumber \\\\
&= -\frac{1}{2} \bigg[ \left( \y - \A\x-\b \right)\T\L\left( \y - \A\x - \b \right) + (\x - \bmu)\T\Lambda(\x - \bmu) \bigg] + c \label{eq:16}
\end{align}
$$

We see that the exponent of the joint distribution is quadratic in terms of $\x$ and $\y$, so $p(\z)$ will be a Gaussian. Then, let's again collect the quadratic and linear terms to find the parameters of the joint distribution. First, expanding $\eqref{eq:16}$, we have

$$
\begin{align*}
& -\frac{1}{2} \y\T\L\y + \y\T\L\A\x - \frac{1}{2}\x\T\A\T\L\A\x + \y\T\L\b - \x\T\A\T\L\b \\\\
&- \frac{1}{2}\x\T\Lambda\x + \x\T\Lambda\bmu\ + c.
\end{align*}
$$

We see that the quadratic terms are given by

$$
\begin{align*}
& -\frac{1}{2}\x\T\Lambda\x - \frac{1}{2}\y\T\L\y - \frac{1}{2}\x\T\A\T\L\A\x + \y\T\L\A\x \\\\[3pt]
&= -\frac{1}{2}\x\T \left( \Lambda + \A\T\L\A \right) \x + \y\T\L\A\x - \frac{1}{2}\y\T\L\y \\\\[3pt]
&= -\frac{1}{2}
\begin{pmatrix}
\x \\\\
\y
\end{pmatrix}\T
\begin{pmatrix}
\Lambda + \A\T\L\A & -\A\T\L \\\\
-\L\A & \L
\end{pmatrix}
\begin{pmatrix}
\x \\\\
\y
\end{pmatrix} \\\\[3pt]
&= -\frac{1}{2} \z\T\mathbf{R}\z,
\end{align*}
$$

where

$$
\mathbf{R} = \begin{pmatrix}
\Lambda + \A\T\L\A & -\A\T\L \\\\
-\L\A & \L
\end{pmatrix}.
$$

Thus, $\mathbf{R}$ is the precision matrix of the joint distribution $p(\z)$, and hence the covariance matrix $\Sigma_z$ can be found by the matrix inversion formula:

$$
\Sigma_z = \mathbf{R}\inv = \begin{pmatrix}
\Lambda\inv & \Lambda\inv\A\T \\\\
\A\Lambda\inv & \L\inv + \A\Lambda\inv\A\T
\end{pmatrix}.
$$

Similarly, we can collect the linear terms to identify the mean:

$$
\begin{align*}
\x\T\Lambda\bmu + \y\T\L\b - \x\T\A\T\L\b &= \x\T \left( \Lambda\bmu - \A\T\L \right) \b + \y\T\L\b \\\\[3pt]
&= \begin{pmatrix}
\x \\\\
\y
\end{pmatrix}\T
\begin{pmatrix}
\Lambda\bmu\ - \A\T\L\b \\\\
\L\b
\end{pmatrix}.
\end{align*}
$$

Again comparing this to $(7)$ we have that

$$
\Sigma_z\inv\bmu_z = 
\begin{pmatrix}
\Lambda\bmu\ - \A\T\L\b \\\\
\L\b
\end{pmatrix}.
$$

Then, the mean is given by

$$
\begin{align*}
\bmu_z &= \Sigma_z \begin{pmatrix}
\Lambda\bmu\ - \A\T\L\b \\\\
\L\b
\end{pmatrix} \\\\[3pt]
&= \begin{pmatrix}
\Lambda\inv & \Lambda\inv\A\T \\\\
\A\Lambda\inv & \L\inv + \A\Lambda\inv\A\T
\end{pmatrix}
\begin{pmatrix}
\Lambda\bmu\ - \A\T\L\b \\\\
\L\b
\end{pmatrix} \\\\[3pt]
&= \begin{pmatrix}
\bmu\ \\\\
\A\bmu\ + \b
\end{pmatrix}
\end{align*}
$$

Now, we can obtain the parameters of the marginal distribution $p(\y)$ using $(14)$ and $(15)$:

$$
\begin{align*}
\bmu_y = \A\bmu + \b, \\\\
\Sigma_y = \L\inv + \A\Lambda\inv\A\T.
\end{align*}
$$

Similarly, we can use our previously derived rules to get an expression for the conditional distribution $p(\x \mid \y)$. In this case, it will be easier to use our expressions for the conditional parameters in terms of the precision matrix. From $(8)$, the conditional covariance is

$$
\begin{align*}
\Sigma_{x\mid y} &= \bR_{xx}\inv \\\\
&= \left( \Lambda + \A\T\L\A \right)\inv,
\end{align*}
$$

and, using $(9)$, the conditional mean is given by

$$
\begin{align*}
\bmu_{x\mid y} &= \bmu - \bR_{xx} \inv \bR_{xy}(\y - \bmu_y) \\\\
&= \bmu + (\Lambda + \A\T\L\A)\inv \A\T\L (\y - \A\bmu - \b) \\\\
&= \bmu + \Sigma_{x\mid y} \bigg[ \A\T\L (\y - \b) + \Lambda\bmu \bigg].
\end{align*}
$$ -->
<h2 id="references-and-further-reading">References and further reading</h2>
<p>The content of this post largely follows section 2.3 of Bishop&rsquo;s <em>Pattern Recognition and Machine Learning</em>. The section on Bayesian linear regression was further inspired by Bishop 3.3, as well as section 2.1 of Rasmussen and Williams&rsquo; <em>Gaussian Processes for Machine Learning</em>. However, the aim of this blog was to supplement these with detailed derivations while providing my own insights and extending the discussions on certain points.</p>
<p>I wrote this blog in an attempt to add my own insight from learning this content, and provide detailed explanations on the points which I struggled with&hellip;</p>
<p>The other resource I used was Murphy&rsquo;s <em>Probabilistic Machine Learning: An Introduction</em>, which I primarily referenced for discussions on many of the mathematical concepts, of which the important ones can be found detailed in the appendix below. Again, any material from this book which I&rsquo;ve included, I aimed to supplement and extend.</p>
<h2 id="appendix">Appendix</h2>
<details>
  <summary>Eigenvalue decomposition</summary>
  <p>
  Given a square matrix $\A \in \R^{n\times n}$, we say $\lambda$ is an eigenvalue of $\A$ with corresponding eigenvector $\u \in \R^n$ if
<p>$$
\A\u = \lambda\u, \qquad \u \neq \zero.
$$</p>
<p>We can write the collection of eigenvector equations for each eigenpair in the following matrix equation:</p>
<p>$$
\A\U = \U\Lambda,
$$</p>
<p>where $\U = \begin{bmatrix} \u_1 &amp;\u_2 &amp;\cdots &amp;\u_n\end{bmatrix}$ contains the eigenvectors $\u_i$ in its columns, and $\Lambda = \diag(\lambda_1, \lambda_2, \dots, \lambda_n).$</p>
<p>If $\A$ is <em>nonsingular</em>, there will be $n$ nonzero eigenvalues, and thus $n$ linearly independent eigenvectors. Furthermore, the eigenvalues of $\A\inv$ will be given by $1/\lambda_i$, for $i \in [n]$. Similarly, since the eigenvectors are linearly independent, then $\U$ is invertible, hence we can write</p>
<p>$$
\begin{equation}\label{A.1}\tag{A.1}
\A = \U\Lambda\U\inv.
\end{equation}
$$</p>
<p>In this case, $\A$ is <em>diagonalizable</em>. <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<p>Furthermore, if $\A$ is <em>real</em> and <em>symmetric</em>, then the eigenvalues of $\A$ are real, and the corresponding eigenvectors are orthonormal, i.e.</p>
<p>$$
\u_i\u_j\T = \delta_{ij}.
$$</p>
<p>In matrix form, this is written as</p>
<p>$$
\U\T\U = \U\U\T = \I.
$$</p>
<p>Hence, $\U$ is an orthogonal matrix, and $\U\T = \U\inv$. Then, from $\eqref{A.1}$, we can write</p>
<p>$$
\begin{align*}
\A &amp;= \U\Lambda\U\T \\
&amp;= \begin{pmatrix}
\u_1 &amp;\u_2 &amp;\cdots &amp;\u_n
\end{pmatrix}
\begin{pmatrix}
\lambda_1 &amp; &amp; &amp; \\
&amp; \lambda_2 &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \lambda_n
\end{pmatrix}
\begin{pmatrix}
\u_1\T \\[2pt]
\u_2\T \\
\vdots \\
\u_n\T
\end{pmatrix} \\
&amp;= \sum_{i=1}^n \lambda_i \u\u_i\T.
\end{align*}
$$</p>
<p>Once we have diagonalized a matrix $\A$, it is easy to invert:
$$
\begin{align*}
\A\inv &amp;= (\U\Lambda\U\T)\inv \\
&amp;= \U\Lambda\inv\U\T \\
&amp;= \sum_{i=1}^n\frac{1}{\lambda_i}\u_i\u_i\T.
\end{align*}
$$</p>
  </p>
</details>
<details>
  <summary>Change of variables formula</summary>
  <p>
  Suppose we have some random variable $x$ with distribution $p(x)$. Then, suppose we transform $x$ via some invertible function $y = f(x)$, where we define the inverse $g = f\inv$, hence $x = g(y)$. Then, suppose we'd like to find the distribution $p(y)$. To do so, we need to maintain the normalization condition:
<p>$$
\int p(y)dy = \int p(x) dx = 1.
$$</p>
<p>Differentiating on both sides with respect to $y$ gives</p>
<p>$$
\frac{d}{dy} \int p(y) dy = \frac{d}{dy} \int p(x) dx.
$$</p>
<p>Using the substitution $x = g(y)$, $dx = g^\prime(y)dy$, we have</p>
<p>$$
\begin{align*}
p(y) &amp;= \frac{d}{dy} \int p(g(y)) g^\prime(y) dy \\
&amp;= p(g(y))g^\prime(y).
\end{align*}
$$</p>
<p>Then, since $g(y) = x$, we have that $g^\prime(y) = dx/dy$, hence</p>
<p>$$
p(y) = p(x) \frac{dx}{dy}.
$$</p>
<p>Note that, in general, the distributions $p(x)$ and $p(y)$ must be nonegative; however, the derivative $dx/dy$ can be negative. Thus, we ensure nonnegativity by taking the absolute value:</p>
<p>$$
p(y) = p(x) \bigg| \frac{dx}{dy} \bigg|.
$$</p>
<p>This defines the <em>change of variables</em> formula. To generalize to the multivariate case, we replace $dx/dy$ with the Jacobian of $g$.</p>
<p>For a detailed and intuitive discussion of the change of variables formula, I like <a href="https://blog.evjang.com/2018/01/nf1.html">Eric Jang&rsquo;s blog post</a>.</p>
  </p>
</details>
<details>
  <summary>Sum and product rules of probability</summary>
  <p>
  </p>
</details>
<details>
  <summary>The Schur complement</summary>
  <p>
  Suppose we have some partitioned matrix
<p>$$
\M = \begin{pmatrix}
\A &amp; \B \\
\bC &amp; \D
\end{pmatrix},
$$</p>
<p>and we&rsquo;d like to compute $\M\inv$. First, we can diagonalize the matrix as follows:</p>
<p>$$
\begin{pmatrix}
\I &amp; -\B\D\inv \\
\zero &amp; \I
\end{pmatrix}
\begin{pmatrix}
\A &amp; \B \\
\bC &amp; \D
\end{pmatrix} = \begin{pmatrix}
\M/\D &amp; \zero \\
\bC &amp; \D
\end{pmatrix},
$$</p>
<p>where I&rsquo;ve defined the <em>Schur complement</em></p>
<p>$$
\M/\D = \A - \B\D\inv\bC.
$$</p>
<p>Then,</p>
<p>$$
\begin{pmatrix}
\M / \D &amp; \zero \\
\bC &amp; \D
\end{pmatrix}
\begin{pmatrix}
\I &amp; \zero \\
-\D\inv\bC &amp; \I
\end{pmatrix} = \begin{pmatrix}
\M/\D &amp; \zero \\
\zero &amp; \D
\end{pmatrix}.
$$</p>
<p>So, we have</p>
<p>$$
\underbrace{
\begin{pmatrix}
\I &amp; -\B\D\inv \\
\zero &amp; \I
\end{pmatrix}
}_X
\underbrace{
\begin{pmatrix}
\A &amp; \B \\
\bC &amp; \D
\end{pmatrix}
}_M \underbrace{
\begin{pmatrix}
\I &amp; \zero \\
-\D\inv\bC &amp; \I
\end{pmatrix}
}_Z = \underbrace{
\begin{pmatrix}
\M /\D &amp; \zero \\
\zero &amp; \D
\end{pmatrix}
}_W.
$$</p>
<p>Taking the inverse of both sides, we get
$$
\begin{align*}
(\X\M\Z)\inv &amp;= \W\inv \\
\Z\inv \M\inv \X\inv &amp;= \W\inv \\
\M\inv &amp;= \Z\W\inv\X.
\end{align*}
$$</p>
<p>which gives</p>
<p>$$
\begin{align*}
\M\inv &amp;= \begin{pmatrix}
\I &amp; \zero \\
-\D\inv\bC &amp; \I
\end{pmatrix}
\begin{pmatrix}
\M /\D &amp; \zero \\
\zero &amp; \D
\end{pmatrix}
\begin{pmatrix}
\I &amp; -\B\D\inv \\
\zero &amp; \I
\end{pmatrix} \\
&amp;= \begin{pmatrix}
(\M /\D)\inv &amp; - (\M /\D)\inv\B\D\inv \\
-\D\inv\bC(\M /\D)\inv &amp; \D\inv + \D\inv\bC(\M \D)\inv\B\D\inv
\end{pmatrix}.
\end{align*}
$$</p>
<p>Alternatively, we could have decomposed the matrix $\M$ in terms of $\A$, giving the Schur complement with respect to $\A$:</p>
<p>$$
\mathbf{M/E = H - GE^{-1}F}
$$ which would yield</p>
<p>$$
\M\inv = \begin{pmatrix}
\A\inv + \A\inv\B(\M /\A)\inv\bC\A\inv &amp; -\A\inv\B(\M /\A)\inv \\
-(\M /\A)\inv\bC\A\inv &amp; (\M /\A)\inv
\end{pmatrix}.
$$</p>
  </p>
</details>
<details>
  <summary>Completing the square</summary>
  <p>
  Suppose we have some quadratic function $f:\R\to\R$ given by
<p>$$
f(x) = ax^2 + bx + c.
$$</p>
<p>Then, we can rewrite this in the form</p>
<p>$$
f(x) = a(x-h)^2 + k,
$$</p>
<p>where</p>
<p>$$
h = -\frac{b}{2a}, \qquad k = c - \frac{b^2}{4a}.
$$</p>
<p>Instead, suppose $f:\R^n \to \R$ is now a quadratic function of some vector $\x$ given by</p>
<p>$$
f(\x) = \x\T\A\x + \x\T\b + \c.
$$</p>
<p>Again, we can rewrite this as</p>
<p>$$
f(\x) = (\x - \mathbf{h})\T\A(\x - \mathbf{h}) + \mathbf{k},
$$</p>
<p>where</p>
<p>$$
\mathbf{h} = -\frac{1}{2}\A\inv\b, \qquad \mathbf{k} = \c - \frac{1}{4} \b\T\A\inv\b.
$$</p>
<p>These can easily be verified by substituting the expressions for $\mathbf{h}$ and $\mathbf{k}$ into $f$.</p>
  </p>
</details>
<h3 id="short-proofs">Short proofs</h3>
<h4 id="1">1.</h4>
<p>Show that uncorrelated does not imply independent. However, in the case of a Gaussian, it does.</p>
<h4 id="2">2.</h4>
<p>Show that if a matrix $\A$ is real and symmetric, then its eigenvalues are real, and the eigenvectors can be chosen to form an orthonormal set.</p>
<h4 id="3">3.</h4>
<p>Show that the inverse of a symmetric matrix is symmetric. We&rsquo;ll use this to argue that the precision matrix $\Lambda$ is symmetric.</p>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Here I&rsquo;ve used the Kronecker delta for notational simplicity:
$$
\delta_{ij} = \begin{cases}
1 &amp;\ i=j, \\
0 &amp;\ i\neq j.
\end{cases}
$$&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Note that since $\U$ is an orthogonal matrix, a linear transformation defined by $\U$ preserves the length of the vector which it transforms, and thus is either a rotation, reflection, or a combination of both.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Most literature notation uses $\Lambda$ for the precision matrix. Not that we have overdefined $\Lambda$, since it also refers to the matrix containing the eigenvalues of $\Sigma$ in our definition for the eigenvalue decomposition. However, for the rest of this blog, $\Lambda$ will refer to the precision matrix.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>To see this, consider the product between two vectors $\a \in \R^m$ and $\b \in \R^n$ defined by $\a\T\A\b$ for some matrix $\A \in \R^{m \times n}.$ Then, since the resulting product is a scalar, we have $\a\T\A\b  = \b\T\A\T\a.$&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>From this, we see that a matrix $\A$ is diaongalizable if it is <em>square</em> and <em>nonsingular</em>. We see that it needs to be square since otherwise the notion of eigenvalues is not well-defined. This is because the eigenvalues are found by solving the characteristic equation $\det(\A - \lambda\I) = 0$, and which the determinant is only defined for square matrices. Moreover, $\A$ needs to be nonsingular so that the eigenvectors are linearly independent. In this case, the matrix $\U$ is nonsingular and hence invertible, allowing us to write $\A = \U\Lambda\U\inv$.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    
  </div>

  

  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      
<style>
  .katex a {
    text-decoration: none;
    color: inherit;
  }
  .katex a:hover {
    text-decoration: none;
  }
</style>

<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ],
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{\\text{#1}}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}",
        "\\R": "\\mathbb{R}",
        "\\bR": "\\mathbf{R}",
        "\\C": "\\mathbb{C}",
        "\\Z": "\\mathbb{Z}",
        "\\N": "\\mathbb{N}",
        "\\Q": "\\mathbb{Q}",
        "\\E": "\\mathbb{E}",
        "\\cD": "\\mathcal{D}",
        "\\var": "\\operatorname{Var}",
        "\\cov":"\\operatorname{cov}",
        "\\x": "\\mathbf{x}",
        "\\X": "\\mathbf{X}",
        "\\w": "\\mathbf{w}",
        "\\W": "\\mathbf{W}",
        "\\y": "\\mathbf{y}",
        "\\z": "\\mathbf{z}",
        "\\Z": "\\mathbf{Z}",
        "\\u": "\\mathbf{u}",
        "\\U": "\\mathbf{U}",
        "\\I": "\\mathbf{I}",
        "\\A": "\\mathbf{A}",
        "\\a": "\\mathbf{a}",
        "\\B": "\\mathbf{B}",
        "\\b": "\\mathbf{b}",
        "\\c": "\\mathbf{c}",
        "\\D": "\\mathbf{D}",
        "\\M": "\\mathbf{M}",
        "\\m": "\\mathbf{m}",
        "\\bC": "\\mathbf{C}",
        "\\J": "\\mathbf{J}",
        "\\L": "\\mathbf{L}",
        "\\bS": "\\mathbf{S}",
        "\\bmu": "\\boldsymbol{\\mu}",
        "\\bphi": "\\boldsymbol{\\phi}",
        "\\zero": "\\mathbf{0}",
        "\\one": "\\mathbf{1}",
        "\\T": "^{\\top}",
        "\\inv": "^{-1}",
        "\\ij": "_{ij}",
        "\\Norm": "\\mathcal{N}",
        "\\gam": "\\text{Gamma}",
        "\\nll": "\\text{NLL}",
        "\\argmin": "\\underset{#1}{\\operatorname{argmin}}",
        "\\argmax": "\\underset{#1}{\\operatorname{argmax}}",
        "\\diag": "\\operatorname{diag}",
        "\\tr": "\\operatorname{tr}",
        "\\pbmu": "\\frac{\\partial}{\\partial \\boldsymbol{\\mu}}",
        "\\pSigma": "\\frac{\\partial}{\\partial \\Sigma}",
        "\\pbx": "\\frac{\\partial}{\\partial \\mathbf{x}}",
        "\\px": "\\frac{\\partial}{\\partial x}",
        "\\pbA": "\\frac{\\partial}{\\partial \\mathbf{A}}",
        "\\ml": "_\\text{ML}",
      }
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>


<script>
  
  function updateFigureNumbers() {

      const figRefs = document.querySelectorAll('.fig-ref');
      figRefs.forEach(ref => {
          const figId = ref.getAttribute('href').slice(1);
          const figElement = document.getElementById(figId);
          if (figElement) {
              const figIndex = Array.from(figures).indexOf(figElement) + 1;
              ref.textContent = `Figure ${figIndex}`;
          }
      });
  }

  
  window.addEventListener('load', updateFigureNumbers);
</script>



</html>