<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content=""/>

<title>
    
    A spelled-out introduction to Gaussian processes | Walter Virany
    
</title>

<link rel="canonical" href="http://localhost:1313/posts/gp_intro/"/>












<link rel="stylesheet" href="/assets/combined.min.70663b98395cb46c50fa4dfa0b5a36b1136b531b6fed206fa9944c2cc27b3221.css" media="all">





  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">Walter Virany</h1>
    
    
    

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /blog
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/me" >
                /me
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/why" >
                /why?
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        







<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">A spelled-out introduction to Gaussian processes</h1>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2025-09-01T00:00:00&#43;00:00">September 1, 2025</time>
      

      
      &nbsp; · &nbsp;
      24 min read
      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <!-- TO DO:
* proper way to format footnote headers
* ctrl + f: we -> I, when applicable
* change \mid to \Big| where appropriate
* am I using \phi instead of \bphi in some places?
* clean up figures: axes, titles, legends, etc.
* consistent format for references (e.g., Bishop, 2006, Bishop (2006), or [Bishop 2006])
* at the end, discuss varying hyperparameters and how to make optimizations with Cholesky factorization
* type hinting for `kernel`; e.g., from typing import Callable?
* double check formulas in `ZeroMeanGP` class -> when to use K vs. K + sigma^2I
* when to use \btheta vs. \theta
-->
<p>Gaussian processes (GPs) have always been a particularly unintuitive topic for me in machine learning. In this blog, I attempt to present a spelled-out introduction to GPs, building up from a simple linear model. I provide both the underlying mathematics as well as a Python implementation. For the sake of clarity, I omit certain practical considerations regarding numerical stability, computational complexity, etc., which can all be found in standard treatments of GPs.</p>
<h2 id="bayesian-linear-regression">Bayesian linear regression</h2>
<h3 id="the-linear-model">The linear model</h3>
<p>To build the foundation for GPs, I&rsquo;ll start by considering a Bayesian treatment of linear regression. We&rsquo;ll see that this is in fact a basic example of a GP.</p>
<p>Consider the linear model</p>
<p>$$
\begin{align*}
f(\x; \w) = \x\T\w,
\end{align*}
$$</p>
<p>where $\x \in \R^d$ is some input vector with $d$ features and $\w \in \R^d$ is the vector of parameters which specify the model. Note that we can incorporate an intercept term by always letting one element of $\x$ be constant, say $x_0 = 1$: <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>$$
f(\x; \w) = w_0 + w_1x_1 + \cdots + w_{d-1}x_{d-1}.
$$</p>
<p>Moreover, we can define a feature transformation $\phi: \R^d \to \R^m$. This transforms our feature vectors as follows:</p>
<p>$$
\phi(\x) =
\begin{bmatrix}
\phi_0(\x) \\
\phi_1(\x) \\
\vdots \\
\phi_{m-1}(\x)
\end{bmatrix}.
$$</p>
<p>Again, letting $\phi_0(\x) = 1$ implicitly defines a bias term. Now, we can redefine our model in terms of these basis functions:</p>
<p>$$
f(\x; \w) = \bphi\T\w,
$$</p>
<p>where $\bphi = \phi(\x)$, and I&rsquo;ve abused notation by now letting $\w \in \R^m$. If the basis functions ${\phi_i}$ are nonlinear in terms of $\x$, we can model nonlinear relatiomships while still enjoying the benefits of a linear model, since $f$ is linear in terms of the learned parameters $\w$.</p>
<p>For example, suppose we have a one-dimensional input $x$, and we wish to model the class of polynomials up to degree $m-1$. Then, we simply define $\phi_j(x) = x^j$, which gives the following model:</p>
<p>$$
\begin{align*}
f(\x; \w) &amp;= \bphi\T\w \\
&amp;= w_0\phi_0(x) + w_1\phi_1(x) + \dots + w_{m-1}\phi_{m-1}(x) \\
&amp;= w_0 + w_1x + w_2x^2 + \dots + w_{m-1}x^{m-1}
\end{align*}
$$</p>
<p>Now, we usually assume that a given observation $(\x, y)$ is corrupted by some noise, which we can model by adding a zero-mean Gaussian random variable to the functional outputs:</p>
<p>$$
y = f(\x; \w) + \epsilon,
$$</p>
<p>where $\epsilon \sim \Norm(0, \sigma^2)$. This gives rise to a probability distribution over $y$: <sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup></p>
<p>$$
\begin{align*}
p(y \mid \x, \w, \sigma^2) = \Norm(y \mid f(\x; \w), \sigma^2).
\end{align*}
$$</p>
<p>Before moving forward, I&rsquo;ll make a quick note on notation: when referring to a general probability distribution $p$, we list the unobserved variables on the LHS of the conditional and the observed variables on the RHS, including hyperparameters like $\sigma^2$, in no particular order. For example, $p(y \mid \x, \w, \sigma^2) = p(y \mid \w, \sigma^2, \x)$. It&rsquo;s completely arbitrary what order the variables are in, so long as they fall on the correct side of the conditional, and often certain variables are omitted from the notation and implicitly assumed. From this point, I will omit hyperparameters from the general expressions for distributions. However, when we refer to a specific distribution like the Gaussian, the positions of given variables follow certain conventions: the first position after the conditional is reserved for the mean, and the second position is reserved for the variance, hence I&rsquo;ll write $p(y \mid \x, \w) = \Norm(y \mid f(\x; \w), \sigma^2)$.</p>
<h3 id="computing-the-parameters">Computing the parameters</h3>
<p>Now, suppose we observe some iid dataset $\cD = (\X, \y)$, where $\X \in \R^{d\times n}$ is the matrix whose columns are the $n$ observed input vectors, and $\y = (y_1, y_2 ,\ldots, y_n)\T$ contains the correspondng target variables. Moreover, we can write the matrix containing our feature vectors as $\bPhi \in \R^{m\times n}$. Then, we have the following matrix equation:</p>
<p>$$
\y = \bPhi\T\w + \bepsilon,
$$</p>
<p>where $\bepsilon \sim \Norm(\zv, \sigma^2\I)$. As is often the case in supervised learning, we seek to find reasonable values for the parameters $\w$ in light of this observed data. In the frequentist approach to linear regression, we might model the likelihood function:</p>
<p>$$
\begin{align*}
p(\cD \mid \w) &amp;= p(\y \mid \X, \w) \\
&amp;= p(y_1, \dots, y_n \mid \x_1, \dots, \x_n, \w) \\
\text{\scriptsize (from iid assumption)} \qquad &amp;= \prod_{i=1}^n p(y_i \mid \x_i, \w) \\
&amp;= \prod_{i=1}^n \Norm(y_i \mid f(\x_i; \w), \sigma^2).
\end{align*}
$$</p>
<p>Then, we would maximize this expression with respect to $\w$, which would give a point estimate for the parameters.</p>
<p>Instead, we will take a Bayesian treatment, which will allow us to compute a probability distribution over all possible values of of the parameters. To do so, we start by defining a prior on $\w$:</p>
<p>$$
p(\w) = \Norm \left( \w \mid \zv, \bSigma \right).
$$</p>
<p>With no previous information about $\w$, it&rsquo;s reasonable to assume that all values of $\w$ are equally likely &mdash; this corresponds to a zero-mean Gaussian. Furthermore, we often assume the elements of $\w$ are independent, so $\bSigma = \alpha\I$, for some constant $\alpha$. However, I&rsquo;ll continue with the general form for the prior covariance.</p>
<p>Now, we&rsquo;d like to infer the values of $\w$ from the observed data by computing the posterior distribution $p(\w \mid \cD)$. To do so, we can model the joint distribution of $\y$ and $\w$, then use the <a href="../gaussian/#conditioning">rules for conditioning</a> on multivariate Guassian distributions.<sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup> First, we note that $\y$ is the <a href="../gaussian/#sum-of-gaussians">sum of two Gaussians</a>; the transformed $\bPhi\T\w$, and $\bepsilon$. Thus, $\y$ will be Gaussian-distributed as follows:</p>
<p>$$
p(\y \mid \X) = \Norm \left( \y \bmid \zv, \bPhi\T\bSigma\bPhi + \sigma^2\I \right).
$$</p>
<p>Then, we compute the covariance between $\y$ and $\w$:</p>
<p>$$
\cov(\y, \w) = \cov(\bPhi\T\w + \bepsilon, \w) = \bPhi\T\cov(\w, \w) = \bPhi\T\bSigma.
$$</p>
<p>The joint distribution is then given by</p>
<p>$$
p(\y, \w \mid \X) = \Norm \left( \left. \begin{bmatrix}
\y \\
\w
\end{bmatrix}\right\vert
\zv, \begin{bmatrix}
\bPhi\T\bSigma\bPhi + \sigma^2\I &amp; \bPhi\T\bSigma \\
\bSigma\bPhi &amp; \bSigma
\end{bmatrix}
\right).
$$</p>
<p>Now we can compute the conditional distribution $p(\w \mid \y, \X),$ which is Gaussian with the following parameters:</p>
<p>$$
\begin{align*}
\bmu_{\w\mid\cD} &amp;= \bSigma\bPhi \left( \bPhi\T\bSigma\bPhi + \sigma^2\I \right)\inv\y, \\
\bSigma_{\w\mid\cD} &amp;= \bSigma - \bSigma\bPhi\left( \bPhi\T\bSigma\bPhi + \sigma^2\I \right)\inv \bPhi\T\bSigma.
\end{align*}
$$</p>
<h3 id="making-predictions">Making predictions</h3>
<p>Using our posterior parameter distribution $p(\w \mid \cD)$, we&rsquo;d like to now make predictions at new test points $\X_\ast$, i.e., we&rsquo;d like to compute the posterior predictive distribution $p(\y_\ast \mid \X_\ast, \cD)$. One way to do this is to average over all values of $\w$:</p>
<p>$$
p(\y_\ast \mid \X_\ast, \cD) = \int p(\y_\ast \mid \X_\ast, \w) p(\w \mid \cD) d\w,
$$</p>
<p>where $p(\y_\ast \mid \X_\ast, \w)$ is the likelihood of the new test points. This integral is tractable, but takes a bit of work.<sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup> An easier way to compute the predictive distribution is to note that, under our model,</p>
<p>$$
\y_\ast = \bPhi_\ast\T\w + \bepsilon.
$$</p>
<p>Then, if we use the posterior distribution over $\w$ and once again use the rules for transforming Gaussians, we have the following result:</p>
<p>$$
p(\y_\ast \mid \X_\ast, \cD) = \Norm \left( \y_\ast \Big| \bPhi_\ast\T\bmu_{\w\mid\cD}, \bPhi_\ast\T\bSigma_{\w\mid\cD}\bPhi_\ast + \sigma^2\I \right).
$$</p>
<h3 id="bayesian-linear-regression-in-python">Bayesian linear regression in Python</h3>
<p>Here I show a simple implementation of Bayesian linear regression in Python.</p>
<p>First, I&rsquo;ll construct a toy dataset:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">import</span> numpy <span style="color:#00f">as</span> np
</span></span><span style="display:flex;"><span><span style="color:#00f">from</span> math <span style="color:#00f">import</span> pi
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>f = <span style="color:#00f">lambda</span> x: x * np.sin(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>xs = np.linspace(0, 2*pi, 100)
</span></span><span style="display:flex;"><span>ys = f(xs)
</span></span></code></pre></div><p>This generates 100 evenly-spaced points on the interval $[0, 2\pi]$, as well as the corresponding functional outputs according to the function $f(x) = x\sin(x)$. To construct a training set, we can uniformly sample points in the domain and evaluate their corresponding functional values:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>np.random.seed(1)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>n_samples = 10
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Noise term with variance 0.1 - note that Var(bX) = b^2 * Var(X)</span>
</span></span><span style="display:flex;"><span>epsilon = np.sqrt(0.1) * np.random.standard_normal(size=n_samples)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_data = np.random.uniform(0, 2*pi, size=n_samples)
</span></span><span style="display:flex;"><span>y_data = f(X_data) + epsilon
</span></span></code></pre></div><p>I&rsquo;ve also added a noise term <code>epsilon</code> with variance 0.1 to the observations. This gives a dataset of noisy observations $\cD = (\X, \y)$ as we saw before, and our goal is to approximate the function $f$ on the entire domain. Before going further, it&rsquo;s always a good idea to visualize the data (if applicable) for any machine learning task:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">import</span> matplotlib.pyplot <span style="color:#00f">as</span> plt
</span></span><span style="display:flex;"><span><span style="color:#00f">import</span> seaborn <span style="color:#00f">as</span> sns
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Set up Seaborn plotting style - this can be ignored</span>
</span></span><span style="display:flex;"><span>sns.set_style(<span style="color:#a31515">&#34;darkgrid&#34;</span>,
</span></span><span style="display:flex;"><span>              {<span style="color:#a31515">&#34;axes.facecolor&#34;</span>: <span style="color:#a31515">&#34;.95&#34;</span>,
</span></span><span style="display:flex;"><span>               <span style="color:#a31515">&#34;axes.edgecolor&#34;</span>: <span style="color:#a31515">&#34;#000000&#34;</span>,
</span></span><span style="display:flex;"><span>               <span style="color:#a31515">&#34;grid.color&#34;</span>: <span style="color:#a31515">&#34;#EBEBE7&#34;</span>,
</span></span><span style="display:flex;"><span>               <span style="color:#a31515">&#34;font.family&#34;</span>: <span style="color:#a31515">&#34;serif&#34;</span>,
</span></span><span style="display:flex;"><span>               <span style="color:#a31515">&#34;axes.labelcolor&#34;</span>: <span style="color:#a31515">&#34;#000000&#34;</span>,
</span></span><span style="display:flex;"><span>               <span style="color:#a31515">&#34;xtick.color&#34;</span>: <span style="color:#a31515">&#34;#000000&#34;</span>,
</span></span><span style="display:flex;"><span>               <span style="color:#a31515">&#34;ytick.color&#34;</span>: <span style="color:#a31515">&#34;#000000&#34;</span>,
</span></span><span style="display:flex;"><span>               <span style="color:#a31515">&#34;grid.alpha&#34;</span>: 0.4 })
</span></span><span style="display:flex;"><span>sns.set_palette(<span style="color:#a31515">&#39;muted&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00f">def</span> create_base_plot():
</span></span><span style="display:flex;"><span>    fig, ax = plt.subplots()
</span></span><span style="display:flex;"><span>    ax.plot(xs, ys, c=<span style="color:#a31515">&#39;k&#39;</span>, ls=<span style="color:#a31515">&#39;dashed&#39;</span>, lw=1, label=<span style="color:#a31515">r</span><span style="color:#a31515">&#39;$f(x) = x\sin(x)$&#39;</span>)
</span></span><span style="display:flex;"><span>    ax.scatter(X_data, y_data, c=<span style="color:#a31515">&#39;firebrick&#39;</span>, s=25, label=<span style="color:#a31515">&#39;Training samples&#39;</span>)
</span></span><span style="display:flex;"><span>    ax.set_xlim(0, 2*pi)
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> ax
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>create_base_plot()
</span></span></code></pre></div><div id="fig1" class="figure">
   <img src="figures/figure1.png" alt="Figure 1" style="width:80%; margin-left:auto; margin-right:auto">
</div>
<p>Now, we can build our model. First, we define a feature map $\phi:\R \to \R^m$ as in the first section, where</p>
<p>$$
\phi(x) = w_0 + w_1x + \cdots + w_{m-1}x^{m-1}.
$$</p>
<p>We can implement this as follows:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">def</span> poly_feature_transform(X: np.ndarray, m: int) -&gt; np.ndarray:
</span></span><span style="display:flex;"><span>    <span style="color:#a31515">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    Transform the input vectors X into polynomial features of degree m.
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    X: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        The input of shape (n, 1); n is number of samples
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    m: int
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        The degree of the polynomial features.
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    X_poly : np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        The transformed feature vector with polynomial features of shape
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        (m, n)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    n = X.shape[0]
</span></span><span style="display:flex;"><span>    X_poly = np.ones(n).reshape(n, 1)  <span style="color:#008000"># Instantiate X_poly with intercept terms</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">for</span> i <span style="color:#00f">in</span> range(1, m):
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Add column of features x^i</span>
</span></span><span style="display:flex;"><span>        X_poly = np.concatenate((X_poly, X.reshape(n, 1)**i), axis=1)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> X_poly.T   <span style="color:#008000"># (m, n)</span>
</span></span></code></pre></div><p>Next, we&rsquo;d like to compute the posterior parameter and predictive distributions, and make predictions at new test points:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">def</span> make_predictions(
</span></span><span style="display:flex;"><span>    X: np.ndarray, X_star: np.ndarray, y: np.ndarray, sigma: float = 0.1
</span></span><span style="display:flex;"><span>):
</span></span><span style="display:flex;"><span>    <span style="color:#a31515">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    Compute posterior paramter distribution and make predictions at test points.
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    Args:
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    X: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Input of shape (m, n); m is dimension of feature space, n is number
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        of samples
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    X_star: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Test points of shape (m, n_star) at which to make new predictions;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        n_star is number of test points
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    y: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Training labels of shape (n,)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    sigma: float (default = 0.1)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Noise variance
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    mean: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Mean of predictive distribution at points X_star; shape = (n_star,)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    std: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Standard deviation of predictive distribution at points X_star;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        shape = (n_star,)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">    &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    m, n = X.shape
</span></span><span style="display:flex;"><span>    n_star = X_star.shape[1]
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">assert</span> X_star.shape[0] == m
</span></span><span style="display:flex;"><span>    <span style="color:#00f">assert</span> y.shape == (n,)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Prior covariance of parameter distribution p(w); mean is zero</span>
</span></span><span style="display:flex;"><span>    S = np.eye(m)   <span style="color:#008000"># (m, m)</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Compute mean and covariance of posterior parameter distribution p(w|D)</span>
</span></span><span style="display:flex;"><span>    S_post = S - S @ X @ np.linalg.inv(X.T @ S @ X + sigma*np.eye(n)) @ X.T @ S
</span></span><span style="display:flex;"><span>    m_post = S @ X @ np.linalg.inv(X.T @ S @ X + sigma*np.eye(n)) @ y
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">assert</span> S_post.shape == (m, m)
</span></span><span style="display:flex;"><span>    <span style="color:#00f">assert</span> m_post.shape == (m,)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Compute mean and standard deviation at locations X_star</span>
</span></span><span style="display:flex;"><span>    mean = X_star.T @ m_post
</span></span><span style="display:flex;"><span>    std = np.sqrt(np.diag(X_star.T @ S_post @ X_star + sigma*np.eye(n_star)))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">assert</span> mean.shape == std.shape == (n_star,)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> mean, std
</span></span></code></pre></div><p>The expressions for computing the distributions of interest are just those derived in the previous sections. Using these functions, we can now perform Bayesian linear regression:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>m = 4  <span style="color:#008000"># Degree of polynomials in feature space is m - 1</span>
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_poly = poly_feature_transform(X_data, m)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>X_star = xs  <span style="color:#008000"># Predict values of each point in domain</span>
</span></span><span style="display:flex;"><span>X_star_poly = poly_feature_transform(X_star, m)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean, std = make_predictions(X_poly, X_star_poly, y_data, sigma=.1)
</span></span></code></pre></div><p>We can now visualize the results:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">def</span> plot_predictions(mean, std):
</span></span><span style="display:flex;"><span>    ax = create_base_plot()
</span></span><span style="display:flex;"><span>    ax.plot(xs, mean, c=<span style="color:#a31515">&#39;midnightblue&#39;</span>, lw=1, label=<span style="color:#a31515">&#39;Model predictions&#39;</span>)  
</span></span><span style="display:flex;"><span>    ax.fill_between(xs, mean-std, mean+std, alpha=.25, label=<span style="color:#a31515">&#39;1 std&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_predictions(mean, std)
</span></span></code></pre></div><div id="fig1" class="figure">
   <img src="figures/figure2.png" alt="Figure 2" style="width:80%; margin-left:auto; margin-right:auto">
</div>
<p>The model predictions are shown in blue, with a shaded area corresponding to one standard deviation about the mean.</p>
<p>I would encourage the reader to vary the complexity of the model by using different values of $m$. Some extreme examples would be $m = 1$ or $2$, which would give constant or linear predictions, respectively. Alternatively, by setting the value of $m$ to be very high, we see that the model overfits the data. This is a nice example of the bias-variance tradeoff.</p>
<h2 id="the-kernel-trick">The kernel trick</h2>
<p>If we write out the mean and covariance for the posterior predictive distribution, we have</p>
<p>$$
\begin{align*}
\bmu_{\y_\ast \mid \cD} &amp;= \bPhi_\ast\T\bmu_{\w\mid\cD} \\
&amp;= \bPhi_\ast\T\bSigma\bPhi \left( \bPhi\T\bSigma\bPhi + \sigma^2\I \right)\inv\y,
\end{align*}
$$</p>
<p>and</p>
<p>$$
\begin{align*}
\bSigma_{\y_\ast \mid \cD} &amp;= \bPhi_\ast\T\bSigma_{\w\mid\cD}\bPhi_\ast + \sigma^2\I \\
&amp;= \bPhi_\ast\T \left[ \bSigma - \bSigma\bPhi \left( \bPhi\T\bSigma\bPhi + \sigma^2\I \right)\inv \bPhi\T\bSigma \right] \bPhi_\ast + \sigma^2\I \\
&amp;= \bPhi_\ast\T\bSigma\bPhi_\ast - \bPhi_\ast\T\bSigma\bPhi \left( \bPhi\T\bSigma\bPhi + \sigma^2\I \right)\inv \bPhi\T\bSigma\bPhi_\ast + \sigma^2\I.
\end{align*}
$$</p>
<p>By doing so, we see that all the dependence of the posterior predictive distribution on the features $\bPhi$ and $\bPhi_\ast$ is in the form of one of the following expressions:</p>
<p>$$
\begin{gather}\label{1}
\bPhi\T\bSigma\bPhi, \quad \bPhi\T\bSigma\bPhi_\ast, \quad \bPhi_\ast\T\bSigma\bPhi, \quad \bPhi_\ast\T\bSigma\bPhi_\ast.
\end{gather}
$$</p>
<p>We can rewrite these quantities as the following matrices:</p>
<p>$$
\begin{align}\label{2}
\K &amp;= \bPhi\T\bSigma\bPhi, \quad \K_\ast = \bPhi\T\bSigma\bPhi_\ast, \quad \K_{\ast\ast} = \bPhi_\ast\T\bSigma\bPhi_\ast.
\end{align}
$$</p>
<p>Note that $\K_\ast = (\bPhi_\ast\T\bSigma\bPhi)\T\!,\,$ so we can represent each of the four quantities in $\eqref{1}$ using the three matrices above. Each element of these matrices can be written in the form</p>
<p>$$
k(\x, \x\p) = \phi(\x)\T\bSigma\phi(\x\p),
$$</p>
<p>where I&rsquo;ve defined a <em>kernel function</em> $k$, also sometimes called a <em>covariance function</em>. By noting that $\bSigma$ is positive definite, and hence has a matrix square root,<sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup> we can rewrite $k$ as
$$
k(\x, \x\p) = \psi(\x)\T\psi(\x\p),
$$
where $\psi(\x) = \Sigma^{1/2}\phi(\x)$. Thus, $k$ corresponds to a valid inner product between two vectors which are functions of the observations. Then, since $k$ corresponds to an inner product between two vectors, the matrices in $\eqref{2}$ are <em>Gram matrices</em>.<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> The Gram matrices can then be written in terms of the kernel function:</p>
<p>$$
\begin{equation}\label{3}
\K = k(\X, \X), \quad \K_\ast = k(\X, \X_\ast), \quad \K_{\ast\ast} = k(\X_\ast, \X_\ast).
\end{equation}
$$</p>
<p>We can also rewrite the parameters of the predictive distribution:</p>
<p>$$
\begin{align}
\bmu_{\y_\ast \mid \cD} &amp;= \K_\ast\T \left( \K + \sigma^2\I \right)\inv\y, \label{4} \\
\bSigma_{\y_\ast \mid \cD} &amp;= \K_{\ast\ast} - \K_\ast\T \left( \K + \sigma^2\I \right)\inv \K_\ast + \sigma^2\I. \label{5}
\end{align}
$$</p>
<p>Thus, we&rsquo;ve shown that for <em>any</em> choice of prior covariance $\bSigma$ and feature map $\phi$, we can express our result in terms of a function $k$ which corresponds to some inner product. If we treat this function as a &ldquo;black box&rdquo;, we could skip the step of explicitly defining these quantities, and simply specify a kernel function to build our Gram matrices. So long as the kernel function we choose can be expressed in terms of a valid inner product, this corresponds to some feature map of the input vectors. I&rsquo;ll shortly discuss how to choose valid kernel functions.</p>
<p>The advantage of this approach is that the feature vectors we&rsquo;d like to work with might be very high dimensional, and it might be cheaper to work in terms of the kernel function.</p>
<p>As a concrete example,<sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup> consider some vector $\x = (x_1, x_2, \dots, x_d) \in R^d$, and suppose we wish to express all the monomials up to degree $2$ in terms of the features of $\x$:</p>
<p>$$
\phi(\x) = \begin{bmatrix}
1 \\
x_1 \\
\vdots \\
x_d \\[3pt]
x_1^2 \\
x_1x_2 \\
\vdots \\[3pt]
x_d^2
\end{bmatrix}.
$$</p>
<p>Noting that there is one constant term, $d$ linear terms, and $d\choose{2}$ quadratic terms, computing this requires $\mathcal{O}(d^2)$ operations:</p>
<p>$$
1 + d + {d\choose{2}} \sim \mathcal{O}(d^2).
$$</p>
<p>For large $d$, this could become prohibitive to compute. Alternatively, we can write the inner product as</p>
<p>$$
\begin{align*}
\phi(\x)\T\phi(\x\p) &amp;= 1 + \sum_{i=1}^dx_ix_i\p + \sum_{i=1}^dx_i^2x_i^{\prime\,2} + 2 \sum_{i=1}^d \sum_{i &lt; j} x_ix_j x_i\p x_j\p \\
&amp;= 1 + \sum_{i=1}^d x_ix_i\p + \left( \sum_{i=1}^d x_ix_i\p \right)^2 \\
&amp;= 1 + \x\T\x\p + (\x\T\x\p)^2.
\end{align*}
$$</p>
<p>Thus, we can define the kernel function $k(\x, \x\p) = 1 + \x\T\x\p + (\x\T\x\p)^2$. In doing so, we can just plug $\x$ into the kernel function and make our predictions based on these values using the expressions for the parameters defined by $\eqref{4}$ and $\eqref{5}$. We never have to compute the explicit feature map $\phi(\x)$, nor the parameters $\w$.</p>
<h3 id="comments-on-valid-kernel-functions">Comments on valid kernel functions</h3>
<p>A caveat to the approach described above is that we must use a <em>valid</em> kernel function, i.e., a kernel which corresponds to an inner product. Luckily, there are some fairly straightforward methods of obtaining these.</p>
<p>There are several ways to check if a function is a valid kernel; one way is to show that, for any set of vectors $S$, the Gram matrix whose elements are given by $\K_{ij} = k(\x_i, \x_j)$ for each $\x_i, \x_j \in S$ is always positive-semidefinite. Another way is to show that $k$ can be represented as $k(\x, \x\p) = \psi(\x)\T\psi(\x\p)$, for some explicit feature map $\psi$, as we saw before.</p>
<p>Moreover, once we have some valid kernel functions, we can use these as building blocks to construct new ones. For example, sums and products of valid kernels yield still valid kernels, along with many other transformations. We can take advantage of these properties to build rich classes of kernel functions.</p>
<p>There are many great resources on covariance functions (namely <a href="#references">Rasmussen &amp; Williams (2006)</a>), so I won&rsquo;t go into much detail here; however, I&rsquo;ll introduce a common kernel which we&rsquo;ll use, known as the <em>squared exponential</em>: <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></p>
<p>$$
k(\x, \x\p) = a^2 \exp \left( -\frac{\left\lVert \x - \x\p \right\rVert^2}{2l^2}\right).
$$</p>
<p>Here, $a$ is known as the amplitude and acts as a scaling factor. The parameter $l$ is known as the lengthscale and controls the contribution to the covariance from points $\x\p \neq \x$.</p>
<h2 id="gaussian-process-regression">Gaussian process regression</h2>
<p>Let&rsquo;s now return to the context of Bayesian linear regression. We have the following expression for the the predictive distribution:</p>
<p>$$
p(\y_\ast \mid \X_\ast, \cD) = \Norm\left( \y_\ast \bmid \bmu_{\y_\ast\mid\cD}, \bSigma_{\y_\ast\mid\cD} \right),
$$</p>
<p>for which the parameters are given by $\eqref{4}$ and $\eqref{5}$.</p>
<p>We previously derived these results by performing inference over the parameters. However, instead of reasoning about $\w$ and $\bphi$, we showed that we can represent the quantities of interest - that is, the parameters of the predictive distribution - solely in terms of a kernel function and given quantities. Then, instead of performing inference in parameter space, we can express the mean and covariance as functions, and our job becomes that of performing inference directly in the space of functions.</p>
<p>This gives rise to the GP framework. We write a GP as</p>
<p>$$
f \sim \GP(f; m, k),
$$</p>
<p>where $f$ is the function that is &ldquo;sampled&rdquo; from the GP. Like the Gaussian distribution, a GP is entirely specified by its first and second moments:</p>
<p>$$
\begin{align*}
m(\x) &amp;= \E[f(\x)], \\
k(\x, \x\p) &amp;= \E[(f(\x) - m(\x))(f(\x\p) - m(\x\p))].
\end{align*}
$$</p>
<p>However, instead of explicitly constructing $k$, we just define some kernel function as discussed in the previous section.</p>
<p>In the following sections I will develop the technology necessary to perform GP regression, which should also illuminate precisely how we model functions with a GP; however, before I do so, I will attempt to provide some intuition. First, I&rsquo;ll state the formal definition of a GP:</p>
<p><strong>Definition.</strong>   <em>A Gaussian Process (GP) is a collection of random variables such that any finite collection of which takes a joint multivariate Gaussian distribution.</em></p>
<p>To see this, consider the case in which we define the function $f$ over a continuous interval $[a, b]$, and we&rsquo;d like to model the relationship between functional outputs $f(\x)$ for each $\x \in [a, b]$. Since this set contains uncountably many variables, we can&rsquo;t use our usual definition for the multivariate Gaussian distribution.  However, this definition states that if we take a finite subset of this interval, say $\{\x_1, \ldots, \x_n\} \subseteq [a, b]$, then the random variables $\{f(\x_1), \ldots, f(\x_n)\}$ will be jointly Gaussian-distributed. The GP specifies how we compute the parameters of this distribution. <sup id="fnref:9"><a href="#fn:9" class="footnote-ref" role="doc-noteref">9</a></sup></p>
<p>This gives rise to one interpretation of GPs: where functions are the infinite-dimensional extension of fininite-dimensional vectors, GPs can be seen as an infinite-dimensional extension of the multivariate Gaussian distribution.</p>
<h3 id="exact-observations">Exact observations</h3>
<p>Analagously to Bayesian linear regression, we start with a <em>prior process</em> over the function $f$. Then, we will use observed data to reason about the distribution over functions.</p>
<p>First, suppose we can observe the functional outputs directly, i.e., with no noise, so that our dataset is $\cD = (\X, \f)$, where $\f = (f(\x_1), \ldots, f(\x_n))\T$. Note that $f$ denotes the actual function we aim to model, and $\f$ denotes the vector of observations of the function.</p>
<p>As before, we&rsquo;d like to predict the functional values $\f_\ast$ at new points $\X_\ast \in \R^{d\times n_\ast}$. Note that $\f$ and $\f_\ast$ are finite subsets of the function $f$ over the domain of interest. Thus, from our definition of a GP, the joint distribution $p(\f, \f_\ast)$ will be multivariate Gaussian with mean and covariance defined by the corresponding functions. Letting the prior mean $m(\x) = 0$, we can model the joint distribution as</p>
<p>$$
\begin{bmatrix}
\f \\
\f_\ast
\end{bmatrix} \sim
\Norm \left(
\zv, \begin{bmatrix}
\;\K\;\; &amp; \K_\ast \\
\;\K_\ast\T &amp; \K_{\ast\ast}
\end{bmatrix}
\right),
$$</p>
<p>where the components of the covariance matrix are given by $\eqref{3}$. As before, we condition $\f_\ast$ on $\f$ to acquire the posterior distribution over the test points:</p>
<p>$$
p(\f_\ast \mid \X_\ast, \cD) = \Norm\left( \f_\ast \bmid \bmu_{f\mid\cD}(\X_\ast), \K_{f\mid\D}(\X_\ast, \X_\ast) \right).
$$</p>
<p>The parameters are given by our usual conditioning formulas:</p>
<p>$$
\begin{align*}
\bmu_{f\mid\cD} &amp;= \K_\ast\T\K\inv\f, \\
\K_{f\mid\cD} &amp;= \K_{\ast\ast} - \K_\ast\T\K\inv\K_\ast.
\end{align*}
$$</p>
<h3 id="noisy-observations">Noisy observations</h3>
<p>We can extend this to the case of noisy observations, i.e., the case in which we cannot observe values of $f$ directly, but some $y$ corrupted by Gaussian noise:</p>
<p>$$
y = f(\x) + \epsilon.
$$</p>
<p>This gives rise to the following distribution over obserations:</p>
<p>$$
\y \sim \Norm(\zv, \K + \sigma^2\I).
$$</p>
<p>As before, we&rsquo;d like to predict the values of test points $\f_\ast$. The joint distribution now takes the form</p>
<p>$$
\begin{bmatrix}
\y \\
\f_\ast
\end{bmatrix} \sim
\Norm \left( \zv, \begin{bmatrix}
\K + \sigma^2\I &amp; \K_\ast \\
\K_\ast\T &amp; \K_{\ast\ast}
\end{bmatrix} \right).
$$</p>
<p>Using the same conditioning rules as before, the parameters of our predictive distribution $p(\f_\ast \mid \X_\ast, \cD)$ are given by</p>
<p>$$
\begin{align*}
\bmu_{f\mid\cD} &amp;= \K_\ast\T \left( \K + \sigma^2\I \right)\inv\y \\
\K_{f\mid\cD} &amp;= \K_{\ast\ast} - \K_\ast\T \left( \K + \sigma^2\I \right)\inv \K_\ast.
\end{align*}
$$</p>
<p>To compute the predictive distribution for $\y_\ast$, we simply add $\epsilon$ to the predicted outputs $\f_\ast$:</p>
<p>$$
p(\y_\ast \mid \X_\ast, \cD) = \Norm \left( \y_\ast \bmid \bmu_{f\mid\cD}, \K_{f\mid\cD} + \sigma^2\I \right).
$$</p>
<h3 id="gaussian-process-regression-in-python">Gaussian process regression in Python</h3>
<p>We now have all the tools to build a GP regression model from scratch:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">class</span> <span style="color:#2b91af">ZeroMeanGP</span>:
</span></span><span style="display:flex;"><span>    <span style="color:#a31515">&#34;&#34;&#34;Gaussian process model with zero mean.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> __init__(
</span></span><span style="display:flex;"><span>        self, X_train: np.ndarray, y_train: np.ndarray, kernel, sigma: float = 0.1
</span></span><span style="display:flex;"><span>    ):
</span></span><span style="display:flex;"><span>        <span style="color:#a31515">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Initialize GP model.
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        X_train: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            Training data of shape (d, n); d is dimension of feature space,
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            n is number of samples
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        y_train: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            Training labels of shape (n,)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        kernel:
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            Kernel function; takes two vectors as input and returns a scalar
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        sigma: Noise variance
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self.kernel = kernel
</span></span><span style="display:flex;"><span>        self.sigma = sigma
</span></span><span style="display:flex;"><span>        self.set_training_data(X_train, y_train)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> set_training_data(self, X_train: np.ndarray, y_train: np.ndarray):
</span></span><span style="display:flex;"><span>        <span style="color:#a31515">&#34;&#34;&#34;Initialize training data.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        self._X_train = X_train
</span></span><span style="display:flex;"><span>        self._y_train = y_train
</span></span><span style="display:flex;"><span>        self._K_train_train = self.make_covar(X_train, X_train) + self.sigma * np.eye(len(X_train))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> make_covar(self, X1, X2):
</span></span><span style="display:flex;"><span>        <span style="color:#a31515">&#34;&#34;&#34;Build covariance matrix from kernel function.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        covar = np.zeros(shape=(X1.shape[0], X2.shape[0]))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Compute pairwise kernel values</span>
</span></span><span style="display:flex;"><span>        <span style="color:#00f">for</span> i, xi <span style="color:#00f">in</span> enumerate(X1):
</span></span><span style="display:flex;"><span>            <span style="color:#00f">for</span> j, xj <span style="color:#00f">in</span> enumerate(X2):
</span></span><span style="display:flex;"><span>                covar[i, j] = self.kernel(xi, xj)
</span></span><span style="display:flex;"><span>        <span style="color:#00f">return</span> covar
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> predict(self, X_test: np.ndarray, full_covar: bool = <span style="color:#00f">False</span>):
</span></span><span style="display:flex;"><span>        <span style="color:#a31515">&#34;&#34;&#34;
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Make predictions at new test points.
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Args:
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        X_test: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            Test points of shape (d, n_star); n_star is number of test samples
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        full_covar: bool (default = False)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            Indicates whether to return full covariance matrix or just diagonal elements
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        Returns:
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        mean: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            Mean of predicted function values at points X_test; shape = (n_star,)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        cov: np.ndarray
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            Covariance of predicted function values; if full_covar,
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">            returns matrix of shape (n_star, n_star), else shape = (n_star,)
</span></span></span><span style="display:flex;"><span><span style="color:#a31515">        &#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        K_train_test = self.make_covar(self._X_train, X_test)
</span></span><span style="display:flex;"><span>        K_test_test = self.make_covar(X_test, X_test)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Pre-compute inverse of covariance for repeated use</span>
</span></span><span style="display:flex;"><span>        K_inv = np.linalg.inv(self._K_train_train)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Posterior mean</span>
</span></span><span style="display:flex;"><span>        mean = K_train_test.T @ K_inv @ self._y_train
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#008000"># Posterior covariance</span>
</span></span><span style="display:flex;"><span>        cov = K_test_test - K_train_test.T @ K_inv @ K_train_test + self.sigma * np.eye(len(X_test))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>        <span style="color:#00f">if</span> full_covar:
</span></span><span style="display:flex;"><span>            <span style="color:#00f">return</span> mean, cov
</span></span><span style="display:flex;"><span>        <span style="color:#00f">else</span>:
</span></span><span style="display:flex;"><span>            <span style="color:#00f">return</span> mean, np.diag(cov)
</span></span></code></pre></div><p>Our GP class takes training data and a kernel function as input, and computes $\K$ by evaluating the pairwise kernel values. Note that we assumed the prior mean is zero. We also have the option to specify the noise variance $\sigma$. We can handle the case of exact observations by setting this equal to zero. Then, we compute the posterior predictions using the formulas derived in the previous section. The variables <code>K_train_test</code> and <code>K_test_test</code> correspond to $\K_\ast$ and $\K_{\ast\ast}$, respectively.</p>
<p>We can now make predictions with our model and visualize the results as follows:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">def</span> squared_exponential(x, x_prime, amp=1, length=.5):
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> amp**2 * np.exp(- (x - x_prime)**2 / (2 * length**2))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>gp = ZeroMeanGP(X_data, y_data, squared_exponential)
</span></span><span style="display:flex;"><span>mean, cov = gp.predict(X_star)
</span></span><span style="display:flex;"><span>std = np.sqrt(cov)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plot_predictions(mean, std, save=<span style="color:#00f">True</span>)
</span></span></code></pre></div><div id="fig3" class="figure">
   <img src="figures/figure3.png" alt="Figure 3" style="width:80%; margin-left:auto; margin-right:auto">
</div>
<h3 id="fitting-the-hyperparameters">Fitting the hyperparameters</h3>
<p>The performance of the GP can depend drastically on the setting of the hyperparameters. In this case, our hyperparameters are the amplitude and noise of the squared exponential function, as well as the noise variance $\sigma^2$.</p>
<p>For simple problems like ours, we can often choose appropriate hyperparameter settings by inspecting a few reasonable options. There are also common methods for choosing reasonable values.<sup id="fnref:10"><a href="#fn:10" class="footnote-ref" role="doc-noteref">10</a></sup> However, for more sophisticated problems, particularly those in high-dimensional spaces, we might like to employ more robust methods for finding reasonable values for the hyperparameters.</p>
<p>We can assess how well the model fits the training data by computing the marginal likelihood, defined as</p>
<p>$$
p(\y \mid \X, \theta) = \Norm \big( \y \mid \bmu(\X; \theta), \K(\X, \X; \btheta) \big),
$$</p>
<p>where $\theta$ is some vector containing the hyperparameters. This measures the likelihood of drawing samples from our modeled distribution Thus, choosing some hyperparameters for which the likelihood is high corresponds to choosing a model which closely approximates the true distribution of the data.</p>
<p>In practice, we usually compute the marginal <em>log</em>-likelihood (MLL). This takes the form:</p>
<p>$$
\begin{align*}
\log p(\y \mid \X, \btheta) &amp;= - \frac{n}{2} \log 2\pi - \frac{1}{2} \log \Big| \K + \sigma^2\I \Big| \\[4pt]
&amp;- \frac{1}{2} \Big( \y - \bmu \Big)\T \Big( \K + \sigma^2\I \Big) \inv \Big( \y - \bmu \Big).
\end{align*}
$$</p>
<p>Note that the first term is constant with respect to the hyperparameters, so we can ignore this when maximizing the MLL. Moreover, we are assuming that the mean function is zero. Thus, we have the following simplified expression for the MLL:</p>
<p>$$
\log p(\y \mid \X, \btheta) \propto - \log \Big| \K + \sigma^2\I \Big| - \y\T \Big( \K + \sigma^2\I \Big) \inv \y.
$$</p>
<p>We can now implement a function in the <code>ZeroMeanGP</code> class to compute the MLL:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">def</span> compute_mll(self):
</span></span><span style="display:flex;"><span>        <span style="color:#a31515">&#34;&#34;&#34;Compute the MLL of training data.&#34;&#34;&#34;</span>
</span></span><span style="display:flex;"><span>        n = len(self._X_train)
</span></span><span style="display:flex;"><span>        K = self._K_train_train
</span></span><span style="display:flex;"><span>        K_inv = np.linalg.inv(K)
</span></span><span style="display:flex;"><span>        mll = - np.log(np.linalg.det(K)) - self._y_train.T @ K_inv @ self._y_train
</span></span><span style="display:flex;"><span>        <span style="color:#00f">return</span> mll
</span></span></code></pre></div><p>Then, we can define the following to optimize the MLL with respect to the hyperparameters:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">from</span> scipy.optimize <span style="color:#00f">import</span> minimize
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00f">def</span> optimize_hyperparams(X_data, y_data, init_params):
</span></span><span style="display:flex;"><span>    <span style="color:#00f">def</span> objective(params):
</span></span><span style="display:flex;"><span>        amp, length, sigma = params
</span></span><span style="display:flex;"><span>        kernel = partial(squared_exponential, amp=amp, length=length)
</span></span><span style="display:flex;"><span>        gp = ZeroMeanGP(X_data, y_data, kernel, sigma=sigma)
</span></span><span style="display:flex;"><span>        <span style="color:#00f">return</span> -gp.compute_mll()  <span style="color:#008000"># negative because we minimize</span>
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    result = minimize(objective, init_params)
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> result.x
</span></span></code></pre></div><p>Finally, let&rsquo;s find the optimal hyperparameters and once more make predictions:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>init_params = (2.0, 1.0, 0.1)
</span></span><span style="display:flex;"><span>optimized_params = optimize_hyperparams(X_data, y_data, init_params)
</span></span><span style="display:flex;"><span>print(optimized_params.round(3))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[2.308 1.331 0.111]
</span></span></code></pre></div><div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>amp, length, sigma = optimized_params
</span></span><span style="display:flex;"><span>kernel = partial(squared_exponential, amp=amp, length=length)
</span></span><span style="display:flex;"><span>gp = ZeroMeanGP(X_data, y_data, kernel, sigma=sigma)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>mean, cov = gp.predict(X_star)
</span></span><span style="display:flex;"><span>std = np.sqrt(cov)
</span></span><span style="display:flex;"><span>plot_predictions(mean, std)
</span></span></code></pre></div><div id="fig4" class="figure">
   <img src="figures/figure4.png" alt="Figure 4" style="width:80%; margin-left:auto; margin-right:auto">
</div>
<p>Thanks for reading!</p>
<h2 id="references">References</h2>
<ol>
<li>
<p>C. M. Bishop, <a href="https://www.microsoft.com/en-us/research/wp-content/uploads/2006/01/Bishop-Pattern-Recognition-and-Machine-Learning-2006.pdf"><em>Pattern Recognition and Machine Learning</em></a>, 2006.</p>
</li>
<li>
<p>C. E. Rasmussen &amp; C. K. I. Williams, <a href="https://gaussianprocess.org/gpml/chapters/RW.pdf"><em>Gaussian Processes for Machine Learning</em></a>, 2006.</p>
</li>
<li>
<p>Henry Chai&rsquo;s course, <a href="https://www.cs.cmu.edu/~hchai2/courses/10624/"><em>Bayesian Methods in Machine Learning</em></a>, 2025.</p>
</li>
</ol>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>For a given variable, I use bold-faced symbols to refer to vectors; e.g., $\x \in \R^d,$ and I use regular symbols to denote scalar values; e.g., the components of $\x$ are $(x_0, x_1, \dots, x_{d-1}).$&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>This can be computed by noting that $y = f + \epsilon$ is an <a href="../gaussian/#affine-transformation">affine transformation</a> of $\epsilon$. In general, given a Gaussian random variable $\x \sim \Norm(\bmu, \bSigma)$, the affine transformation $\y = \A\x + \b$ will also be Gaussian-dsitributed with $\y \sim \Norm(\A\bmu + \b, \A\bSigma\A\T)$.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Alternatively, we could compute the posterior directly via Bayes&rsquo; rule:
$$
p(\w \mid \cD) = \frac{p(\cD\mid\w)p(\w)}{p(\cD)}
$$&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>This integral can be done explicitly by writing out the integrand as a product of Gaussians, then completing the square in the exponent in terms of $\w$. The integrand takes the form of a Gaussian distribution over $\w$, which can be easily computed by equating it to the reciprocal of its normalization constant, and the resulting predictive distribution takes the form of a Gaussian in terms of the variables which are left over, i.e., those which were factored out of the integral.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>To see this, note that any $n\times n$ positive definite matrix has a valid eigenvalue decomposition. Thus, we can write $\bSigma = \U\Lambda\U\T$, where $\U\U\T = \I$, $\Lambda = \diag(\lambda_1, \dots, \lambda_n)$, and ${\lambda_i}$ are the eigenvalues of $\bSigma$ (also note that, since $\bSigma$ is positive definite, its eigenvalues are positive). Then, we define the matrix square root as $\bSigma^{1/2} = \U\Lambda^{1/2}\U\T$, where $\Lambda^{1/2}$ is, perhaps unsurprisingly, the matrix whose diagonal elements are given by the square roots of the eigenvalues.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>A Gram matrix is one whose elements are given by the pairwise inner products of a set of vectors. Thus, the kernel function $k$ corresponds to a valid inner product between the vectors $\psi(\x)$ and $\psi(\x\p)$.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>I got this example from Prof. Henry Chai&rsquo;s <a href="https://www.cs.cmu.edu/~hchai2/courses/10624/lectures/Lecture7_Slides.pdf">lecture slides</a> on the kernel trick.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>Some (justifiably) prefer the name &ldquo;exponentiated quadratic&rdquo;.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:9">
<p>Note that, in the case where a GP is defined over finitely many variables, this just reduces to the familiar multivariate Gaussian distribution.&#160;<a href="#fnref:9" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:10">
<p>For example when using the squared exponentional covariance function, it is commong to set the amplitude equal to the variance of the training set and the lengthscale equal to $.01$ times the amplitude. Similarly, it is common to use a constant mean function equal to the average of the training set. Moreover, one might initialize the hyperparameters to these settings, then maximize the MLL with respect to the hyperparameters under gradient descent.&#160;<a href="#fnref:10" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    
    <script src="https://giscus.app/client.js"
        data-repo="wvirany/blog"
        data-repo-id="R_kgDOMs9-zQ"
        data-category=""
        data-category-id="DIC_kwDOMs9-zc4CrdIu"
        data-mapping="pathname"
        data-strict="0"
        data-reactions-enabled="1"
        data-emit-metadata="0"
        data-input-position="top"
        data-theme="noborder_light"
        data-lang="en"
        data-loading="lazy"
        crossorigin="anonymous"
        async>
</script>

    
  </div>

  

  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      
<style>
  .katex a {
    text-decoration: none;
    color: inherit;
  }
  .katex a:hover {
    text-decoration: none;
  }
</style>

<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ],
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}",
        "\\R": "\\mathbb{R}",
        "\\bR": "\\mathbf{R}",
        "\\C": "\\mathbb{C}",
        "\\Z": "\\mathbb{Z}",
        "\\N": "\\mathbb{N}",
        "\\Q": "\\mathbb{Q}",
        "\\E": "\\mathbb{E}",
        "\\cD": "\\mathcal{D}",
        "\\var": "\\operatorname{Var}",
        "\\cov":"\\operatorname{cov}",
        "\\x": "\\mathbf{x}",
        "\\X": "\\mathbf{X}",
        "\\w": "\\mathbf{w}",
        "\\W": "\\mathbf{W}",
        "\\y": "\\mathbf{y}",
        "\\z": "\\mathbf{z}",
        "\\Z": "\\mathbf{Z}",
        "\\u": "\\mathbf{u}",
        "\\U": "\\mathbf{U}",
        "\\V": "\\mathbf{V}",
        "\\I": "\\mathbf{I}",
        "\\zv": "\\mathbf{0}",
        "\\A": "\\mathbf{A}",
        "\\a": "\\mathbf{a}",
        "\\B": "\\mathbf{B}",
        "\\b": "\\mathbf{b}",
        "\\c": "\\mathbf{c}",
        "\\D": "\\mathbf{D}",
        "\\f": "\\mathbf{f}",
        "\\M": "\\mathbf{M}",
        "\\m": "\\mathbf{m}",
        "\\bC": "\\mathbf{C}",
        "\\J": "\\mathbf{J}",
        "\\K": "\\mathbf{K}",
        "\\L": "\\mathbf{L}",
        "\\bS": "\\mathbf{S}",
        "\\bmu": "\\boldsymbol{\\mu}",
        "\\bphi": "\\boldsymbol{\\phi}",
        "\\bepsilon": "\\boldsymbol{\\epsilon}",
        "\\bSigma": "\\boldsymbol{\\Sigma}",
        "\\bLambda": "\\boldsymbol{\\Lambda}",
        "\\bPhi": "\\boldsymbol{\\Phi}",
        "\\btheta": "\\boldsymbol{\\theta}",
        "\\zero": "\\mathbf{0}",
        "\\one": "\\mathbf{1}",
        "\\T": "^{\\top}",
        "\\p": "^\\prime",
        "\\inv": "^{-1}",
        "\\ij": "_{ij}",
        "\\Norm": "\\mathcal{N}",
        "\\GP": "\\mathcal{GP}",
        "\\bmid": "\\,\\Big|\\,",
        "\\gam": "\\text{Gamma}",
        "\\nll": "\\text{NLL}",
        "\\argmin": "\\underset{#1}{\\operatorname{argmin}}",
        "\\argmax": "\\underset{#1}{\\operatorname{argmax}}\\;",
        "\\diag": "\\operatorname{diag}",
        "\\tr": "\\operatorname{tr}",
        "\\pbmu": "\\frac{\\partial}{\\partial \\boldsymbol{\\mu}}",
        "\\pSigma": "\\frac{\\partial}{\\partial \\Sigma}",
        "\\pbx": "\\frac{\\partial}{\\partial \\mathbf{x}}",
        "\\px": "\\frac{\\partial}{\\partial x}",
        "\\pbA": "\\frac{\\partial}{\\partial \\mathbf{A}}",
        "\\ml": "_\\text{ML}",
      }
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>


<script>
  
  function updateFigureNumbers() {

      const figRefs = document.querySelectorAll('.fig-ref');
      figRefs.forEach(ref => {
          const figId = ref.getAttribute('href').slice(1);
          const figElement = document.getElementById(figId);
          if (figElement) {
              const figIndex = Array.from(figures).indexOf(figElement) + 1;
              ref.textContent = `Figure ${figIndex}`;
          }
      });
  }

  
  window.addEventListener('load', updateFigureNumbers);
</script>



</html>