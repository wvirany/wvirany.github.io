<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content=""/>

<title>
    
    Some math behind the Gaussian distribution | Walter Virany
    
</title>

<link rel="canonical" href="http://localhost:1313/posts/gaussian/"/>












<link rel="stylesheet" href="/assets/combined.min.c5b19f349890ba8c8308e8d948f1754211fca49303531d62bb79927877c7df0e.css" media="all">





  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">Walter Virany</h1>
    
    
    

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /blog
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/me" >
                /me
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/why" >
                /why?
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        







<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">Some math behind the Gaussian distribution</h1>
    

    

    <p class="single-readtime">
      
      
      
      <time datetime="2025-06-09T00:00:00&#43;00:00">June 9, 2025</time>
      

      
      &nbsp; Â· &nbsp;
      36 min read
      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <!-- 
To Do:

- Reread, correct errors
- Fill in "short proofs", appendix (triangle inequality, sum/product rules)
- Fix equation numbers
- Finish references section
- Nice ToC?
- Double check resulting equations



- Content:
  - interpretation of moments
  - marginalization vs. conditioning
  - my approach: trying to do detailed derivations; other sources focus on intuition + visualization
  - Comment on frequent strategy of removing constants from sums in the exponent because they just become multiplicative scalars
  - In "moments": show the formula for the second moment of a univariate Gaussian, and reference it properly (whether it's a footnote or appendix)
-->
<style>
  details {
    border: 1px solid black;
    border-radius: 8px;
    padding: 0.5em 0.5em 0em;
    margin-bottom: 2em;
    margin-top: 2em;
  }
  
  summary {
    font-weight: bold;
    margin: -0.5em -0.5em 0;
    padding: 0.5em;
    cursor: pointer;
    border-bottom: 1px solid #aaa;
    border-radius: 8px 8px 0 0;
  }
  
  details[open] summary {
    border-bottom: 1px solid #aaa;
    margin-bottom: 0;
  }
  
  details p {
    padding-top: 0.1em;
  }
</style>
<p>Despite its ubiquity, I have frequently found myself in a state somewhere between <em>discomfort</em> and <em>panic</em> each time I am faced with the task of manipulating the Gaussian distribution, particularly in multiple dimensions. So, I&rsquo;ve taken the opportunity to work through some detailed derivations involving the Gaussian.</p>
<p>In this blog, I focus on the multivariate Gaussian distribution, beginning by reasoning about its shape and properties in high dimensions. Then, I derive some useful formulas such as conditioning, marginalization, and certain transformations.</p>
<h2 id="properties">Properties</h2>
<p>In this section, I&rsquo;ll start by considering some basic properties of the Gaussian distribution. First, I&rsquo;ll show that surfaces on which the likelihood is constant form ellipsoids. Then, we&rsquo;ll see that the multivariate Gaussian distribution is indeed normalized, making it a valid probability distribution. Finally, we&rsquo;ll consider the first- and second-order moments of the multivariate Gaussian distribution to provide an interpretation of its parameters.</p>
<p>We write the Gaussian distribution for a random vector $\x \in \R^d$ as</p>
<p>$$
\begin{equation*}
\Norm(\x \mid \bmu, \bSigma) = \frac{1}{(2\pi)^{d/2}\lvert \bSigma \rvert^{1/2}}\exp\left( -\frac{1}{2} (\x - \bmu)\T\bSigma\inv(\x - \bmu)\right),
\end{equation*}
$$</p>
<p>where $\bmu \in \R^d$ is the mean vector and $\bSigma \in \R^{d\times d}$ is the covariance matrix. Often, we choose to work with the quadratic form in the exponent, which we define</p>
<p>$$
\begin{equation}\label{1}
\Delta^2 = (\x - \bmu)\T\bSigma\inv(\x-\bmu).
\end{equation}
$$</p>
<p>$\Delta$ is called the <em>Mahalanobis distance</em>, and is analagous to the z-score of a univariate Gaussian random variable $X$:</p>
<p>$$
\begin{equation*}
Z = \frac{X - \mu}{\sigma}.
\end{equation*}
$$</p>
<p>The z-score measures the number of standard deviations $X$ is from the mean. Unlike the z-score, however, the Mahalanobis distance depends on the <em>dimension</em>; one unit in Euclidean distance from the mean in one direction may not be the same as one unit distance in another direction, in terms of $\Delta$, since the variance along one dimension of $\x$ may be different than that along another. However, if $\bSigma$ is the identity matrix, then $\Delta$ is in fact equivalent to the Euclidean distance.</p>
<p>Since the covariance matrix $\bSigma$ is real and symmetric, we can perform <a href="#appendix">eigenvalue decomposition</a> to write it in the form</p>
<p>$$
\begin{align*}
\bSigma &amp;=  \U\bLambda\U\T \\
&amp;=\sum_{i=1}^d\lambda_i\u_i\u_i\T.
\end{align*}
$$</p>
<p>Here, $\U$ is the matrix whose rows are given by $\u_i\T$, the eigenvectors of $\bSigma$, and $\bLambda = \diag(\lambda_1, \lambda_2, \ldots, \lambda_d)$ contains the corresponding eigenvalues. Note that we can choose the eigenvectors to be orthonormal, i.e., <sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup></p>
<p>$$
\begin{equation*}
\u_i\T\u_j = \delta_{ij}.
\end{equation*}
$$</p>
<p>Thus, $\U$ is an orthogonal matrix, so $\U\U\T = \I$, and $\U\T = \U\inv$. Moreover, we can easily write the inverse of the covariance matrix as</p>
<p>$$
\begin{equation*}
\bSigma\inv = \sum_{i=1}^d\frac{1}{\lambda_i}\u_i\u_i\T.
\end{equation*}
$$</p>
<p>Substituting this into $\eqref{1}$, we get</p>
<p>$$
\begin{align*}
\Delta^2 &amp;= \sum_{i=1}^d \frac{1}{\lambda_i}(\x - \bmu)\T\u_i\u_i\T(\x - \bmu) \\[1pt]
&amp;= \sum_{i=1}^d\frac{y_i^2}{\lambda_i},
\end{align*}
$$</p>
<p>where I&rsquo;ve introduced</p>
<p>$$
\begin{equation*}
y_i = \u_i\T(\x - \bmu).
\end{equation*}
$$</p>
<p>The set $\{y_i\}$ can then be seen as a transformed coordinate system, shifted by $\bmu$ and rotated by $\u_i$.<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup> Alternatively, we can write this as a vector:</p>
<p>$$
\begin{equation}\label{2}
\y = \U(\x - \bmu).
\end{equation}
$$</p>
<p>Now, all of the dependence of the Gaussian on $\x$ is determined by $\Delta^2$. Thus, the Gaussian is constant on surfaces for which $\Delta^2$ is constant. Then, let</p>
<p>$$
\begin{equation*}
\Delta^2 = \sum_{i=1}^d\frac{y_i^2}{\lambda_i} = r
\end{equation*}
$$</p>
<p>for some constant $r$. This defines the equation of an ellipsoid in $d$ dimensions.</p>
<details>
<summary>Example: Level sets of the Gaussian</summary>
<p>
In the case of the univariate Gaussian distribution, we often like to talk about the probability that an observation will fall within some range of values. For example, we might like to know the probability that a random variable will fall within one standard deviation from the mean.
<p>As an example, I&rsquo;ll consider the analagous case for a bivariate Gaussian, in which we would like to find the ellipses corresponding to the probabilities that a point falls within one, two, or three standard deviations from the mean.</p>
<p>First consider a univariate Gaussian random variable $X \sim \Norm(\mu, \sigma^2)$. The probability that $\X$ is within one standard deviation from the mean is given by</p>
<p>$$
\begin{align*}
P(\lvert X - \mu \rvert \leq \sigma) &amp;= P(-\sigma \leq X - \mu \leq \sigma) \\
&amp;= P(-1 \leq \frac{X - \mu}{\sigma} \leq 1) \\
&amp;= P(-1 \leq Z \leq 1),
\end{align*}
$$</p>
<p>where $Z = \frac{X - \mu}{\sigma}$ is a standard Gaussian random variable. Then, this probability is given by</p>
<p>$$
\begin{align*}
P(-1 \leq Z \leq 1) &amp;= P(Z \leq 1) - P(Z \leq -1) \\
&amp;= \Phi(1) - \Phi(-1)
\end{align*}
$$</p>
<p>where $\Phi(\cdot)$ is the cdf of $Z$, for which the functional values are usually determined from a <a href="https://engineering.purdue.edu/ChanGroup/ECE302/files/normal_cdf.pdf">table</a>, or when in doubt, we can make computers think for us:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-python" data-lang="python"><span style="display:flex;"><span><span style="color:#00f">from</span> scipy.stats <span style="color:#00f">import</span> norm
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(<span style="color:#a31515">f</span><span style="color:#a31515">&#34;</span><span style="color:#a31515">{</span>norm.cdf(1) - norm.cdf(-1)<span style="color:#a31515">:</span><span style="color:#a31515">.3f</span><span style="color:#a31515">}</span><span style="color:#a31515">&#34;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>0.683
</span></span></code></pre></div><p>We can do this in a similar fashion to find the probabilities that $X$ falls within $2\sigma$ or $3\sigma$ from the mean, for which the values are approximately $0.954$ and $0.997$, respectively.</p>
<p>In the multivariate case, we seek to find some constant $k$ for which</p>
<p>$$
\begin{equation*}
P(\Delta^2 \leq k^2) = 0.683,
\end{equation*}
$$</p>
<p>That is, the Mahalanobis distance for which the probability that a point $\x$ will be within this distance from the mean is $0.683$. To do so, we note that $\Delta^2$ follows a chi-squared distribution. To see this, recall the expression for $\Delta^2$:</p>
<p>$$
\begin{align*}
\Delta^2 &amp;= (\x - \bmu)\T\U\T\bLambda\inv\U(\x - \bmu) \\
&amp;= \y\T\bLambda\inv\y.
\end{align*}
$$</p>
<p>Then, $\y$ is a random vector with zero mean and diagonal covariance $\bLambda$. Since it has diagonal covariance, the elements of $\y$ are uncorrelated. In general, uncorrelated does not imply independence; however, it can be shown that, in the case of Gaussians, it does. Then, consider yet another transformation:</p>
<p>$$
\begin{equation*}
\z = \bLambda^{-1/2}\y,
\end{equation*}
$$</p>
<p>where $\bLambda^{-1/2} = \diag(\lambda_1^{-1/2}, \ldots, \lambda_d^{-1/2})$. The elements of $\z$ have been standardized, so $\z$ is a vector of standard Gaussian random variables. Then, we have</p>
<p>$$
\begin{align*}
\Delta^2 &amp;= \z\T\z \\
&amp;= z_1^2 + z_2^2 + \cdots + z_d^2.
\end{align*}
$$</p>
<p>Since each $z_i$ is independent and follows a standard Gaussian distribution, the sum $\Delta^2$ takes a chi-squared distribution with $d$ degrees of freedom. Then, consider the cdf of a chi-squared random variable with $d=2$:</p>
<p>$$
\begin{equation*}
F_{\chi_2^2} (x) = P(\chi_2^2 \leq x).
\end{equation*}
$$</p>
<p>This is analgous to using $\Phi(\cdot)$ in the univariate case. In this case, however, we know $F_{\chi_2^2} (x) = 0.683$, and we wish to find x. To do so, we can make use of the inverse cdf, otherwise known as the <em>quantile function</em>:</p>
<p>$$
\begin{equation*}
k = F_{\chi_2^2}\inv (p) = Q(p).
\end{equation*}
$$</p>
<p>We can evaluate this in several ways, but the cdf of $\chi_2^2$ takes a nice form, so we&rsquo;ll do it by hand:</p>
<p>$$
\begin{equation*}
F_{\chi^2_2}(x) = 1 - e^{-x/2}
\end{equation*}
$$</p>
<p>Hence,</p>
<p>$$
\begin{align*}
Q(p) &amp;= \log \frac{1}{(1 - p)^2} \\
Q(0.683) &amp;\approx 2.28.
\end{align*}
$$</p>
<p>Thus, the value of $k$ for which $P(\Delta^2 \leq k) = 0.683$ is approximately $2.28$. The equation for the corresponding ellipse in $y$-space is then given by</p>
<p>$$
\begin{equation*}
\frac{y_1^2}{\lambda_1} + \frac{y_2^2}{\lambda_2} = 2.28,
\end{equation*}
$$</p>
<p>which is an axis-aligned ellipse with semi-major and semi-minor axes given by $\sqrt{2.28\lambda_1}$ and $\sqrt{2.28\lambda_2}$ (the larger of the two being semi-major, the smaller being semi-minor). We can similarly evaluate $Q(.954)$ and $Q(.997)$, which give approximately $6.16$ and $11.62$, respectively. We can verify this with <code>scipy</code>:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>confidence_levels = [0.68, 0.954, 0.997]
</span></span><span style="display:flex;"><span>q_values = np.array([chi2.ppf(level, df=2) <span style="color:#00f">for</span> level <span style="color:#00f">in</span> confidence_levels])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(q_values.round(3))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[2.28 6.16 11.62]
</span></span></code></pre></div><p>Now, let&rsquo;s make this example concrete: suppose $p(\x)$ is a Gaussian with the following parameters:</p>
<p>$$
p(\x) = \Norm\left(
\begin{bmatrix}
0.4 \\
0.7
\end{bmatrix},
\begin{bmatrix}
1.0 &amp; 0.3 \\
0.3 &amp; 0.8 \\
\end{bmatrix}\right).
$$</p>
<p>First, I&rsquo;ll use <code>numpy</code> to solve for the eigenvalues:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">import</span> numpy <span style="color:#00f">as</span> np
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Define the mean and covariance matrix</span>
</span></span><span style="display:flex;"><span>mean = np.array([0.4, 0.7])
</span></span><span style="display:flex;"><span>cov = np.array([[1.0, 0.3], [0.3, 0.8]])
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Calculate eigenvalues and eigenvectors</span>
</span></span><span style="display:flex;"><span>eigenvalues, eigenvectors = np.linalg.eigh(cov)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>U = eigenvectors.T
</span></span><span style="display:flex;"><span>e1, e2 = eigenvalues
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>print(eigenvalues.round(3))
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>[0.584 1.216]
</span></span></code></pre></div><p>As previously stated, we&rsquo;d like to find the ellipses corresponding to the three confidence intervals above. I&rsquo;ll define two functions:</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span><span style="color:#00f">def</span> generate_ellipse_points(e1, e2, q, n=100):
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Create parameter t from 0 to 2pi for parametric representation</span>
</span></span><span style="display:flex;"><span>    t = np.linspace(0, 2*np.pi, n)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Parametric equations for ellipse</span>
</span></span><span style="display:flex;"><span>    y1 = np.sqrt(e1 * q) * np.cos(t)
</span></span><span style="display:flex;"><span>    y2 = np.sqrt(e2 * q) * np.sin(t)
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Return 2xn matrix of points on ellipse</span>
</span></span><span style="display:flex;"><span>    points = np.vstack([y1, y2])
</span></span><span style="display:flex;"><span>    <span style="color:#00f">return</span> points
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#00f">def</span> plot_ellipse(points, color=<span style="color:#a31515">&#39;midnightblue&#39;</span>, alpha=1, label=<span style="color:#00f">None</span>):
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Plot ellipse from 2xn points matrix</span>
</span></span><span style="display:flex;"><span>    plt.plot(points[0], points[1], color=color, linewidth=1, alpha=alpha, label=label)
</span></span></code></pre></div><p>The first function generates a $2 \times n$ matrix of points which fall on a given ellipse, and the second plots the corresponding ellipse. I&rsquo;ll use these to generate the ellipses in $y$-space, and transform them into $x$-space using the inverse transformation of $\eqref{2}$:</p>
<p>$$
\begin{equation*}
\x = \U\T\y + \bmu.
\end{equation*}
$$</p>
<div class="highlight"><pre tabindex="0" style="background-color:#fff;-moz-tab-size:4;-o-tab-size:4;tab-size:4;"><code class="language-py" data-lang="py"><span style="display:flex;"><span>plt.figure()
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Generate and plot points for each ellipse</span>
</span></span><span style="display:flex;"><span><span style="color:#00f">for</span> q <span style="color:#00f">in</span> q_values:
</span></span><span style="display:flex;"><span>    y = generate_ellipse_points(e1, e2, q)
</span></span><span style="display:flex;"><span>    x = U @ y + mean.reshape(-1, 1)
</span></span><span style="display:flex;"><span>    plot_ellipse(x)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Add mean point</span>
</span></span><span style="display:flex;"><span>plt.scatter(mean[0], mean[1], color=<span style="color:#a31515">&#39;midnightblue&#39;</span>, s=10, marker=<span style="color:#a31515">&#39;x&#39;</span>, alpha=.5)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Plot samples</span>
</span></span><span style="display:flex;"><span>x_samples = np.random.multivariate_normal(mean, cov, 1000)
</span></span><span style="display:flex;"><span>plt.scatter(x_samples[:, 0], x_samples[:, 1], alpha=0.3, s=5, color=<span style="color:#a31515">&#39;gray&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span><span style="color:#008000"># Plot the eigenvectors in the original space</span>
</span></span><span style="display:flex;"><span><span style="color:#00f">for</span> i <span style="color:#00f">in</span> range(2):
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Scale eigenvectors using eigenvalues</span>
</span></span><span style="display:flex;"><span>    scale = 1.5*np.sqrt(eigenvalues[i])
</span></span><span style="display:flex;"><span>    vec = eigenvectors[:, i] * scale
</span></span><span style="display:flex;"><span>    
</span></span><span style="display:flex;"><span>    <span style="color:#008000"># Plot eigenvector from mean point</span>
</span></span><span style="display:flex;"><span>    plt.arrow(mean[0], mean[1], vec[0], vec[1], 
</span></span><span style="display:flex;"><span>              head_width=0.1, head_length=0.1, ec=<span style="color:#a31515">&#39;firebrick&#39;</span>, fc=<span style="color:#a31515">&#39;firebrick&#39;</span>,
</span></span><span style="display:flex;"><span>              length_includes_head=<span style="color:#00f">True</span>, label=<span style="color:#a31515">f</span><span style="color:#a31515">&#39;Eigenvector </span><span style="color:#a31515">{</span>i+1<span style="color:#a31515">}</span><span style="color:#a31515">&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt.axis(<span style="color:#a31515">&#39;equal&#39;</span>)
</span></span><span style="display:flex;"><span>plt.xlabel(<span style="color:#a31515">&#39;$x_1$&#39;</span>)
</span></span><span style="display:flex;"><span>plt.ylabel(<span style="color:#a31515">&#39;$x_2$&#39;</span>)
</span></span><span style="display:flex;"><span>
</span></span><span style="display:flex;"><span>plt.tight_layout()
</span></span><span style="display:flex;"><span>plt.show()
</span></span></code></pre></div><div id="fig1" class="figure">
   <img src="figures/ellipse_plot.png" alt="ellipse_plot.png" style="width:80%; margin-left:auto; margin-right:auto">
</div>
</p>
</details>
<h3 id="normalization">Normalization</h3>
<p>Now, our goal is to show that the multivariate Gaussian distribution is normalized. Let&rsquo;s consider the Gaussian in the transformed coordinate system $\{y_i\}$. Rearranging $\eqref{2}$, we can write the transformation as</p>
<p>$$
\begin{equation*}
\x = g(\y) = \U\T\y + \bmu.
\end{equation*}
$$</p>
<p>Then, to transform from $\x$-space to $\y$-space, we use the <a href="#appendix">change of variables formula</a>, given by</p>
<p>$$
\begin{align*}
p_y(\y) &amp;= p_x(\x)\lvert \J \rvert \\[2pt]
&amp;= p_x(g(\y))\lvert \J \rvert.
\end{align*}
$$</p>
<p>Here, $\J$ is the Jacobian whose elements are given by</p>
<p>$$
\begin{equation*}
J_{ij} = \frac{\partial x_i}{\partial y_j}.
\end{equation*}
$$</p>
<p>The derivative of $\x$ with respect to $\y$ is $\U\T$, hence the elements of $\J$ are</p>
<p>$$
J_{ij} = U_{ji}.
$$</p>
<p>Then, to find the determinant of the Jacobian, we have</p>
<p>$$
\lvert \J \rvert ^2 = \lvert \U\T \rvert ^2 = \lvert \U\T \rvert \lvert \U \rvert = \lvert \U\T\U \rvert = \lvert \I \rvert = \mathbf{1}.
$$</p>
<p>Thus, $\lvert \J \rvert = \mathbf{1}$, making our transformation</p>
<p>$$
p_y(\y) = p_x(g(\y)).
$$</p>
<p>Then, we can write the Gaussian in terms of $\y$ as</p>
<p>$$
\begin{align*}
p_y(\y) &amp;= \frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}}\exp\left( -\frac{1}{2} (\x - \bmu)\T\bSigma\inv(\x - \bmu) \right).
\end{align*}
$$</p>
<p>Examining the term in the exponent, we have</p>
<p>$$
\begin{align*}
(\x - \bmu)\T\bSigma\inv(\x - \bmu)  &amp;= (\U\T\y)\T \bSigma\inv(\U\T\y) \\[1pt]
&amp;= \y\T\U (\U\T\bLambda\U)\inv \U\T\y \\[1pt]
&amp;= \y\T\U \U\inv\bLambda (\U\T)\inv \U\T\y \\[1pt]
&amp;= \y\T\bLambda\y.
\end{align*}
$$</p>
<p>So,</p>
<p>$$
\begin{align*}
p_y(\y) &amp;= \frac{1}{(2\pi)^{d/2}\lvert \bSigma \rvert ^{1/2}} \exp \left( -\frac{1}{2} \y\T\bLambda\y \right) \nonumber \\[1pt]
&amp;= \frac{1}{(2\pi)^{d/2}\lvert \bSigma \rvert ^{1/2}} \exp \left( -\frac{1}{2} \sum_{i=1}^d \frac{y_i^2}{\lambda_i} \right).
\end{align*}
$$</p>
<p>Then, it&rsquo;s useful to show that</p>
<p>$$
\begin{align*}
\lvert \bSigma \rvert &amp;= \lvert \U\bLambda\U\T \rvert = \lvert \U \rvert \lvert \bLambda \rvert \lvert \U\T \rvert = \lvert \bLambda \rvert = \prod_{i=1}^d \lambda_i,
\end{align*}
$$</p>
<p>noting that the determinant of a diagonal matrix is equal to the product of its diagonal elements, hence</p>
<p>$$
\frac{1}{\lvert \bSigma \rvert^{1/2}} = \prod_{i=1}^d \frac{1}{\sqrt{\lambda_i}}.
$$</p>
<p>Thus, noting that the exponent of a sum becomes a product of exponents, we have</p>
<p>$$
\begin{equation}\label{3}
p_y(\y) = \prod_{i=1}^d \frac{1}{\sqrt{2\pi\lambda_i}} \exp \left( -\frac{y_i^2}{2\lambda_i} \right).
\end{equation}
$$</p>
<p>Then,</p>
<p>$$
\begin{align*}
\int_\y p_y(\y) d\y &amp;= \prod_{i=1}^d \int_{y_i}  \frac{1}{\sqrt{2\pi\lambda_i}} \exp \left( -\frac{y_i^2}{2\lambda_i} \right) dy_i.
\end{align*}
$$</p>
<p>We see that each element of the product is just a univariate Gaussian over $y_i$ with mean $0$ and variance $\lambda_i$, each of which integrates to 1. This shows that $p_y(\y)$, and thus $p_x(\x)$, is indeed normalized.</p>
<h3 id="moments">Moments</h3>
<p>Finally, I will examine the first and second moments of the Gaussian. The first moment is given by</p>
<p>$$
\begin{align*}
\E[\x] &amp;= \frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2} (\x - \bmu)\T\bSigma\inv(\x - \bmu) \right) \x \, d\x \\[3pt]
&amp;= \frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2} \z\T\bSigma\inv\z \right) (\z + \bmu ) \, d\z,
\end{align*}
$$</p>
<p>where I&rsquo;ve introduced the change of variables $\z = \x - \bmu$. We can split this up as</p>
<p>$$
\frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}} \Bigg[ \int\exp\left( -\frac{1}{2} \z\T\bSigma\inv\z \right) \z \, d\z + \int\exp\left( -\frac{1}{2} \z\T\bSigma\inv\z \right) \bmu \, d\z \Bigg].
$$</p>
<p>Inspecting the first term, we see that $\exp(-\frac{1}{2}\z\T\bSigma\inv\z)$ is an even function in $\z$, and $\z$ is odd. So, the product is an odd function, hence the integral over a symmetric domain (in this case all of $\R^d$) is zero. The second term is just $\bmu$ times a Gaussian, which will integrate to 1 when multiplied by the normalization constant. Thus, we have the (perhaps unsurprising) result:</p>
<p>$$
\E[\x] = \bmu.
$$</p>
<p>Evidently, the mean parameter is just the expectation, or the average, of $\x$.</p>
<p>Now, in the univariate case, the second moment is given by $\E[x^2]$. In the multivariate case, there are $d^2$ second moments, each given by $\E[x_i x_j]$ for $i, j \in [d]$. We can group these together to form the matrix $\E[\x\x\T]$. We write this as</p>
<p>$$
\begin{align}
\E[\x\x\T] &amp;= \frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2}(\x - \bmu)\T\bSigma\inv (\x-\bmu) \right) \x\x\T d\x \nonumber \\[3pt]
&amp;= \frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2}\z\T\bSigma\inv \z \right) (\z + \bmu)(\z + \bmu)\T d\z \nonumber \\[3pt]
&amp;= \frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}}\int\exp\left( -\frac{1}{2}\z\T\bSigma\inv \z \right) (\z\z\T + 2\z\T\bmu + \bmu\bmu\T)  d\z. \label{4}
\end{align}
$$</p>
<p>By the same arguments as before, the term involving $\z\T\bmu$ will vanish due to symmetry, and the term involving $\bmu\bmu\T$ will integrate to $\bmu\bmu\T$ due to normalization. Then, we are left with the term involving $\z\z\T$. Using the eigenvalue decomposition of $\bSigma$, we can write</p>
<p align=center>
$\y = \U\z, \quad$ or $\quad \z = \U\T\y$.
</p>
<p>Recall that $\U$ is the matrix whose rows are given by the eigenvectors of $\bSigma$. So, $\U\T$ is the matrix whose <em>columns</em> are given by the eigenvectors. Thus,</p>
<p>$$
\begin{align*}
\z &amp;= \begin{bmatrix}
\u_1 &amp; \u_2 &amp; \cdots &amp; \u_d
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_d
\end{bmatrix} \\[3pt]
&amp;= \begin{bmatrix}
u_{11} &amp; u_{21} &amp; \cdots &amp; u_{d1} \\
u_{12} &amp; u_{22} &amp; \cdots &amp; u_{d2} \\
\vdots &amp; \vdots &amp; \ddots &amp; \vdots \\
u_{1d} &amp; u_{2d} &amp; \cdots &amp; u_{dd} \\
\end{bmatrix}
\begin{bmatrix}
y_1 \\
y_2 \\
\vdots \\
y_d
\end{bmatrix} \\[3pt]
&amp;= \begin{bmatrix}
u_{11}y_1 + u_{21}y_2 + \cdots + u_{d1}y_d \\
u_{12}y_1 + u_{22}y_2 + \cdots + u_{d2}y_d \\
\vdots \\
u_{1d}y_1 + u_{2d}y_2 + \cdots + u_{dd}y_d \\
\end{bmatrix} = \sum_{i=1}^d y_i\u_i,
\end{align*}
$$</p>
<p>where $u_{ij}$ is the $j$th element of $\u_i$. Then, using this expression for $\z$, and recalling the form for $p_y(\y)$ in $\eqref{3}$, we can write the first term of $\eqref{4}$ as</p>
<p>$$
\begin{align}
&amp;\frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}} \int \exp \left( - \sum_{k=1}^d \frac{y_k^2}{2\lambda_k} \right) \sum_{i=1}^d\sum_{j=1}^d y_i y_j \u_i\u_j\T d\y \nonumber \\[2pt]
&amp;\qquad= \frac{1}{(2\pi)^{d/2}\lvert\bSigma\rvert^{1/2}} \sum_{i=1}^d\sum_{j=1}^d \u_i\u_j\T  \int \exp \left( - \sum_{k=1}^d \frac{y_k^2}{2\lambda_k} \right) y_i y_j d\y. \label{5}
\end{align}
$$</p>
<p>Now, the integral takes the form</p>
<p>$$
\begin{align*}
\int \exp \left( - \sum_{k=1}^d y_k^2\right) y_i y_j d\y.
\end{align*}
$$</p>
<p>When $i\neq j$, we can expand this as the product</p>
<p>$$
\begin{align*}
&amp; \prod_{k=1}^d \int \exp(-y_k^2) y_i y_j d\y \\[2pt]
&amp;= \int \! \exp(-y_1^2) dy_1 \cdots \! \int \exp(-y_i^2) y_i dy_i \cdots \int \! \exp(-y_j^2) y_j dy_j \cdots \int \! \exp(-y_d^2) dy_d.
\end{align*}
$$</p>
<p>In this case, due to our symmetry arguments, the terms involving $y_i$ and $y_j$ vanish, and hence the integral vanishes when $i\neq j$. If $i=j$, then the double sum in $\eqref{5}$ reduces to</p>
<p>$$
\sum_{i=1}^d \u_i\u_i\T \prod_{k=1}^d \int \frac{1}{\sqrt{2\pi\lambda_k}} \exp \left( - \frac{y_k^2}{2\lambda_k} \right) y_i^2 dy_k,
$$</p>
<p>where I brought the normalization constant inside the product. The terms in the product for which $i \neq k$ are just univariate Gaussian, and hence normalize to $1$. Thus, the only term left in the product is</p>
<p>$$
\int \frac{1}{\sqrt{2\pi\lambda_k}} \exp \left( - \frac{y_i^2}{2\lambda_i} \right) y_i^2 dy_i,
$$</p>
<p>which is just the expression for the second moment of a univariate Gaussian with mean $0$ and variance $\lambda_i$. In general, the second moment of a univariate Gaussian $\Norm(x \mid \mu, \sigma^2)$ is $\mu^2 + \sigma^2$. Thus, we are left with</p>
<p>$$
\begin{align}
\E[\x\x\T] &amp;= \bmu\bmu\T + \sum_{i=1}^d \u_i\u_i\T \lambda_i \nonumber \\[1pt]
&amp;= \bmu\bmu\T + \bSigma.
\end{align}
$$</p>
<p>So, we have that the first and second moments of the Gaussian are given by $\E[\x] = \bmu$ and $\E[\x\x\T] = \bmu\bmu\T + \bSigma$, respectively.</p>
<h2 id="conditioning">Conditioning</h2>
<p>Now, suppose we have some random vector $\z \in \R^d$, specified by a Gaussian distribution:</p>
<p>$$
\z \sim \Norm(\z \mid \bmu, \bSigma).
$$</p>
<p>Then, suppose we partition $\z$ into two disjoint vectors $\x \in \R^m$ and $\y \in \R^{d-m}$:</p>
<p>$$
\z = \begin{pmatrix}
\x \\
\y
\end{pmatrix},
$$</p>
<p>and our goal is to find an expression for the conditional distribution $p(\x \mid \y)$. The parameters specifying the joint distribution can likewise be partitioned as follows:</p>
<p>$$
\bmu\ = \begin{pmatrix}
\bmu_x \\
\bmu_y
\end{pmatrix}, \quad
\bSigma = \begin{pmatrix}
\bSigma_{xx} &amp; \bSigma_{xy} \\
\bSigma_{yx} &amp; \bSigma_{yy}
\end{pmatrix}.
$$</p>
<p>Note that since $\bSigma$ is symmetric, we have $\bSigma_{xx} = \bSigma_{xx}\T, \bSigma_{yy} = \bSigma_{yy}\T$, and $\bSigma_{xy} = \bSigma_{yx}\T$. It&rsquo;s also useful to define the precision matrix $\bLambda = \bSigma\inv$, and its partitioned form: <sup id="fnref:3"><a href="#fn:3" class="footnote-ref" role="doc-noteref">3</a></sup></p>
<p>$$
\bLambda = \begin{pmatrix}
\bLambda_{xx} &amp; \bLambda_{xy} \\
\bLambda_{yx} &amp; \bLambda_{yy}
\end{pmatrix}.
$$</p>
<p>Since the inverse of a symmetric matrix is itself symmetric, we have that $\bLambda = \bLambda\T$, hence the same properties hold as the covariance matrix regarding the symmetry between constituent parts of the partitioned matrix. However, it&rsquo;s important to note that the partitioned matrices of the precision matrix are not simply the inverses of the corresponding elements of the covariance matrix. Instead, we&rsquo;ll shortly see how to take the inverse of a partitioned matrix.</p>
<p>Now, one way to find an expression for the conditional $p(\x \mid \y)$ would be to simply use the <a href="#appendix">product rule of probability</a>:</p>
<p>$$
\begin{align*}
p(\x, \y) &amp;= p(\x \mid \y) \, p(\y) \\[3pt]
\Rightarrow \quad p(\x \mid \y) &amp;= \frac{p(\x, \y)}{p(\y)}.
\end{align*}
$$</p>
<p>However, normalizing the resulting expression can be cumbersome. Instead, let&rsquo;s consider the quadratic form in the exponent of the joint distribution:</p>
<p>$$
\begin{align}
&amp; -\frac{1}{2}(\z - \bmu)\T\bSigma\inv (\z - \bmu) \nonumber \\[10pt]
&amp;\qquad= -\frac{1}{2} \begin{pmatrix}
\x - \bmu_x \\
\y - \bmu_y
\end{pmatrix}\T
\begin{pmatrix}
\bLambda_{xx} &amp; \bLambda_{xy} \\
\bLambda_{yx} &amp; \bLambda_{yy}
\end{pmatrix}
\begin{pmatrix}
\x - \bmu_x \\
\y - \bmu_y
\end{pmatrix} \nonumber \\[15pt]
&amp;\qquad= -\frac{1}{2} (\x - \bmu_x)\T\bLambda_{xx} (\x - \bmu_x) - \frac{1}{2}(\x - \bmu_x)\T \bLambda_{xy} (\y - \bmu_y) \nonumber \\
&amp;\qquad\qquad - \frac{1}{2}(\y-\bmu_y)\T\bLambda_{yx} (\x - \bmu_x) - \frac{1}{2} (\y - \bmu_y)\T \bLambda_{yy} (\y - \bmu_y) \nonumber \\[10pt]
&amp;\qquad= -\frac{1}{2} (\x - \bmu_x)\T\bLambda_{xx} (\x - \bmu_x) - (\x-\bmu_x)\T\bLambda_{xy} (\y - \bmu_y) \nonumber \\
&amp;\qquad\qquad - \frac{1}{2} (\y - \bmu_y)\T \bLambda_{yy} (\y - \bmu_y).
\end{align}
$$</p>
<p>In the last line, I use the fact that <sup id="fnref:4"><a href="#fn:4" class="footnote-ref" role="doc-noteref">4</a></sup></p>
<p>$$
(\x - \bmu_x)\T\bLambda_{xy}  (y - \bmu_y) = (\y - \bmu_y)\T\bLambda_{yx}  (\x - \bmu_x).
$$</p>
<p>I&rsquo;ll repeatedly use this fact in the following calculations to combine cross terms.</p>
<p>Evaluating the conditional $p(\x \mid \y)$ involves fixing $\y$ and treating this as a function of $\x$. Then, since the expression in $(6)$ is a quadratic function of $\x$, the resulting distribution $p(\x \mid \y)$ will also take the form of a Gaussian. So, our goal is to find the mean $\bmu_{x\mid y}$ and covariance $\bSigma_{x\mid y}$ which specify this distribution. To do so, note that in general, we can write the exponent of a Gaussian as</p>
<p>$$
\begin{equation}\label{eq:generalgaussian}
-\frac{1}{2}(\z - \bmu)\T \bSigma\inv (\z - \bmu) = -\frac{1}{2}\z\T\bSigma\inv\z + \z\T\bSigma\inv\bmu\ + c,
\end{equation}
$$</p>
<p>where $c$ denotes all the terms independent of $\z$. Thus, if we can rewrite $(6)$ in this form, we can identify the coefficients of the quadratic and linear terms in $\x$ as the mean and covariance of $p(\x \mid \y)$. This may not seem clear at first, but I think going through the process will illuminate things.</p>
<p>Expanding $(6)$ gives</p>
<p>$$
-\frac{1}{2} \x\T\bLambda_{xx} \x + \x\T\bLambda_{xx} \bmu_x - \x\T\bLambda_{xy} \y + \x\T\bLambda_{xy} \bmu_y + c,
$$</p>
<p>where $c$ again denotes all terms which do not depend on $\x$. Equating this to the general form as in the right-hand side of $\eqref{eq:generalgaussian}$, we have</p>
<p>$$
-\frac{1}{2} \x\T\bLambda_{xx} \x + \x\T\bLambda_{xx} \bmu_x - \x\T\bLambda_{xy} \y + \x\T\bLambda_{xy} \bmu_y = -\frac{1}{2}\x\T\bSigma_{x\mid y}\inv \x + \x\T\bSigma_{x\mid y}\inv \bmu_{x \mid y}.
$$</p>
<p>Immediately, we can equate the quadratic terms to see that</p>
<p>$$
\begin{equation}
\bSigma_{x\mid y}\inv = \bLambda_{xx}.
\end{equation}
$$</p>
<p>Then, collecting the linear terms, we have</p>
<p>$$
\x\T\bLambda_{xx} \bmu_x - \x\T\bLambda_{xy} \y + \x\T \bLambda_{xy} \bmu_y = \x\T\left( \bLambda_{xx} \bmu_x - \bLambda_{xy}(\y - \bmu_y) \right).
$$</p>
<p>Thus, we have</p>
<p>$$
\bSigma_{x\mid y}\inv \bmu_{x\mid y} = \bLambda_{xx} \bmu_x - \bLambda_{xy} (\y - \bmu_y),
$$</p>
<p>or, using $(8)$:</p>
<p>$$
\begin{equation}
\bmu_{x\mid y} = \bmu_x - \bLambda_{xx}\inv\bLambda_{xy} (\y - \bmu_y).
\end{equation}
$$</p>
<p>Here, we&rsquo;ve expressed the quantities $\bmu_{x\mid y}$ and $\bSigma_{x\mid y}$ in terms of $\bLambda$. Instead, we can express them in terms of $\bSigma$. To do so, we&rsquo;ll use the matrix inversion identity:</p>
<p>$$
\begin{pmatrix}
\A &amp; \B \\
\bC &amp; \D
\end{pmatrix}\inv = \begin{pmatrix}
\M &amp; -\M\B\D\inv \\
-\D\inv\bC\M &amp; \D\inv + \D\inv\bC\M\B\D
\end{pmatrix},
$$</p>
<p>where $\M$ is the <a href="#appendix">Schur complement</a>, defined</p>
<p>$$
\M = (\A - \B\D\inv\bC)\inv.
$$</p>
<p>Then, since</p>
<p>$$
\begin{pmatrix}
\bLambda_{xx} &amp; \bLambda_{xy} \\
\bLambda_{yx} &amp; \bLambda_{yy}
\end{pmatrix}\inv =
\begin{pmatrix}
\bSigma_{xx} &amp; \bSigma_{xy} \\
\bSigma_{yx} &amp; \bSigma_{yy}
\end{pmatrix},
$$</p>
<p>we have</p>
<p>$$
\begin{align*}
\bLambda_{xx} &amp;= (\bSigma_{xx} - \bSigma_{xy}\bSigma_{yy}\inv\bSigma_{yx})\inv, \\[4pt]
\bLambda_{xy} &amp;= - (\bSigma_{xx} - \bSigma_{xy}\bSigma_{yy}\inv\bSigma_{yx})\inv \bSigma_{xy} \bSigma_{yy}\inv.
\end{align*}
$$</p>
<p>Plugging these expressions into $(8)$ and $(9)$ gives</p>
<p>$$
\bSigma_{x\mid y} = \bSigma_{xx} - \bSigma_{xy}\bSigma_{yy}\inv\bSigma_{yx}
$$</p>
<p>and</p>
<p>$$
\begin{align*}
\bmu_{x\mid y} &amp;= \bmu_x + (\bSigma_{xx} - \bSigma_{xy}\bSigma_{yy}\inv\bSigma_{yx}) (\bSigma_{xx} - \bSigma_{xy}\bSigma_{yy}\inv\bSigma_{yx})\inv \bSigma_{xy} \bSigma_{yy}\inv (\y - \bmu_y) \\[2pt]
&amp;= \bmu_x - \bSigma_{xy}\bSigma_{yy}\inv (\y - \bmu_y).
\end{align*}
$$</p>
<p>Thus, $p(\x \mid \y)$ is the Gaussian distribution given by the following parameters:</p>
<p>$$
\begin{align}
\quad \bmu_{x\mid y} &amp;= \bmu_x + \bSigma_{xy}\bSigma_{yy}\inv(\y - \bmu_y) \quad \\[2pt]
\quad \bSigma_{x\mid y} &amp;= \bSigma_{xx} - \bSigma_{xy}\bSigma_{yy}\inv\bSigma_{yx}. \quad
\end{align}
$$</p>
<h2 id="marginalization">Marginalization</h2>
<p>Now, given the joint distribution $p(\x, \y)$ as above, suppose we wish to find the marginal distribution</p>
<p>$$
\begin{equation}
p(\x) = \int p(\x, \y) d\y.
\end{equation}
$$</p>
<p>Our goal, then, is to integrate out $\y$ to obtain a function of $\x$. Then, we can normalize the resulting function of $\x$ to obtain a valid probability distribution. To do so, let&rsquo;s again consider the quadratic form in the exponent given by $(6)$. First, we collect all terms which depend on $\y$:</p>
<p>$$
\begin{align}
&amp;- (\x - \bmu_x)\T\bLambda_{xy} (\y - \bmu_y) - \frac{1}{2} (\y - \bmu_y)\T\bLambda_{yy} (\y - \bmu_y) \nonumber \\[2pt]
&amp;\qquad= -\frac{1}{2} \y\T \bLambda_{yy} \y + \y\T\bLambda_{yy} \bmu_y - \y\T \bLambda_{yx} (\x - \bmu_x) \nonumber \\[2pt]
&amp;\qquad= -\frac{1}{2}\y\T \bLambda_{yy} \y + \y\T \m,
\end{align}
$$</p>
<p>where I&rsquo;ve introduced</p>
<p>$$
\m = \bLambda_{yy} \bmu_y - \bLambda_{yx} (\x - \bmu_x).
$$</p>
<p>By <a href="#appendix">completing the square</a>, we can write $(13)$ as</p>
<p>$$
-\frac{1}{2} (\y - \bLambda_{yy}\inv\m)\T \bLambda_{yy} (\y - \bLambda_{yy}\inv\m) + \frac{1}{2}\m\T\bLambda_{yy}\inv \m.
$$</p>
<p>Note that $\m$ does not depend on $\y$; however, it does depend on $\x$. Now, we&rsquo;re able to factor the integral in $(11)$ as</p>
<p>$$
\exp\big( g(\x) \big)\int \exp \left\{ -\frac{1}{2} (\y - \bLambda_{yy}\inv\m)\T \bLambda_{yy} (\y - \bLambda_{yy}\inv\m) \right\} d\y,
$$</p>
<p>where $g(\x)$ contains all the remaining terms which do not depend on $\y$. This integral is now easy to compute, since it is just an unnormalized Gaussian and will evaluate to the reciprocal of the corresponding normalization factor. Thus, the marginal distribution $p(\x)$ will have the exponential form given by $g(\x)$, and we can perform the same analysis by inspection to retrieve the values for the corresponding parameters $\bmu_x$ and $\bSigma_x$.</p>
<p>To acquire an expression for $g(\x)$, we consider all the remaining terms:</p>
<p>$$
\begin{align*}
g(\x) &amp;= -\frac{1}{2} (\x - \bmu_x)\T\bLambda_{xx} (\x - \bmu_x) + \x\T\bLambda_{xy} \bmu_y + \frac{1}{2}\m\T \bLambda_{yy}\inv \m \\[3pt]
&amp;= -\frac{1}{2} \x\T\bLambda_{xx} \x + \x\T \left( \bLambda_{xx} \bmu_x + \bLambda_{xy} \bmu_y \right) \\
&amp;\quad + \frac{1}{2} \bigg[ \big( \bLambda_{yy} \bmu_y - \bLambda_{yx}(\x-\bmu_x) \big)\T \bLambda_{yy}\inv \big( \bLambda_{yy} \bmu_y - \bLambda_{yx}(\x-\bmu_x) \big) \bigg].
\end{align*}
$$</p>
<p>Then, expanding this and dropping all constant terms with respect to $\x$, we have</p>
<p>$$
\begin{align*}
g(\x) &amp;= -\frac{1}{2}\x\T \left( \bLambda_{xx} + \bLambda_{xy}\bLambda_{yy}\inv\bLambda_{yx} \right) \x + \x\T\left( \bLambda_{xx} + \bLambda_{xy}\bLambda_{yy}\inv\bLambda_{yx} \right) \bmu_x \\[2pt]
&amp;= -\frac{1}{2}\x\T \bSigma_{xx}\inv \x + \x\T\bSigma_{xx}\inv\bmu_x,
\end{align*}
$$</p>
<p>where we have</p>
<p>$$
\bSigma_{xx} = \left( \bLambda_{xx} + \bLambda_{xy}\bLambda_{yy}\inv\bLambda_{yx} \right)\inv
$$</p>
<p>from the matrix inversion formula. Comparing this to our general form in $(7)$, we have the following expressions for the mean and covariance of the marginal $p(\x)$:</p>
<p>$$
\begin{align}
\E[\x] &amp;= \bmu_x \\
\cov[\x] &amp;= \bSigma_{xx}.
\end{align}
$$</p>
<p>That is, the mean and covariance of the marginal distribution are found by simply taking the &ldquo;slices&rdquo; of the partitioned matrices from the joint distribution which correspond to the marginal variable.</p>
<h2 id="transformations">Transformations</h2>
<p>Here I derive the resulting expressions for the new pdfs of the random variables which are the result of several different transformations of Gaussian random variables. Interestingly, we&rsquo;ll see that the Gaussian is closed under a variety of transformations.</p>
<h3 id="affine-transformation">Affine transformation</h3>
<p>Suppose $\x \in \R^d$, with $p(\x) = \Norm(\x \mid \bmu, \bSigma)$. Then, let $\y = \A\x + \b$, for some $\A \in \R^{n \times d}$ and $\b \in \R^n$. Now, we&rsquo;d like to find the density for $\y$.</p>
<p>The derivation is fairly straightforward. We start by using the change of variables formula:</p>
<p>$$
p(\y) = p(\x) \bigg| \frac{\partial\x}{\partial\y} \bigg|.
$$</p>
<p>Since this is an affine transformation, the Jacobian $\lvert \partial\x / \partial\y \rvert$ will be a constant, and hence is just a scaling factor. Thus, $p(\y)$ must take the same forma as $p(\x)$, i.e., a Gaussian.</p>
<p>Then, we can find the expressions for the mean and covariance of $p(\y)$. The mean is given by</p>
<p>$$
\bmu_y = \E[\A\x + \b] = \A \E[\x] + \b = \A \bmu + \b,
$$</p>
<p>and the covariance is given by</p>
<p>$$
\bSigma_y = \E[\y\y\T] - \bmu_y\bmu_y\T.
$$</p>
<p>Expanding the expression for the matrix of second moments gives</p>
<p>$$
\begin{align*}
\E[\y\y\T] &amp;= \E\left[ (\A\x+\b) (\A\x+\b)\T \right] \\
&amp;= \E[\A\x (\A\x)\T + 2 \b\T\A\x + \b\b\T] \\
&amp;= \A\E[\x\x\T]\A\T + 2\b\A\E[\x] + \b\b\T \\
&amp;= \A (\bSigma + \bmu\bmu\T) \A\T
\end{align*}
$$</p>
<h3 id="sum-of-gaussians">Sum of Gaussians</h3>
<p>The derivation for the sum of two Gaussian random variables is quite a bit more involved. For this, I&rsquo;ll do the derivation for the univariate case, then generalize to multiple dimensions.</p>
<p>To start, note that the density function for the sum of any two independent random variables is given by their convolution. That is, suppose $X$ and $Y$ are independent random variables, with $X \sim f_x(x)$, and $Y \sim f_Y(y)$. Let $Z = X + Y$. Then,</p>
<p>$$
f_Z(z) = \int_{-\infty}^{\infty} f_X(x)f_Y(z-x) dx.
$$</p>
<p>To see this, consider the cumulative distribution function of $Z$:</p>
<p>$$
F_Z(z) = P(Z \leq z) = P(X + Y \leq z) = P(\{(x, y) : x + y \leq z\}).
$$</p>
<p>That is, for any point $(x, y)$, this is given by the probability that this point will have a sum less than or equal to $z$. Note that in our case, $x$ and $y$ are Gaussian-distributed, so $(x,y)$ can be any point in $\R^2$. In other words, the probability that the sampled point falls below the line $y = z - x$:</p>
<div id="fig2" class="figure">
   <img src="figures/line.svg" alt="Line: y = z - x" style="width:60%; margin-left:auto; margin-right:auto">
</div>
<p>If we take $x \in (-\infty, \infty)$, then $y \in (-\infty, z - x)$, so this probability is given by the following integral: <sup id="fnref:5"><a href="#fn:5" class="footnote-ref" role="doc-noteref">5</a></sup></p>
<p>$$
\begin{align*}
F_Z(z) = \int_{-\infty}^\infty \left( \int_{-\infty}^{z-x} f_X(x) f_Y(y) dy \right) dx.
\end{align*}
$$</p>
<p>Then, since the pdf of $z$ is given by the derivative of the cdf, we have</p>
<p>$$
\begin{align*}
f_Z(z) &amp;= \frac{d}{dz} \int_{-\infty}^\infty \left( \int_{-\infty}^{z-x} f_X(x) f_Y(y) dy \right) dx \\
&amp;= \int_{-\infty}^\infty f_X(x) \left( \frac{d}{dz} \int_{-\infty}^{z-x} f_Y(y) dy \right) dx.
\end{align*}
$$</p>
<p>To be explicit, let&rsquo;s make the $u$-substitution $u = z - x$. Then, $du = dz$, and $u = y$, so</p>
<p>$$
f_Z(z) = \int_{-\infty}^\infty f_X(x) \left( \frac{d}{du} \int_{-\infty}^u f_Y(u\p) du\p \right) dx.
$$</p>
<p>This is now in the correct form to apply the fundamental theorem of calculus, which gives</p>
<p>$$
\begin{align*}
f_Z(z) &amp;= \int_{-\infty}^\infty f_X(x) f_Y(u) dx \\
&amp;= \int_{-\infty}^\infty f_X(x) f_Y(z-x) dx.
\end{align*}
$$</p>
<p>This is exactly the convolution between the functions $f_X, f_Y$!</p>
<p>Now, to compute this convolution, we could write out the expressions for the two Gaussian functions, and it would essentially be a process of completing the square in the exponents as before.</p>
<p>However, an (arguably more <em>fun</em>) alternative is to use the convolution theorem &mdash; this makes use of the fact that the Fourier transform of the convolution between two functions is just the product of the Fourier transform of each function. So, if we can find the Fourier transform for a Gaussian function, then we can simply perform multiplication in Fourier space, then transform back to real space using the inverse Fourier transform:</p>
<p>$$
f \ast g = \mathcal{F}\inv \{ F(\omega) G(\omega) \},
$$</p>
<p>where $F$ and $G$ are the Fourier transforms of $f$ and $g$, respectively &mdash; this is the convolution theorem.</p>
<p>To start, I&rsquo;ll state what&rsquo;s known as the &ldquo;Fourier transform pair&rdquo;:</p>
<p>$$
\begin{align}
f(x) &amp;= \frac{1}{2\pi} \int_{-\infty}^\infty F(\omega)e^{i\omega x}d\omega, \label{16}\tag{16} \\
F(\omega) &amp;= \int_{-\infty}^\infty f(x)e^{-i\omega x}dx.
\end{align}
$$</p>
<p>Now, suppose our function is a general Gaussian with mean $\mu$ and covariance $\sigma^2$:</p>
<p>$$
f(x) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( - \frac{(x - \mu)^2}{2\sigma^2} \right).
$$</p>
<p>Then, the Fourier transform is given by</p>
<p>$$
\begin{align*}
F(\omega) &amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^\infty \exp \left( - \frac{(x - \mu)^2}{2\sigma^2} \right) \exp(-i\omega x) dx \\
&amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \int_{-\infty}^\infty \exp \biggl\{ -\frac{1}{2\sigma^2} \left( x^2 - 2\mu x + \mu^2 + 2\sigma^2 i \omega x \right) \biggr\} dx \\
&amp;= \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( -\frac{\mu^2}{2\sigma^2} \right) \int_{-\infty}^\infty \exp \biggl\{ -\frac{1}{2\sigma^2} \left( x^2 - 2\mu x + 2\sigma^2i\omega x \right) \bigg\} dx.
\end{align*}
$$</p>
<p>By completing the square in terms of $x$ and simplifying, we get</p>
<p>$$
F(\omega) = \frac{1}{\sqrt{2\pi\sigma^2}} \exp \left( - \frac{\sigma^2\omega^2}{2} - \mu i\omega \right) \int_{-\infty}^\infty \exp \biggl\{ -\frac{(x - \mu + \sigma^2i\omega)^2}{2\sigma^2} \biggr\}dx.
$$</p>
<p>Furthermore, I&rsquo;ll make the u-substitution:</p>
<p>$$
u = \frac{1}{\sqrt{2\sigma^2}}(x-\mu), \quad du = \frac{1}{\sqrt{2\sigma^2}}dx.
$$</p>
<p>Then, we can rewrite the Fourier transform as</p>
<p>$$
F(\omega) = \frac{1}{\sqrt{\pi}} \exp \left( - \frac{\sigma^2\omega^2}{2} - \mu i\omega \right) \underbrace{\int_{-\infty}^\infty\exp \Biggl\{ - \left( u + \sqrt{\frac{\sigma^2}{2}}i\omega \right)^2 \Biggr\} du}_I.
$$</p>
<p>So, we&rsquo;d like to evaluate the integral above, which I&rsquo;ll denote $I$. To do so, I&rsquo;ll use a method from complex analysis known as contour integration (see the <a href="#references">references</a> for a resource on contour integration).<sup id="fnref:6"><a href="#fn:6" class="footnote-ref" role="doc-noteref">6</a></sup> Consider the following contour in the complex plane, which I&rsquo;ll denote $C$:</p>
<div id="fig3" class="figure">
   <img src="figures/rectangular_contour.svg" alt="rectangular_contour.svg" style="width:80%; margin-left:15%;">
</div>
<p>The basic idea behind contour integration is as follows: by <a href="https://en.wikipedia.org/wiki/Cauchy%27s_integral_theorem">Cauchy&rsquo;s theorem</a>, we know that the integral of an analytic function $f(z)$ over a closed contour $C$ is 0. That is,</p>
<p>$$
\oint_C f(z)dz = 0.
$$</p>
<p>A sufficient condition for $f(z)$ to be analytic in the complex plane is that it is differentiable at each point $z \in \C$. Then, we can decompose the integral along $C$ into each of its constituent parts:</p>
<p>$$
\begin{align*}
\oint_C f(z) dz &amp;= \int_{-a}^a f(x)dx + \int_0^b f(a + iy) dy \\
&amp;+ \int_a^{-a} f(x + bi) dx + \int_b^0 f(-a + iy)dy,
\end{align*}
$$</p>
<p>where I&rsquo;ve used the fact that we can write a general complex number as $z = x + iy$. Then, if we let $f(z) = e^{-z^2}$ and set the RHS equal to zero, we have</p>
<p>$$
\int_{-a}^a e^{-x^2} dx + \int_a^{-a} e^{-(x + ib)^2} dx + \int_0^b e^{-(a+iy)^2}dy + \int_b^0 e^{-(-a+iy)^2}dy = 0.
$$</p>
<p>Note that the second term on the LHS takes the same form as $I$ in the limit as $a \to \infty$. Our goal will be to solve for this. By rearranging and taking the absolute value of both sides, we can write</p>
<p>$$
\biggl| \int_{-a}^a e^{-x^2}dx - \int_{-a}^a e^{-(x+ib)^2}dx \biggr| = \biggl| \int_0^be^{-(-a+iy)^2}dy - \int_0^be^{-(a+iy)^2}dy \biggr|.
$$</p>
<p>Now, using the <a href="#appendix">triangle inequality</a>, we have</p>
<p>$$
\biggl| \int_{-a}^a e^{-x^2}dx - \int_{-a}^a e^{-(x+ib)^2}dx \biggr| \leq \int_0^b \lvert e^{-(-a+iy)^2} \rvert dy + \int_0^b \lvert e^{-(a+iy)^2}\rvert dy.
$$</p>
<p>Then, examining the term on the RHS:</p>
<p>$$
\begin{align*}
&amp;\int_0^b \lvert e^{-(-a+iy)^2} \rvert dy + \int_0^b \lvert e^{-(a+iy)^2}\rvert dy\\
&amp;= \int_0^b \lvert e^{-a^2 + 2aiy + y^2} \rvert dy + \int_0^b \lvert e^{-a^2 - 2aiy + y^2} \rvert dy \\
&amp;= e^{-a^2} \left( \int_0^b e^{y^2} \underbrace{\lvert e^{2aiy} \rvert}_1 dy + \int_0^b e^{y^2} \underbrace{\lvert e^{-2aiy} \rvert}_1 dy \right) \\
&amp;= 2e^{-a^2} \int_0^b e^{y^2} dy.
\end{align*}
$$</p>
<p>Now, using integration by parts, we can show that</p>
<p>$$
\int_0^b e^{y^2}dy = be^{b^2} - 2\int_0^b y^2e^{y^2}dy.
$$</p>
<p>Since the second term on the RHS is positive for any $b$, we have that</p>
<p>$$
\int_0^be^{y^2}dy \leq be^{b^2},
$$</p>
<p>and hence is bounded by some constant in terms of $b$. Thus, when we take the limit as $a \to \infty$, this term will vanish, and we are left with</p>
<p>$$
\biggl| \int_{-\infty}^\infty e^{-x^2}dx - \int_{-\infty}^\infty e^{-(x+ib)^2}dx \biggr| \leq 0.
$$</p>
<p>Since the term on the LHS is nonnegative, it must be zero, hence</p>
<p>$$
\int_{-\infty}^\infty e^{-(x+ib)^2}dx = \int_{-\infty}^\infty e^{-x^2}dx.
$$</p>
<p>The term on the LHS takes the exact form of $I$ &mdash; independent of $b$! Thus,</p>
<p>$$
I = \int_{-\infty}^\infty e^{-x^2}dx.
$$</p>
<p>To solve this integral, we can square both sides and convert to polar coordinates:</p>
<p>$$
\begin{align*}
I^2 &amp;= \left( \int_{-\infty}^\infty e^{-x^2}dx \right) \left( \int_{-\infty}^\infty e^{-y^2}dy \right) \\
&amp;= \int_{-\infty}^\infty\int_{-\infty}^\infty e^{-(x^2 + y^2)}dxdy \\
&amp;= \int_0^{2\pi}\int_0^\infty e^{-r^2} rdrd\theta \\
&amp;= 2\pi \int_0^\infty re^{-r^2} dr.
\end{align*}
$$</p>
<p>By making the $u$-substitution $u = r^2$, we have</p>
<p>$$
\begin{align*}
I^2 &amp;= \pi \int_0^\infty e^{-u}du \\
&amp;= \pi.
\end{align*}
$$</p>
<p>Thus, $I = \sqrt{\pi}$. Plugging this back into our expression for the Fourier transform gives</p>
<p>$$
F(\omega) = \exp \left( -\frac{\sigma^2\omega^2}{2} \right)\exp \left( -\mu i\omega \right).
$$</p>
<p>This is the general form for the Fourier transform of a Gaussian with mean $\mu$ and variance $\sigma^2$. Now, we might recall our original goal: to compute the pdf of $Z = X + Y$, where</p>
<p>$$
f_X(x) = \Norm(\mu_X, \sigma_X^2), \quad f_Y(y) = \Norm(\mu_Y, \sigma_Y^2).
$$</p>
<p>As I previously stated, the pdf of $Z$ is given by</p>
<p>$$
f_Z(z) = \int_{-\infty}^\infty f_X(x) f_Y(z-x)dx.
$$</p>
<p>Taking the Fourier transform,</p>
<p>$$
\begin{align*}
F_Z(\omega) &amp;= F_X(\omega)F_Y(\omega) \\
&amp;= \exp \left( - \frac{\sigma_X^2\omega^2}{2} \right) \exp \left( -\mu_X i\omega \right) \exp \left( - \frac{\sigma_Y^2\omega^2}{2} \right) \exp \left( -\mu_Y i\omega \right) \\
&amp;= \exp \left( -\frac{\left(\sigma_X^2 + \sigma_Y^2 \right)\omega^2}{2}\right) \exp \bigl( - \left( \mu_X + \mu_Y \right)i\omega \bigr).
\end{align*}
$$</p>
<p>We see this takes the form as a Fourier transform of a Gaussian with mean $\mu_X + \mu_Y$ and variance $\sigma_X^2 + \sigma_Y^2$, hence</p>
<p>$$
f_Z(z) = \Norm(\mu_X + \mu_Y, \sigma_X^2 + \sigma_Y^2).
$$</p>
<p>Thus, the sum of two Gaussians results in another Gaussian, whose parameters are the sums of those of each individual Gaussian.</p>
<p>Now, I will argue that this holds true for multivariate Gaussians as well. Suppose $\x, \y \in \R^d$ are Gaussian-distributed random vectors, with $\x \sim \Norm(\bmu_X, \bSigma_X)$ and $\y \sim \Norm(\bmu_Y, \bSigma_Y)$, and let $\z = \x + \y$. The pdf of $\z$ is still given by the convolution:</p>
<p>$$
f_Z(\z) = \int_{\R^d} f_X(\x)f_Y(\z - \x)d\x.
$$</p>
<p>By expanding this integral, we can show that it takes the general exponential-quadratic form of a multivariate Gaussian; however, I&rsquo;ll avoid doing this as it is very similar to the derivations from previous sections. Then, to find the parameters of this distribution we can compute the first and second moments of $\z$: <sup id="fnref:7"><a href="#fn:7" class="footnote-ref" role="doc-noteref">7</a></sup></p>
<p>$$
\E[\z] = \E[\x + \y] = \E[\x] + \E[\y] = \bmu_X + \bmu_Y,
$$</p>
<p>and</p>
<p>$$
\begin{align*}
\E[\z\z\T] &amp;= \E[(\x + \y)(\x + \y)\T] \\
&amp;= \E[\x\x\T + 2\x\y\T + \y\y\T] \\
&amp;= \E[\x\x\T] + \E[\y\y\T] + 2\E[\x] \E[\y\T].
\end{align*}
$$</p>
<p>where, in the last line, I used the fact that $\x$ and $\y$ are independent to write $\E[\x\y\T] = \E[\x]\E[\y\T]$. Then,</p>
<p>$$
\begin{align*}
\E[\z\z\T] &amp;= \bmu_X\bmu_X\T + \bSigma_X + \bmu_Y\bmu_Y\T + \bSigma_Y + 2\bmu_X\bmu_Y\T \\
&amp;= \left( \bmu_X + \bmu_Y \right)(\bmu_X + \bmu_Y)\T + \bSigma_X + \bSigma_Y.
\end{align*}
$$</p>
<p>Thus, we see that $\z \sim \Norm\left( \bmu_X + \bmu_Y, \bSigma_X + \bSigma_Y \right)$.</p>
<h2 id="references">References</h2>
<p>The content of this post largely follows section 2.3 of Bishop&rsquo;s <em>Pattern Recognition and Machine Learning</em>. I also found myself frequently cross-referencing Murphy&rsquo;s <em>Probabilistic Machine Learning: An Introduction</em>, as well as using this for some of the mathematical concepts, of which the important ones can be found detailed in the appendix below.</p>
<p>For the discussion on the sum of two Gaussians, the convolution operation, and the Fourier transform of the Gaussian, I referenced the following:</p>
<ul>
<li><a href="">Wikipedia</a> &mdash; wikipedia!</li>
<li><a href="https://math.libretexts.org/Bookshelves/Differential_Equations/Introduction_to_Partial_Differential_Equations_(Herman)/09%3A_Transform_Techniques_in_Physics/9.06%3A_The_Convolution_Operation">This website</a> &mdash; This gives a great tutorial on the convolution operation and also shows how to compute the convolution of two zero-mean Gaussians</li>
<li><a href="https://www.ee.iitb.ac.in/~belur/ee210/current-tutorials-quizzes/Kenneth-Zeger-UCSD-Gaussian-polar-coordinate.pdf">This guy&rsquo;s notes</a> &mdash; this is a nice tutorial on taking the Fourier transform of a zero-mean Gaussian.</li>
<li><a href="">Complex textbook</a> &mdash; this is the main text I used for complex analysis; chapter 4 deals with contour integration as well as the Fourier transform.</li>
</ul>
<p>However, I filled in a few of the gaps, and generalized the results for the Fourier transform of the Gaussian to a general Gaussian with nonzero mean.</p>
<h2 id="appendix">Appendix</h2>
<details>
  <summary>Eigenvalue decomposition</summary>
  <p>
  Given a square matrix $\A \in \R^{n\times n}$, we say $\lambda$ is an eigenvalue of $\A$ with corresponding eigenvector $\u \in \R^n$ if
<p>$$
\A\u = \lambda\u, \qquad \u \neq \zero.
$$</p>
<p>We can write the collection of eigenvector equations for each eigenpair in the following matrix equation:</p>
<p>$$
\A\U = \U\bLambda,
$$</p>
<p>where $\U = \begin{bmatrix} \u_1 &amp;\u_2 &amp;\cdots &amp;\u_n\end{bmatrix}$ contains the eigenvectors $\u_i$ in its columns, and $\bLambda = \diag(\lambda_1, \lambda_2, \dots, \lambda_n)$.</p>
<p>If $\A$ is <em>nonsingular</em>, there will be $n$ nonzero eigenvalues, and thus $n$ linearly independent eigenvectors. Furthermore, the eigenvalues of $\A\inv$ will be given by $1/\lambda_i$, for $i \in [n]$. Similarly, since the eigenvectors are linearly independent, then $\U$ is invertible, hence we can write</p>
<p>$$
\begin{equation}\label{A.1}\tag{A.1}
\A = \U\bLambda\U\inv.
\end{equation}
$$</p>
<p>In this case, $\A$ is <em>diagonalizable</em>. <sup id="fnref:8"><a href="#fn:8" class="footnote-ref" role="doc-noteref">8</a></sup></p>
<p>Furthermore, if $\A$ is <em>real</em> and <em>symmetric</em>, then the eigenvalues of $\A$ are real, and the corresponding eigenvectors are orthonormal, i.e.</p>
<p>$$
\u_i\u_j\T = \delta_{ij}.
$$</p>
<p>In matrix form, this is written as</p>
<p>$$
\U\T\U = \U\U\T = \I.
$$</p>
<p>Hence, $\U$ is an orthogonal matrix, and $\U\T = \U\inv$. Then, from $\eqref{A.1}$, we can write</p>
<p>$$
\begin{align*}
\A &amp;= \U\bLambda\U\T \\
&amp;= \begin{pmatrix}
\u_1 &amp;\u_2 &amp;\cdots &amp;\u_n
\end{pmatrix}
\begin{pmatrix}
\lambda_1 &amp; &amp; &amp; \\
&amp; \lambda_2 &amp; &amp; \\
&amp; &amp; \ddots &amp; \\
&amp; &amp; &amp; \lambda_n
\end{pmatrix}
\begin{pmatrix}
\u_1\T \\[2pt]
\u_2\T \\
\vdots \\
\u_n\T
\end{pmatrix} \\
&amp;= \sum_{i=1}^n \lambda_i \u\u_i\T.
\end{align*}
$$</p>
<p>Once we have diagonalized a matrix $\A$, it is easy to invert:
$$
\begin{align*}
\A\inv &amp;= (\U\bLambda\U\T)\inv \\
&amp;= \U\bLambda\inv\U\T \\
&amp;= \sum_{i=1}^n\frac{1}{\lambda_i}\u_i\u_i\T.
\end{align*}
$$</p>
  </p>
</details>
<details>
  <summary>Change of variables formula</summary>
  <p>
  Suppose we have some random variable $x$ with distribution $p(x)$. Then, suppose we transform $x$ via some invertible function $y = f(x)$, where we define the inverse $g = f\inv$, hence $x = g(y)$. Then, suppose we'd like to find the distribution $p(y)$. To do so, we need to maintain the normalization condition:
<p>$$
\int p(y)dy = \int p(x) dx = 1.
$$</p>
<p>Differentiating on both sides with respect to $y$ gives</p>
<p>$$
\frac{d}{dy} \int p(y) dy = \frac{d}{dy} \int p(x) dx.
$$</p>
<p>Using the substitution $x = g(y)$, $dx = g^\prime(y)dy$, we have</p>
<p>$$
\begin{align*}
p(y) &amp;= \frac{d}{dy} \int p(g(y)) g^\prime(y) dy \\
&amp;= p(g(y))g^\prime(y).
\end{align*}
$$</p>
<p>Then, since $g(y) = x$, we have that $g^\prime(y) = dx/dy$, hence</p>
<p>$$
p(y) = p(x) \frac{dx}{dy}.
$$</p>
<p>Note that, in general, the distributions $p(x)$ and $p(y)$ must be nonegative; however, the derivative $dx/dy$ can be negative. Thus, we ensure nonnegativity by taking the absolute value:</p>
<p>$$
p(y) = p(x) \bigg| \frac{dx}{dy} \bigg|.
$$</p>
<p>This defines the <em>change of variables</em> formula. To generalize to the multivariate case, we replace $dx/dy$ with the Jacobian of $g$.</p>
<p>For a detailed and intuitive discussion of the change of variables formula, I like <a href="https://blog.evjang.com/2018/01/nf1.html">Eric Jang&rsquo;s blog post</a>.</p>
  </p>
</details>
<details>
  <summary>Sum and product rules of probability</summary>
  <p>
  </p>
</details>
<details>
  <summary>The Schur complement</summary>
  <p>
  Suppose we have some partitioned matrix
<p>$$
\M = \begin{pmatrix}
\A &amp; \B \\
\bC &amp; \D
\end{pmatrix},
$$</p>
<p>and we&rsquo;d like to compute $\M\inv$. First, we can diagonalize the matrix as follows:</p>
<p>$$
\begin{pmatrix}
\I &amp; -\B\D\inv \\
\zero &amp; \I
\end{pmatrix}
\begin{pmatrix}
\A &amp; \B \\
\bC &amp; \D
\end{pmatrix} = \begin{pmatrix}
\M/\D &amp; \zero \\
\bC &amp; \D
\end{pmatrix},
$$</p>
<p>where I&rsquo;ve defined the <em>Schur complement</em></p>
<p>$$
\M/\D = \A - \B\D\inv\bC.
$$</p>
<p>Then,</p>
<p>$$
\begin{pmatrix}
\M / \D &amp; \zero \\
\bC &amp; \D
\end{pmatrix}
\begin{pmatrix}
\I &amp; \zero \\
-\D\inv\bC &amp; \I
\end{pmatrix} = \begin{pmatrix}
\M/\D &amp; \zero \\
\zero &amp; \D
\end{pmatrix}.
$$</p>
<p>So, we have</p>
<p>$$
\underbrace{
\begin{pmatrix}
\I &amp; -\B\D\inv \\
\zero &amp; \I
\end{pmatrix}
}_X
\underbrace{
\begin{pmatrix}
\A &amp; \B \\
\bC &amp; \D
\end{pmatrix}
}_M \underbrace{
\begin{pmatrix}
\I &amp; \zero \\
-\D\inv\bC &amp; \I
\end{pmatrix}
}_Z = \underbrace{
\begin{pmatrix}
\M /\D &amp; \zero \\
\zero &amp; \D
\end{pmatrix}
}_W.
$$</p>
<p>Taking the inverse of both sides, we get
$$
\begin{align*}
(\X\M\Z)\inv &amp;= \W\inv \\
\Z\inv \M\inv \X\inv &amp;= \W\inv \\
\M\inv &amp;= \Z\W\inv\X.
\end{align*}
$$</p>
<p>which gives</p>
<p>$$
\begin{align*}
\M\inv &amp;= \begin{pmatrix}
\I &amp; \zero \\
-\D\inv\bC &amp; \I
\end{pmatrix}
\begin{pmatrix}
\M /\D &amp; \zero \\
\zero &amp; \D
\end{pmatrix}
\begin{pmatrix}
\I &amp; -\B\D\inv \\
\zero &amp; \I
\end{pmatrix} \\
&amp;= \begin{pmatrix}
(\M /\D)\inv &amp; - (\M /\D)\inv\B\D\inv \\
-\D\inv\bC(\M /\D)\inv &amp; \D\inv + \D\inv\bC(\M \D)\inv\B\D\inv
\end{pmatrix}.
\end{align*}
$$</p>
<p>Alternatively, we could have decomposed the matrix $\M$ in terms of $\A$, giving the Schur complement with respect to $\A$:</p>
<p>$$
\mathbf{M/E = H - GE^{-1}F}
$$ which would yield</p>
<p>$$
\M\inv = \begin{pmatrix}
\A\inv + \A\inv\B(\M /\A)\inv\bC\A\inv &amp; -\A\inv\B(\M /\A)\inv \\
-(\M /\A)\inv\bC\A\inv &amp; (\M /\A)\inv
\end{pmatrix}.
$$</p>
  </p>
</details>
<details>
  <summary>Completing the square</summary>
  <p>
  Suppose we have some quadratic function $f:\R\to\R$ given by
<p>$$
f(x) = ax^2 + bx + c.
$$</p>
<p>Then, we can rewrite this in the form</p>
<p>$$
f(x) = a(x-h)^2 + k,
$$</p>
<p>where</p>
<p>$$
h = -\frac{b}{2a}, \qquad k = c - \frac{b^2}{4a}.
$$</p>
<p>Instead, suppose $f:\R^n \to \R$ is now a quadratic function of some vector $\x$ given by</p>
<p>$$
f(\x) = \x\T\A\x + \x\T\b + \c.
$$</p>
<p>Again, we can rewrite this as</p>
<p>$$
f(\x) = (\x - \mathbf{h})\T\A(\x - \mathbf{h}) + \mathbf{k},
$$</p>
<p>where</p>
<p>$$
\mathbf{h} = -\frac{1}{2}\A\inv\b, \qquad \mathbf{k} = \c - \frac{1}{4} \b\T\A\inv\b.
$$</p>
<p>These can easily be verified by substituting the expressions for $\mathbf{h}$ and $\mathbf{k}$ into $f$.</p>
  </p>
</details>
<details>
  <summary>Triangle inequality</summary>
  <p>
  </p>
</details>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>Here I&rsquo;ve used the Kronecker delta for notational simplicity:
$$
\delta_{ij} = \begin{cases}
1 &amp;\ i=j, \\
0 &amp;\ i\neq j.
\end{cases}
$$&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>Note that since $\U$ is an orthogonal matrix, a linear transformation defined by $\U$ preserves the length of the vector which it transforms, and thus is either a rotation, reflection, or a combination of both.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:3">
<p>Most literature notation uses $\bLambda$ for the precision matrix. Not that we have overdefined $\bLambda$, since it also refers to the matrix containing the eigenvalues of $\bSigma$ in our definition for the eigenvalue decomposition. However, for the rest of this blog, $\bLambda$ will refer to the precision matrix.&#160;<a href="#fnref:3" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:4">
<p>To see this, consider the product between two vectors $\a \in \R^m$ and $\b \in \R^n$ defined by $\a\T\A\b$ for some matrix $\A \in \R^{m \times n}$. Then, since the resulting product is a scalar, we have $\a\T\A\b  = \b\T\A\T\a$.&#160;<a href="#fnref:4" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:5">
<p>Alternatively, we could have taken $y \in (-\infty, \infty)$, and $x \in (-\infty, z - y)$. This would give the following expression for the convolution:
$$
\begin{align*}
f_Z(z) &amp;= \int_{-\infty}^\infty \left( \int_{-\infty}^{z-y} f_X(x)f_Y(y) dx \right) dy \\
&amp;= \int_{-\infty}^\infty f_X(z-y)f_Y(y)dy
\end{align*}
$$
It can be shown, however, it is fairly straightforward to show that the convolution is commutative, i.e., $f * g = g * f$.&#160;<a href="#fnref:5" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:6">
<p>I&rsquo;ll point out that, while fascinating, this is just a method of integration for those who are curious; it is not the primary focus of my blog, which is to better understand the Gaussian distribution.&#160;<a href="#fnref:6" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:7">
<p>Note that, in general, finding the first and second moments is not enough to establish that the distribution is a Gaussian. For example, suppose we have two random variables $X, Y$ sampled from a Rademacher distribution (i.e., they take on values $\pm 1$ with equal probabilities). Each of these have mean $0$ and variance $1$. Then, the sum $Z = X + Y$ takes on values $\{ -2, 0, 2 \}$, with respective probabilities $\{ 1/4, 1/2, 1/4 \}$. $Z$ will then have mean $0$ and variance $2$ &mdash; the same parameters we would expect for the sum of two Gaussians. Thus, we must first show that $Z$ is Gaussian distributed; then, we can find it&rsquo;s parameters by computing the moments.&#160;<a href="#fnref:7" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:8">
<p>From this, we see that a matrix $\A$ is diaongalizable if it is <em>square</em> and <em>nonsingular</em>. We see that it needs to be square since otherwise the notion of eigenvalues is not well-defined. This is because the eigenvalues are found by solving the characteristic equation $\det(\A - \lambda\I) = 0$, and which the determinant is only defined for square matrices. Moreover, $\A$ needs to be nonsingular so that the eigenvectors are linearly independent. In this case, the matrix $\U$ is nonsingular and hence invertible, allowing us to write $\A = \U\bLambda\U\inv$.&#160;<a href="#fnref:8" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    
  </div>

  

  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      
<style>
  .katex a {
    text-decoration: none;
    color: inherit;
  }
  .katex a:hover {
    text-decoration: none;
  }
</style>

<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.8/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ],
      trust: (context) => ['\\htmlId', '\\href'].includes(context.command),
      macros: {
        "\\eqref": "\\href{###1}{(\\text{#1})}",
        "\\ref": "\\href{###1}{\\text{#1}}",
        "\\label": "\\htmlId{#1}{}",
        "\\R": "\\mathbb{R}",
        "\\bR": "\\mathbf{R}",
        "\\C": "\\mathbb{C}",
        "\\Z": "\\mathbb{Z}",
        "\\N": "\\mathbb{N}",
        "\\Q": "\\mathbb{Q}",
        "\\E": "\\mathbb{E}",
        "\\cD": "\\mathcal{D}",
        "\\var": "\\operatorname{Var}",
        "\\cov":"\\operatorname{cov}",
        "\\x": "\\mathbf{x}",
        "\\X": "\\mathbf{X}",
        "\\w": "\\mathbf{w}",
        "\\W": "\\mathbf{W}",
        "\\y": "\\mathbf{y}",
        "\\z": "\\mathbf{z}",
        "\\Z": "\\mathbf{Z}",
        "\\u": "\\mathbf{u}",
        "\\U": "\\mathbf{U}",
        "\\V": "\\mathbf{V}",
        "\\I": "\\mathbf{I}",
        "\\A": "\\mathbf{A}",
        "\\a": "\\mathbf{a}",
        "\\B": "\\mathbf{B}",
        "\\b": "\\mathbf{b}",
        "\\c": "\\mathbf{c}",
        "\\D": "\\mathbf{D}",
        "\\M": "\\mathbf{M}",
        "\\m": "\\mathbf{m}",
        "\\bC": "\\mathbf{C}",
        "\\J": "\\mathbf{J}",
        "\\K": "\\mathbf{K}",
        "\\L": "\\mathbf{L}",
        "\\bS": "\\mathbf{S}",
        "\\bmu": "\\boldsymbol{\\mu}",
        "\\bphi": "\\boldsymbol{\\phi}",
        "\\bepsilon": "\\boldsymbol{\\epsilon}",
        "\\bSigma": "\\boldsymbol{\\Sigma}",
        "\\bLambda": "\\boldsymbol{\\Lambda}",
        "\\bPhi": "\\boldsymbol{\\Phi}",
        "\\zero": "\\mathbf{0}",
        "\\one": "\\mathbf{1}",
        "\\T": "^{\\top}",
        "\\p": "^\\prime",
        "\\inv": "^{-1}",
        "\\ij": "_{ij}",
        "\\Norm": "\\mathcal{N}",
        "\\gam": "\\text{Gamma}",
        "\\nll": "\\text{NLL}",
        "\\argmin": "\\underset{#1}{\\operatorname{argmin}}",
        "\\argmax": "\\underset{#1}{\\operatorname{argmax}}",
        "\\diag": "\\operatorname{diag}",
        "\\tr": "\\operatorname{tr}",
        "\\pbmu": "\\frac{\\partial}{\\partial \\boldsymbol{\\mu}}",
        "\\pSigma": "\\frac{\\partial}{\\partial \\Sigma}",
        "\\pbx": "\\frac{\\partial}{\\partial \\mathbf{x}}",
        "\\px": "\\frac{\\partial}{\\partial x}",
        "\\pbA": "\\frac{\\partial}{\\partial \\mathbf{A}}",
        "\\ml": "_\\text{ML}",
      }
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>


<script>
  
  function updateFigureNumbers() {

      const figRefs = document.querySelectorAll('.fig-ref');
      figRefs.forEach(ref => {
          const figId = ref.getAttribute('href').slice(1);
          const figElement = document.getElementById(figId);
          if (figElement) {
              const figIndex = Array.from(figures).indexOf(figElement) + 1;
              ref.textContent = `Figure ${figIndex}`;
          }
      });
  }

  
  window.addEventListener('load', updateFigureNumbers);
</script>



</html>