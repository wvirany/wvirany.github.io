<!DOCTYPE html>
<html lang="en-us"
  dir="ltr">

  <head><script src="/livereload.js?mindelay=10&amp;v=2&amp;port=1313&amp;path=livereload" data-no-instant defer></script>
    <meta charset="utf-8">
<meta name="viewport" content="width=device-width">



<link rel="icon" type="image/ico" href="http://localhost:1313//favicon.ico">
<link rel="icon" type="image/png" sizes="16x16" href="http://localhost:1313//favicon-16x16.png">
<link rel="icon" type="image/png" sizes="32x32" href="http://localhost:1313//favicon-32x32.png">
<link rel="icon" type="image/png" sizes="192x192" href="http://localhost:1313//android-chrome-192x192.png">
<link rel="apple-touch-icon" sizes="180x180" href="http://localhost:1313//apple-touch-icon.png">

<meta name="description" content=""/>

<title>
    
    Protein Representation Learning with a VQ-VAE | Walter Virany
    
</title>

<link rel="canonical" href="http://localhost:1313/posts/vq-vae/"/>












<link rel="stylesheet" href="/assets/combined.min.c5b19f349890ba8c8308e8d948f1754211fca49303531d62bb79927877c7df0e.css" media="all">





  </head>

  

  
  
  

  <body class="light">

    <div class="content">
      <header>
        

<div class="header">

    

    <h1 class="header-title">Walter Virany</h1>

    <div class="flex">
        

        
        
      
        <p class="small ">
            <a href="/" >
                /home
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/about" >
                /about
            </a>
        </p>
        
      
        <p class="small ">
            <a href="/why" >
                /why?
            </a>
        </p>
        
        
    </div>

    

</div>

      </header>

      <main class="main">
        







<div >

  <div class="single-intro-container">

    

    <h1 class="single-title">Protein Representation Learning with a VQ-VAE</h1>
    

    

    <p class="single-readtime">
      

      
      &nbsp; Â· &nbsp;
      3 min read
      
    </p>

  </div>

  

  

  

  

  <div class="single-content">
    <p>In this blog, I build a VQ-VAE for for protein structure tokenization which learns meaningful representations of proteins.</p>
<h2 id="variational-autoencoders">Variational Autoencoders</h2>
<p>First, I&rsquo;ll start with an overview of Variational Autoencoders (VAE), as the VQ-VAE is a natural extension.</p>
<p>A VAE consists of two main components: an encoder and a decoder network. The encoder network projects samples into a low-dimensional latent space<sup id="fnref:1"><a href="#fn:1" class="footnote-ref" role="doc-noteref">1</a></sup>, which usually takes the form of a standard Gaussian, whereas the decoder network reconstructs data samples from these low-dimensional representations. Thus, the training objective aims to maximize the likelihood of generated samples coming from the true data distribution with the decoder while accurately modeling the latent space with the encoder.</p>
<p>Once trained, new data points can be generated by sampling from the latent distribution, and passing samples through the decoder. Alternatively, the encoder network can be used to produce meaningful low-dimensional representations of data, which is useful for data compression, representation learning, and other downstream tasks.</p>
<h4 id="variational-inference-and-the-evidence-lower-bound">Variational Inference and the Evidence Lower Bound</h4>
<p>Suppose our data $\mathbf{x}$ is generated by some random process, which depends on a continuous random variable $\mathbf{z}$. First, $\mathbf{z}$ is sampled from a prior distribution $p(\mathbf{z})$. Then, the data samples are generated from a conditional distribution $p(\mathbf{x}|\mathbf{z})$. We assume both of these distributions to be Gaussian. Unfortunately, we don&rsquo;t know these underlying distributions, nor can we observe the latent variables $\mathbf{z}$. However, we <em>can</em> approximate them.</p>
<p>Given observations $\mathbf{x}$, we can compute the posterior of the latent distribution $p(\mathbf{z|x})$. This can be used to gain information about the true latent variable distribution. Using Bayes&rsquo; formula, we can write</p>
<p>$$
p(\mathbf{z}|\mathbf{x}) = \frac{p(x|z)p(z)}{p(x)}.
$$</p>
<p>However, this depends on the the marginal distribution of the data $p(\mathbf{x})$, the computation of which involves integrating all possible values of $\mathbf{z}$:</p>
<p>$$
p(\mathbf{x}) = \int p(\mathbf{x}|\mathbf{z}) p(\mathbf{z}) d\mathbf{z}.
$$</p>
<p>Instead of trying to compute the posterior distribution, we can instead approximate it. This is done via our encoder network, which we denote $q_{\theta}({\mathbf{z}})$. Thus, we try to minimize the <a href="https://en.wikipedia.org/wiki/Kullback%E2%80%93Leibler_divergence">KL divergence</a> between the approximate latent density and the posterior distribution:</p>
<p>$$
q_{\theta}^{*}(\mathbf{z}) \in \argmin_{\theta} D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})).
$$</p>
<p>Of course, we don&rsquo;t know the posterior. However, we can rewrite this as</p>
<p>$$
\begin{align*}
D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})) &amp;= \mathbb{E} \left[ \log \left( \frac{q_{\theta}(\mathbf{z})}{p(\mathbf{z}|\mathbf{x})} \right) \right] \\
&amp;= \mathbb{E} \left[ \log q_\theta(\mathbf{z}) \right] - \mathbb{E} \left[ \log p(\mathbf{z} | \mathbf{x}) \right] \\
&amp;= \mathbb{E} \left[ \log q_{\theta}(\mathbf{z}) \right] - \mathbb{E} \left[ \log p(\mathbf{z}, \mathbf{x}) \right] + \mathbb{E} \left[ \log p(\mathbf{x}) \right],
\end{align*}
$$</p>
<p>where the expectation is with respect to $\mathbf{z} \sim q_{\theta}(\mathbf{z})$. Thus, we can drop the expectation around the last term, since it is independent of $\mathbf{z}$. By rearranging this, we see that</p>
<p>$$
\log p(\mathbf{x}) - D_{KL} (q_{\theta}(\mathbf{z}) || p(\mathbf{z}|\mathbf{x})) = \mathbb{E} \left[ \log p(\mathbf{z}, \mathbf{x}) \right] - \mathbb{E} \left[ \log q_{\theta}(\mathbf{z}) \right].
$$</p>
<p>We see that by maximizing the RHS, we simultaneously maximize the likelihood of the data while minimizing the KL divergence between the the true and approximate posterior distributions. We define this to be the evidence lower bound (ELBO), which we use as our loss function in training<sup id="fnref:2"><a href="#fn:2" class="footnote-ref" role="doc-noteref">2</a></sup>.</p>
<p>To compute the ELBO</p>
<h3 id="references-and-further-reading">References and Further Reading</h3>
<ol>
<li>
<p>A. v. d. Oord, O. Vinyals, and K. Kavukcuoglu. <a href="http://arxiv.org/abs/1711.00937">Neural Discrete Representation Learning.</a>. NeurIPS 2017.</p>
</li>
<li>
<p>ESM3 (or ESMFold)</p>
</li>
</ol>
<div class="footnotes" role="doc-endnotes">
<hr>
<ol>
<li id="fn:1">
<p>The term &ldquo;latent&rdquo; means hidden; we generally cannot observe these variables.&#160;<a href="#fnref:1" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
<li id="fn:2">
<p>The marginal distribution $p(\mathbf{x})$ is also known as the &ldquo;evidence&rdquo;. Thus, since the KL divergence is always positive, we see that $\log p(\mathbf{x}) \geq \text{ELBO}$, hence the name.&#160;<a href="#fnref:2" class="footnote-backref" role="doc-backlink">&#x21a9;&#xfe0e;</a></p>
</li>
</ol>
</div>

    
  </div>

  

  

  

  
  <div class="back-to-top">
    <a href="#top">
      back to top
    </a>
  </div>
  

</div>


      </main>
    </div>

    <footer>
      
<link rel="stylesheet"
  href="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.css">
<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/katex.min.js"></script>

<script defer
  src="https://cdn.jsdelivr.net/npm/katex@0.16.0/dist/contrib/auto-render.min.js"
  onload="renderMathInElement(document.body);"></script>

<script>
  document.addEventListener("DOMContentLoaded", function () {
    renderMathInElement(document.body, {
      delimiters: [
        { left: "$$", right: "$$", display: true },
        { left: "$", right: "$", display: false }
      ]
    });
  });
</script>

    </footer>

  </body>

  <script>

  function isAuto() {
    return document.body.classList.contains("auto");
  }

  function setTheme() {
    if (!isAuto()) {
      return
    }

    document.body.classList.remove("auto");
    let cls = "light";
    if (window.matchMedia && window.matchMedia('(prefers-color-scheme: dark)').matches) {
      cls = "dark";
    }

    document.body.classList.add(cls);
  }

  function invertBody() {
    document.body.classList.toggle("dark");
    document.body.classList.toggle("light");
  }

  if (isAuto()) {
    window.matchMedia('(prefers-color-scheme: dark)').addListener(invertBody);
  }

  setTheme();

</script>


<script>
  
  function updateFigureNumbers() {

      const figRefs = document.querySelectorAll('.fig-ref');
      figRefs.forEach(ref => {
          const figId = ref.getAttribute('href').slice(1);
          const figElement = document.getElementById(figId);
          if (figElement) {
              const figIndex = Array.from(figures).indexOf(figElement) + 1;
              ref.textContent = `Figure ${figIndex}`;
          }
      });
  }

  
  window.addEventListener('load', updateFigureNumbers);
</script>



</html>